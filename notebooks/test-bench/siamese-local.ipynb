{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "siamese.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python38164bitdlvenv8c880a4f959d41a0bd88c28d905ec3ee",
   "display_name": "Python 3.8.1 64-bit ('DL': venv)"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cT7lOrijlJs"
   },
   "source": [
    "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
    "\n",
    "In this notebook we reproduce Table 1 in the original \n",
    "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
    "\n",
    "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Twhmbb8kXNQ"
   },
   "source": [
    "\n",
    "--------------------------------\n",
    "# How/Why Siamese Networks Work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5M1FkjdQluR8"
   },
   "source": [
    "# One-Shot Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qac1GqFnl58c"
   },
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Mcpj2P3l8So"
   },
   "source": [
    "# Running the experiment on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "\n",
    "amsterdam = timezone('Europe/Amsterdam')\n",
    "datetime_format = '%Y-%m-%d-T-(%H-%M-%S)'\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPlU4gaHlY5t"
   },
   "source": [
    "-------------------------------------\n",
    "## Definition of the netwok architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
    "  def __init__(self):\n",
    "    super(ConvNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
    "    self.conv3 = nn.Conv2d(128,128,4)\n",
    "    self.conv4 = nn.Conv2d(128,256, 4)\n",
    "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.pool(F.relu(self.conv1(x)))\n",
    "    out = self.pool(F.relu(self.conv2(out)))\n",
    "    out = self.pool(F.relu(self.conv3(out)))\n",
    "    out = F.relu(self.conv4(out))\n",
    "    out = out.view(-1, 256*6*6)\n",
    "    # We get the h feature vectors\n",
    "    out = F.sigmoid(self.fc1(out))\n",
    "    return out\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
    "  def __init__(self, net):\n",
    "    # Receives a net as a parameter, we can just have 1 net \n",
    "    # but do the forward pass twice! and then just update once, much more \n",
    "    # elegant\n",
    "    super(SiameseNet, self).__init__()\n",
    "    # Instantiate two of the same class\n",
    "    self.convnet = net\n",
    "    # Final layer and output\n",
    "    self.prediction_layer = nn.Linear(4096,1)\n",
    "\n",
    "  def forward(self,x1, x2):\n",
    "    \"\"\"Computes the forward given two images\"\"\"\n",
    "    h1 = self.convnet(x1)\n",
    "    h2 = self.convnet(x2)\n",
    "    h = self.calculate_l1_distance(h1, h2)\n",
    "    out = F.sigmoid(self.prediction_layer(h))\n",
    "    return out\n",
    "  \n",
    "  def calculate_l1_distance(self, h1, h2):\n",
    "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
    "    return torch.abs(h1-h2)\n",
    "\n",
    "\n",
    "# How to initialize the weights according to the paper\n",
    "def weights_init(model):\n",
    "  if isinstance(model, nn.Conv2d):\n",
    "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
    "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
    "  elif isinstance(model, nn.Linear):\n",
    "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
    "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Siamese Network and Initialize weights according to specifications\n",
    "- Conv layers: \n",
    "  - Weights: Normal(0, 1e-2)\n",
    "  - Bias: Normal(0.5, 1e-2)\n",
    "- Linear layers: \n",
    "  - Weights: Normal(0, 0.2)\n",
    "  - Bias: Normal(0.5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SiameseNet(\n  (convnet): ConvNet(\n    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n  )\n  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "conv = ConvNet()\n",
    "siamese = SiameseNet(conv)\n",
    "siamese.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P_10kSyZO0p"
   },
   "source": [
    "### Define the Loss (CrossEntropy) and the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Learning rate decay per epoch\n",
    "lr_decay_rate = 0.99\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#optimizer = optim.SGD(siamese.parameters(), lr = 0.01, momentum=0.5, weight_decay=2e-4)\n",
    "optimizer = optim.SGD(siamese.parameters(), lr = 0.1, weight_decay=2e-4)\n",
    "optim_scheduler = optim.lr_scheduler.CyclicLR(optimizer,base_lr=1e-3,max_lr=1e-2,step_size_up=2000)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtINTkF9mZUC"
   },
   "source": [
    "---\n",
    "## Hyperparameter Setting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GgaTl2Tmvg0"
   },
   "source": [
    "---\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, data_path):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path: str\n",
    "      Path to the pickle file\n",
    "    \"\"\"\n",
    "    self.data = None\n",
    "    self.alphabet_index = None\n",
    "    with open(data_path, \"rb\") as f:\n",
    "      X, i = pickle.load(f)\n",
    "      self.data = X.astype(\"float32\")\n",
    "      self.alphabet_index = i\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    index: int\n",
    "      index from which to get the data\n",
    "    \"\"\"\n",
    "    # get dimensions of the data\n",
    "    num_letters, num_drawings, height, width = self.data.shape\n",
    "\n",
    "    # initialize index2\n",
    "    # index of second letter from pair same as first\n",
    "    index2 = index\n",
    "\n",
    "    # randomly choose a variant of the letter\n",
    "    drawing_index = np.random.choice(num_drawings)\n",
    "    drawing_index2 = np.random.choice(num_drawings)\n",
    "\n",
    "    # choose image for first letter\n",
    "    X1 = self.data[index, drawing_index, :, :].reshape(width, height)\n",
    "    \n",
    "    # set label to be 1, i.e. same letter\n",
    "    y = np.array([1.0], dtype=\"float32\")\n",
    "\n",
    "    # with 50% probability,\n",
    "    # pick an image of a different letter\n",
    "    # and change the label to 0, i.e. different letter\n",
    "    if np.random.uniform() >= 0.5:\n",
    "      index2 = (index + np.random.randint(1, num_letters)) % num_letters\n",
    "      y = np.array([0.0], dtype=\"float32\")\n",
    "    \n",
    "    # choose image for the second letter\n",
    "    X2 = self.data[index2, drawing_index2, :, :].reshape(width, height)\n",
    "\n",
    "    return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/processed/train.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fed69ff6e521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../data/processed/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d377e6202d67>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/processed/train.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create the dataset object\n",
    "data_path = \"../../data/processed/\"\n",
    "\n",
    "train_set = Dataset(os.path.join(data_path, \"train.pkl\"))\n",
    "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "siamese.to(device, dtype=torch.float32)\n",
    "\n",
    "# set parameters for data creation\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': num_workers}\n",
    "\n",
    "# create the dataloader object which returns a generator over the data\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "eval_generator = data.DataLoader(eval_set, **params)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj8kcukdmc5b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---------------------------------\n",
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create writer for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
    "logs_path = \"./logs/\" + dt + \"/\"\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass\n",
    "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-482b3290fac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# create writer for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
    "logs_path = \"./logs/\" + dt + \"/\"\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass\n",
    "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)\n",
    "\n",
    "\n",
    "# run training\n",
    "for epoch in range(200):\n",
    "  running_loss = 0.0\n",
    "  i = 0\n",
    "  \n",
    "  for X1, X2, y in train_generator:\n",
    "    X1 = X1.to(device)\n",
    "    X2 = X2.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    X1 = X1.view(-1, 1, 105, 105)\n",
    "    X2 = X2.view(-1, 1, 105, 105)\n",
    "    \n",
    "\n",
    "    outputs = siamese(X1, X2)\n",
    "    # print(outputs, outputs.dtype)\n",
    "    # print(outputs.shape)\n",
    "    # print(y.shape)\n",
    "    # print(outputs)\n",
    "    # outputs_ = torch.cat((outputs.view(-1, 1), (1-outputs).view(-1, 1)), dim=1)\n",
    "    loss = criterion(outputs , y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "      writer.add_scalar(\"training loss (per epoch)\", loss.item(), epoch)\n",
    "      writer.add_scalar(\"learning rate (per epoch)\", optim_scheduler.state_dict()[\"_last_lr\"][0], epoch)\n",
    "    \n",
    "    writer.add_scalar(\"training loss\", loss.item(), epoch * len(train_generator) + i)\n",
    "\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / (i+1)))\n",
    "    i+=1\n",
    "  # Update the learning rate\n",
    "  optim_scheduler.step()\n",
    "\n",
    "  #\n",
    "    \n",
    "  \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gDe5nYvhFQc"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/processed/eval.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f950d88bbc95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Test to see if labels are correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d377e6202d67>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/processed/eval.pkl'"
     ]
    }
   ],
   "source": [
    "### Test to see if labels are correct\n",
    "\n",
    "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
    "\n",
    "batch_size = 1\n",
    "eval_generator = data.DataLoader(eval_set, **{'batch_size': batch_size})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "i=1\n",
    "columns = 2\n",
    "rows = batch_size\n",
    "for x1, x2, y in eval_generator:\n",
    "  print(x1.shape)\n",
    "  print(x2.shape)\n",
    "  fig.add_subplot(rows, columns, i)\n",
    "  # img = np.random.randint(10, size=(h,w))\n",
    "  plt.imshow(x1.squeeze())\n",
    "  i+=1\n",
    "  fig.add_subplot(rows, columns, i)\n",
    "  plt.imshow(x2.squeeze())\n",
    "  i+=1\n",
    "  if (y == 1):\n",
    "    print(\"same letter\")\n",
    "  else:\n",
    "    print(\"different letter\")\n",
    "  break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
    "save_path = \"./saves/\" + dt + \"/\"\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass\n",
    "torch.save(siamese, save_path + \"siamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}