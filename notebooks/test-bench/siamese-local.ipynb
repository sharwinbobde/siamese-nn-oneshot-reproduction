{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "siamese.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python38164bitdlvenv8c880a4f959d41a0bd88c28d905ec3ee",
   "display_name": "Python 3.8.1 64-bit ('DL': venv)"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cT7lOrijlJs"
   },
   "source": [
    "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
    "\n",
    "In this notebook we reproduce Table 1 in the original \n",
    "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
    "\n",
    "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Twhmbb8kXNQ"
   },
   "source": [
    "\n",
    "--------------------------------\n",
    "# How/Why Siamese Networks Work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5M1FkjdQluR8"
   },
   "source": [
    "# One-Shot Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qac1GqFnl58c"
   },
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Mcpj2P3l8So"
   },
   "source": [
    "# Running the experiment on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "\n",
    "amsterdam = timezone('Europe/Amsterdam')\n",
    "datetime_format = '%Y-%m-%d-T-(%H-%M-%S)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPlU4gaHlY5t"
   },
   "source": [
    "-------------------------------------\n",
    "## Definition of the netwok architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
    "  def __init__(self):\n",
    "    super(ConvNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
    "    self.conv3 = nn.Conv2d(128,128,4)\n",
    "    self.conv4 = nn.Conv2d(128,256, 4)\n",
    "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.pool(F.relu(self.conv1(x)))\n",
    "    out = self.pool(F.relu(self.conv2(out)))\n",
    "    out = self.pool(F.relu(self.conv3(out)))\n",
    "    out = F.relu(self.conv4(out))\n",
    "    out = out.view(-1, 256*6*6)\n",
    "    # We get the h feature vectors\n",
    "    out = F.sigmoid(self.fc1(out))\n",
    "    return out\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
    "  def __init__(self, net):\n",
    "    # Receives a net as a parameter, we can just have 1 net \n",
    "    # but do the forward pass twice! and then just update once, much more \n",
    "    # elegant\n",
    "    super(SiameseNet, self).__init__()\n",
    "    # Instantiate two of the same class\n",
    "    self.convnet = net\n",
    "    # Final layer and output\n",
    "    self.prediction_layer = nn.Linear(4096,1)\n",
    "\n",
    "  def forward(self,x1, x2):\n",
    "    \"\"\"Computes the forward given two images\"\"\"\n",
    "    h1 = self.convnet(x1)\n",
    "    h2 = self.convnet(x2)\n",
    "    h = self.calculate_l1_distance(h1, h2)\n",
    "    out = F.sigmoid(self.prediction_layer(h))\n",
    "    return out\n",
    "  \n",
    "  def calculate_l1_distance(self, h1, h2):\n",
    "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
    "    return torch.abs(h1-h2)\n",
    "\n",
    "\n",
    "# How to initialize the weights according to the paper\n",
    "def weights_init(model):\n",
    "  if isinstance(model, nn.Conv2d):\n",
    "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
    "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
    "  elif isinstance(model, nn.Linear):\n",
    "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
    "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Siamese Network and Initialize weights according to specifications\n",
    "- Conv layers: \n",
    "  - Weights: Normal(0, 1e-2)\n",
    "  - Bias: Normal(0.5, 1e-2)\n",
    "- Linear layers: \n",
    "  - Weights: Normal(0, 0.2)\n",
    "  - Bias: Normal(0.5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "SiameseNet(\n  (convnet): ConvNet(\n    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n  )\n  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = ConvNet()\n",
    "siamese = SiameseNet(conv)\n",
    "siamese.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4P_10kSyZO0p"
   },
   "source": [
    "### Define the Loss (CrossEntropy) and the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Learning rate decay per epoch\n",
    "lr_decay_rate = 0.99\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#optimizer = optim.SGD(siamese.parameters(), lr = 0.01, momentum=0.5, weight_decay=2e-4)\n",
    "optimizer = optim.SGD(siamese.parameters(), lr = 0.1, weight_decay=2e-4)\n",
    "optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = lr_decay_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtINTkF9mZUC"
   },
   "source": [
    "---\n",
    "## Hyperparameter Setting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GgaTl2Tmvg0"
   },
   "source": [
    "---\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, data_path):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path: str\n",
    "      Path to the pickle file\n",
    "    \"\"\"\n",
    "    self.data = None\n",
    "    self.alphabet_index = None\n",
    "    with open(data_path, \"rb\") as f:\n",
    "      X, i = pickle.load(f)\n",
    "      self.data = X.astype(\"float32\")\n",
    "      self.alphabet_index = i\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    index: int\n",
    "      index from which to get the data\n",
    "    \"\"\"\n",
    "    # get dimensions of the data\n",
    "    num_letters, num_drawings, height, width = self.data.shape\n",
    "\n",
    "    # initialize index2\n",
    "    # index of second letter from pair same as first\n",
    "    index2 = index\n",
    "\n",
    "    # randomly choose a variant of the letter\n",
    "    drawing_index = np.random.choice(num_drawings)\n",
    "    drawing_index2 = np.random.choice(num_drawings)\n",
    "\n",
    "    # choose image for first letter\n",
    "    X1 = self.data[index, drawing_index, :, :].reshape(width, height)\n",
    "    \n",
    "    # set label to be 1, i.e. same letter\n",
    "    y = np.array([1.0], dtype=\"float32\")\n",
    "\n",
    "    # with 50% probability,\n",
    "    # pick an image of a different letter\n",
    "    # and change the label to 0, i.e. different letter\n",
    "    if np.random.uniform() >= 0.5:\n",
    "      index2 = (index + np.random.randint(1, num_letters)) % num_letters\n",
    "      y = np.array([0.0], dtype=\"float32\")\n",
    "    \n",
    "    # choose image for the second letter\n",
    "    X2 = self.data[index2, drawing_index2, :, :].reshape(width, height)\n",
    "\n",
    "    return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create the dataset object\n",
    "data_path = \"../../data/processed/\"\n",
    "\n",
    "train_set = Dataset(os.path.join(data_path, \"train.pkl\"))\n",
    "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "siamese.to(device, dtype=torch.float32)\n",
    "\n",
    "# set parameters for data creation\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': num_workers}\n",
    "\n",
    "# create the dataloader object which returns a generator over the data\n",
    "train_generator = data.DataLoader(train_set, **params)\n",
    "eval_generator = data.DataLoader(eval_set, **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bj8kcukdmc5b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---------------------------------\n",
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create writer for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
    "logs_path = \"./logs/\" + dt + \"/\"\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass\n",
    "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[101,     1] loss: 0.773\n[101,     2] loss: 0.794\n[101,     3] loss: 0.770\n[101,     4] loss: 0.777\n[101,     5] loss: 0.767\n[101,     6] loss: 0.762\n[101,     7] loss: 0.759\n[101,     8] loss: 0.758\n[102,     1] loss: 0.875\n[102,     2] loss: 0.780\n[102,     3] loss: 0.761\n[102,     4] loss: 0.777\n[102,     5] loss: 0.783\n[102,     6] loss: 0.786\n[102,     7] loss: 0.780\n[102,     8] loss: 0.775\n[103,     1] loss: 0.885\n[103,     2] loss: 0.836\n[103,     3] loss: 0.779\n[103,     4] loss: 0.769\n[103,     5] loss: 0.807\n[103,     6] loss: 0.803\n[103,     7] loss: 0.808\n[103,     8] loss: 0.820\n[104,     1] loss: 0.756\n[104,     2] loss: 0.817\n[104,     3] loss: 0.799\n[104,     4] loss: 0.807\n[104,     5] loss: 0.821\n[104,     6] loss: 0.826\n[104,     7] loss: 0.827\n[104,     8] loss: 0.810\n[105,     1] loss: 0.844\n[105,     2] loss: 0.818\n[105,     3] loss: 0.810\n[105,     4] loss: 0.793\n[105,     5] loss: 0.782\n[105,     6] loss: 0.787\n[105,     7] loss: 0.791\n[105,     8] loss: 0.798\n[106,     1] loss: 0.813\n[106,     2] loss: 0.852\n[106,     3] loss: 0.798\n[106,     4] loss: 0.810\n[106,     5] loss: 0.788\n[106,     6] loss: 0.796\n[106,     7] loss: 0.794\n[106,     8] loss: 0.787\n[107,     1] loss: 0.914\n[107,     2] loss: 0.860\n[107,     3] loss: 0.862\n[107,     4] loss: 0.854\n[107,     5] loss: 0.863\n[107,     6] loss: 0.856\n[107,     7] loss: 0.859\n[107,     8] loss: 0.865\n[108,     1] loss: 0.877\n[108,     2] loss: 0.853\n[108,     3] loss: 0.838\n[108,     4] loss: 0.814\n[108,     5] loss: 0.801\n[108,     6] loss: 0.809\n[108,     7] loss: 0.813\n[108,     8] loss: 0.816\n[109,     1] loss: 0.805\n[109,     2] loss: 0.806\n[109,     3] loss: 0.791\n[109,     4] loss: 0.811\n[109,     5] loss: 0.810\n[109,     6] loss: 0.799\n[109,     7] loss: 0.794\n[109,     8] loss: 0.778\n[110,     1] loss: 0.847\n[110,     2] loss: 0.798\n[110,     3] loss: 0.766\n[110,     4] loss: 0.791\n[110,     5] loss: 0.774\n[110,     6] loss: 0.762\n[110,     7] loss: 0.759\n[110,     8] loss: 0.757\n[111,     1] loss: 0.899\n[111,     2] loss: 0.864\n[111,     3] loss: 0.835\n[111,     4] loss: 0.829\n[111,     5] loss: 0.816\n[111,     6] loss: 0.825\n[111,     7] loss: 0.834\n[111,     8] loss: 0.840\n[112,     1] loss: 0.798\n[112,     2] loss: 0.778\n[112,     3] loss: 0.804\n[112,     4] loss: 0.790\n[112,     5] loss: 0.814\n[112,     6] loss: 0.822\n[112,     7] loss: 0.841\n[112,     8] loss: 0.842\n[113,     1] loss: 0.886\n[113,     2] loss: 0.791\n[113,     3] loss: 0.805\n[113,     4] loss: 0.811\n[113,     5] loss: 0.810\n[113,     6] loss: 0.808\n[113,     7] loss: 0.809\n[113,     8] loss: 0.811\n[114,     1] loss: 0.871\n[114,     2] loss: 0.751\n[114,     3] loss: 0.737\n[114,     4] loss: 0.748\n[114,     5] loss: 0.746\n[114,     6] loss: 0.762\n[114,     7] loss: 0.768\n[114,     8] loss: 0.781\n[115,     1] loss: 0.867\n[115,     2] loss: 0.784\n[115,     3] loss: 0.779\n[115,     4] loss: 0.812\n[115,     5] loss: 0.814\n[115,     6] loss: 0.823\n[115,     7] loss: 0.844\n[115,     8] loss: 0.860\n[116,     1] loss: 1.026\n[116,     2] loss: 1.026\n[116,     3] loss: 0.984\n[116,     4] loss: 0.986\n[116,     5] loss: 0.976\n[116,     6] loss: 0.973\n[116,     7] loss: 0.971\n[116,     8] loss: 0.954\n[117,     1] loss: 0.897\n[117,     2] loss: 0.941\n[117,     3] loss: 0.906\n[117,     4] loss: 0.872\n[117,     5] loss: 0.859\n[117,     6] loss: 0.846\n[117,     7] loss: 0.858\n[117,     8] loss: 0.860\n[118,     1] loss: 0.834\n[118,     2] loss: 0.942\n[118,     3] loss: 0.939\n[118,     4] loss: 0.903\n[118,     5] loss: 0.907\n[118,     6] loss: 0.886\n[118,     7] loss: 0.899\n[118,     8] loss: 0.908\n[119,     1] loss: 0.906\n[119,     2] loss: 0.952\n[119,     3] loss: 0.930\n[119,     4] loss: 0.911\n[119,     5] loss: 0.881\n[119,     6] loss: 0.880\n[119,     7] loss: 0.866\n[119,     8] loss: 0.884\n[120,     1] loss: 0.852\n[120,     2] loss: 0.897\n[120,     3] loss: 0.922\n[120,     4] loss: 0.908\n[120,     5] loss: 0.920\n[120,     6] loss: 0.918\n[120,     7] loss: 0.909\n[120,     8] loss: 0.922\n[121,     1] loss: 0.997\n[121,     2] loss: 0.953\n[121,     3] loss: 0.920\n[121,     4] loss: 0.914\n[121,     5] loss: 0.922\n[121,     6] loss: 0.902\n[121,     7] loss: 0.906\n[121,     8] loss: 0.925\n[122,     1] loss: 0.878\n[122,     2] loss: 0.883\n[122,     3] loss: 0.863\n[122,     4] loss: 0.905\n[122,     5] loss: 0.891\n[122,     6] loss: 0.895\n[122,     7] loss: 0.884\n[122,     8] loss: 0.873\n[123,     1] loss: 0.954\n[123,     2] loss: 0.971\n[123,     3] loss: 0.936\n[123,     4] loss: 0.891\n[123,     5] loss: 0.881\n[123,     6] loss: 0.877\n[123,     7] loss: 0.876\n[123,     8] loss: 0.880\n[124,     1] loss: 0.944\n[124,     2] loss: 0.930\n[124,     3] loss: 0.889\n[124,     4] loss: 0.895\n[124,     5] loss: 0.908\n[124,     6] loss: 0.912\n[124,     7] loss: 0.888\n[124,     8] loss: 0.895\n[125,     1] loss: 0.844\n[125,     2] loss: 0.859\n[125,     3] loss: 0.824\n[125,     4] loss: 0.838\n[125,     5] loss: 0.850\n[125,     6] loss: 0.876\n[125,     7] loss: 0.881\n[125,     8] loss: 0.873\n[126,     1] loss: 0.857\n[126,     2] loss: 0.842\n[126,     3] loss: 0.850\n[126,     4] loss: 0.864\n[126,     5] loss: 0.868\n[126,     6] loss: 0.866\n[126,     7] loss: 0.872\n[126,     8] loss: 0.862\n[127,     1] loss: 0.915\n[127,     2] loss: 0.931\n[127,     3] loss: 0.867\n[127,     4] loss: 0.848\n[127,     5] loss: 0.844\n[127,     6] loss: 0.840\n[127,     7] loss: 0.829\n[127,     8] loss: 0.844\n[128,     1] loss: 0.780\n[128,     2] loss: 0.833\n[128,     3] loss: 0.854\n[128,     4] loss: 0.842\n[128,     5] loss: 0.828\n[128,     6] loss: 0.821\n[128,     7] loss: 0.815\n[128,     8] loss: 0.829\n[129,     1] loss: 0.820\n[129,     2] loss: 0.791\n[129,     3] loss: 0.775\n[129,     4] loss: 0.765\n[129,     5] loss: 0.775\n[129,     6] loss: 0.780\n[129,     7] loss: 0.775\n[129,     8] loss: 0.787\n[130,     1] loss: 0.857\n[130,     2] loss: 0.872\n[130,     3] loss: 0.883\n[130,     4] loss: 0.866\n[130,     5] loss: 0.856\n[130,     6] loss: 0.841\n[130,     7] loss: 0.852\n[130,     8] loss: 0.865\n[131,     1] loss: 0.839\n[131,     2] loss: 0.846\n[131,     3] loss: 0.844\n[131,     4] loss: 0.842\n[131,     5] loss: 0.823\n[131,     6] loss: 0.831\n[131,     7] loss: 0.833\n[131,     8] loss: 0.832\n[132,     1] loss: 0.711\n[132,     2] loss: 0.724\n[132,     3] loss: 0.737\n[132,     4] loss: 0.762\n[132,     5] loss: 0.748\n[132,     6] loss: 0.744\n[132,     7] loss: 0.757\n[132,     8] loss: 0.755\n[133,     1] loss: 0.794\n[133,     2] loss: 0.854\n[133,     3] loss: 0.820\n[133,     4] loss: 0.835\n[133,     5] loss: 0.830\n[133,     6] loss: 0.840\n[133,     7] loss: 0.835\n[133,     8] loss: 0.834\n[134,     1] loss: 0.862\n[134,     2] loss: 0.821\n[134,     3] loss: 0.876\n[134,     4] loss: 0.855\n[134,     5] loss: 0.836\n[134,     6] loss: 0.834\n[134,     7] loss: 0.822\n[134,     8] loss: 0.831\n[135,     1] loss: 0.862\n[135,     2] loss: 0.750\n[135,     3] loss: 0.733\n[135,     4] loss: 0.755\n[135,     5] loss: 0.738\n[135,     6] loss: 0.751\n[135,     7] loss: 0.750\n[135,     8] loss: 0.736\n[136,     1] loss: 0.829\n[136,     2] loss: 0.798\n[136,     3] loss: 0.827\n[136,     4] loss: 0.803\n[136,     5] loss: 0.792\n[136,     6] loss: 0.813\n[136,     7] loss: 0.796\n[136,     8] loss: 0.776\n[137,     1] loss: 0.823\n[137,     2] loss: 0.779\n[137,     3] loss: 0.796\n[137,     4] loss: 0.786\n[137,     5] loss: 0.778\n[137,     6] loss: 0.795\n[137,     7] loss: 0.798\n[137,     8] loss: 0.813\n[138,     1] loss: 0.961\n[138,     2] loss: 0.832\n[138,     3] loss: 0.804\n[138,     4] loss: 0.818\n[138,     5] loss: 0.813\n[138,     6] loss: 0.812\n[138,     7] loss: 0.807\n[138,     8] loss: 0.792\n[139,     1] loss: 0.814\n[139,     2] loss: 0.754\n[139,     3] loss: 0.768\n[139,     4] loss: 0.764\n[139,     5] loss: 0.771\n[139,     6] loss: 0.778\n[139,     7] loss: 0.783\n[139,     8] loss: 0.774\n[140,     1] loss: 0.888\n[140,     2] loss: 0.809\n[140,     3] loss: 0.780\n[140,     4] loss: 0.807\n[140,     5] loss: 0.836\n[140,     6] loss: 0.827\n[140,     7] loss: 0.820\n[140,     8] loss: 0.812\n[141,     1] loss: 0.771\n[141,     2] loss: 0.754\n[141,     3] loss: 0.791\n[141,     4] loss: 0.787\n[141,     5] loss: 0.770\n[141,     6] loss: 0.771\n[141,     7] loss: 0.776\n[141,     8] loss: 0.782\n[142,     1] loss: 0.768\n[142,     2] loss: 0.822\n[142,     3] loss: 0.815\n[142,     4] loss: 0.836\n[142,     5] loss: 0.837\n[142,     6] loss: 0.829\n[142,     7] loss: 0.816\n[142,     8] loss: 0.821\n[143,     1] loss: 0.758\n[143,     2] loss: 0.776\n[143,     3] loss: 0.766\n[143,     4] loss: 0.747\n[143,     5] loss: 0.767\n[143,     6] loss: 0.760\n[143,     7] loss: 0.749\n[143,     8] loss: 0.743\n[144,     1] loss: 0.776\n[144,     2] loss: 0.784\n[144,     3] loss: 0.777\n[144,     4] loss: 0.794\n[144,     5] loss: 0.789\n[144,     6] loss: 0.789\n[144,     7] loss: 0.795\n[144,     8] loss: 0.801\n[145,     1] loss: 0.829\n[145,     2] loss: 0.771\n[145,     3] loss: 0.790\n[145,     4] loss: 0.800\n[145,     5] loss: 0.833\n[145,     6] loss: 0.827\n[145,     7] loss: 0.840\n[145,     8] loss: 0.832\n[146,     1] loss: 0.945\n[146,     2] loss: 0.856\n[146,     3] loss: 0.855\n[146,     4] loss: 0.827\n[146,     5] loss: 0.829\n[146,     6] loss: 0.836\n[146,     7] loss: 0.832\n[146,     8] loss: 0.823\n[147,     1] loss: 0.888\n[147,     2] loss: 0.862\n[147,     3] loss: 0.800\n[147,     4] loss: 0.773\n[147,     5] loss: 0.792\n[147,     6] loss: 0.786\n[147,     7] loss: 0.784\n[147,     8] loss: 0.793\n[148,     1] loss: 0.764\n[148,     2] loss: 0.742\n[148,     3] loss: 0.760\n[148,     4] loss: 0.739\n[148,     5] loss: 0.755\n[148,     6] loss: 0.780\n[148,     7] loss: 0.805\n[148,     8] loss: 0.834\n[149,     1] loss: 0.980\n[149,     2] loss: 0.970\n[149,     3] loss: 0.958\n[149,     4] loss: 0.926\n[149,     5] loss: 0.900\n[149,     6] loss: 0.887\n[149,     7] loss: 0.865\n[149,     8] loss: 0.859\n[150,     1] loss: 0.722\n[150,     2] loss: 0.723\n[150,     3] loss: 0.748\n[150,     4] loss: 0.769\n[150,     5] loss: 0.781\n[150,     6] loss: 0.769\n[150,     7] loss: 0.769\n[150,     8] loss: 0.776\n[151,     1] loss: 0.781\n[151,     2] loss: 0.774\n[151,     3] loss: 0.751\n[151,     4] loss: 0.770\n[151,     5] loss: 0.777\n[151,     6] loss: 0.772\n[151,     7] loss: 0.766\n[151,     8] loss: 0.763\n[152,     1] loss: 0.917\n[152,     2] loss: 0.819\n[152,     3] loss: 0.807\n[152,     4] loss: 0.793\n[152,     5] loss: 0.802\n[152,     6] loss: 0.792\n[152,     7] loss: 0.792\n[152,     8] loss: 0.780\n[153,     1] loss: 0.810\n[153,     2] loss: 0.794\n[153,     3] loss: 0.808\n[153,     4] loss: 0.810\n[153,     5] loss: 0.807\n[153,     6] loss: 0.812\n[153,     7] loss: 0.803\n[153,     8] loss: 0.809\n[154,     1] loss: 0.681\n[154,     2] loss: 0.731\n[154,     3] loss: 0.739\n[154,     4] loss: 0.735\n[154,     5] loss: 0.742\n[154,     6] loss: 0.749\n[154,     7] loss: 0.757\n[154,     8] loss: 0.756\n[155,     1] loss: 0.883\n[155,     2] loss: 0.812\n[155,     3] loss: 0.826\n[155,     4] loss: 0.816\n[155,     5] loss: 0.793\n[155,     6] loss: 0.795\n[155,     7] loss: 0.790\n[155,     8] loss: 0.793\n[156,     1] loss: 0.744\n[156,     2] loss: 0.786\n[156,     3] loss: 0.781\n[156,     4] loss: 0.790\n[156,     5] loss: 0.787\n[156,     6] loss: 0.786\n[156,     7] loss: 0.796\n[156,     8] loss: 0.812\n[157,     1] loss: 0.810\n[157,     2] loss: 0.790\n[157,     3] loss: 0.799\n[157,     4] loss: 0.790\n[157,     5] loss: 0.781\n[157,     6] loss: 0.774\n[157,     7] loss: 0.788\n[157,     8] loss: 0.789\n[158,     1] loss: 0.700\n[158,     2] loss: 0.675\n[158,     3] loss: 0.719\n[158,     4] loss: 0.739\n[158,     5] loss: 0.748\n[158,     6] loss: 0.741\n[158,     7] loss: 0.752\n[158,     8] loss: 0.765\n[159,     1] loss: 0.820\n[159,     2] loss: 0.743\n[159,     3] loss: 0.771\n[159,     4] loss: 0.787\n[159,     5] loss: 0.763\n[159,     6] loss: 0.771\n[159,     7] loss: 0.772\n[159,     8] loss: 0.770\n[160,     1] loss: 0.838\n[160,     2] loss: 0.772\n[160,     3] loss: 0.776\n[160,     4] loss: 0.776\n[160,     5] loss: 0.755\n[160,     6] loss: 0.767\n[160,     7] loss: 0.762\n[160,     8] loss: 0.766\n[161,     1] loss: 0.756\n[161,     2] loss: 0.761\n[161,     3] loss: 0.737\n[161,     4] loss: 0.712\n[161,     5] loss: 0.719\n[161,     6] loss: 0.730\n[161,     7] loss: 0.733\n[161,     8] loss: 0.738\n[162,     1] loss: 0.824\n[162,     2] loss: 0.798\n[162,     3] loss: 0.821\n[162,     4] loss: 0.797\n[162,     5] loss: 0.786\n[162,     6] loss: 0.790\n[162,     7] loss: 0.792\n[162,     8] loss: 0.779\n[163,     1] loss: 0.739\n[163,     2] loss: 0.743\n[163,     3] loss: 0.753\n[163,     4] loss: 0.786\n[163,     5] loss: 0.758\n[163,     6] loss: 0.758\n[163,     7] loss: 0.772\n[163,     8] loss: 0.777\n[164,     1] loss: 0.712\n[164,     2] loss: 0.697\n[164,     3] loss: 0.737\n[164,     4] loss: 0.725\n[164,     5] loss: 0.722\n[164,     6] loss: 0.726\n[164,     7] loss: 0.737\n[164,     8] loss: 0.744\n[165,     1] loss: 0.796\n[165,     2] loss: 0.741\n[165,     3] loss: 0.749\n[165,     4] loss: 0.741\n[165,     5] loss: 0.757\n[165,     6] loss: 0.765\n[165,     7] loss: 0.775\n[165,     8] loss: 0.767\n[166,     1] loss: 0.788\n[166,     2] loss: 0.771\n[166,     3] loss: 0.790\n[166,     4] loss: 0.793\n[166,     5] loss: 0.787\n[166,     6] loss: 0.811\n[166,     7] loss: 0.808\n[166,     8] loss: 0.783\n[167,     1] loss: 0.780\n[167,     2] loss: 0.721\n[167,     3] loss: 0.707\n[167,     4] loss: 0.731\n[167,     5] loss: 0.737\n[167,     6] loss: 0.750\n[167,     7] loss: 0.750\n[167,     8] loss: 0.747\n[168,     1] loss: 0.748\n[168,     2] loss: 0.726\n[168,     3] loss: 0.732\n[168,     4] loss: 0.748\n[168,     5] loss: 0.742\n[168,     6] loss: 0.764\n[168,     7] loss: 0.783\n[168,     8] loss: 0.780\n[169,     1] loss: 0.762\n[169,     2] loss: 0.721\n[169,     3] loss: 0.749\n[169,     4] loss: 0.755\n[169,     5] loss: 0.746\n[169,     6] loss: 0.760\n[169,     7] loss: 0.768\n[169,     8] loss: 0.775\n[170,     1] loss: 0.813\n[170,     2] loss: 0.757\n[170,     3] loss: 0.731\n[170,     4] loss: 0.768\n[170,     5] loss: 0.772\n[170,     6] loss: 0.781\n[170,     7] loss: 0.776\n[170,     8] loss: 0.779\n[171,     1] loss: 0.736\n[171,     2] loss: 0.708\n[171,     3] loss: 0.734\n[171,     4] loss: 0.734\n[171,     5] loss: 0.742\n[171,     6] loss: 0.748\n[171,     7] loss: 0.768\n[171,     8] loss: 0.789\n[172,     1] loss: 0.826\n[172,     2] loss: 0.754\n[172,     3] loss: 0.752\n[172,     4] loss: 0.789\n[172,     5] loss: 0.777\n[172,     6] loss: 0.776\n[172,     7] loss: 0.771\n[172,     8] loss: 0.768\n[173,     1] loss: 0.830\n[173,     2] loss: 0.769\n[173,     3] loss: 0.775\n[173,     4] loss: 0.781\n[173,     5] loss: 0.770\n[173,     6] loss: 0.752\n[173,     7] loss: 0.759\n[173,     8] loss: 0.768\n[174,     1] loss: 0.770\n[174,     2] loss: 0.706\n[174,     3] loss: 0.718\n[174,     4] loss: 0.714\n[174,     5] loss: 0.723\n[174,     6] loss: 0.739\n[174,     7] loss: 0.747\n[174,     8] loss: 0.738\n[175,     1] loss: 0.804\n[175,     2] loss: 0.733\n[175,     3] loss: 0.731\n[175,     4] loss: 0.737\n[175,     5] loss: 0.737\n[175,     6] loss: 0.747\n[175,     7] loss: 0.740\n[175,     8] loss: 0.756\n[176,     1] loss: 0.720\n[176,     2] loss: 0.742\n[176,     3] loss: 0.753\n[176,     4] loss: 0.765\n[176,     5] loss: 0.754\n[176,     6] loss: 0.757\n[176,     7] loss: 0.755\n[176,     8] loss: 0.760\n[177,     1] loss: 0.708\n[177,     2] loss: 0.715\n[177,     3] loss: 0.720\n[177,     4] loss: 0.734\n[177,     5] loss: 0.741\n[177,     6] loss: 0.743\n[177,     7] loss: 0.744\n[177,     8] loss: 0.748\n[178,     1] loss: 0.804\n[178,     2] loss: 0.783\n[178,     3] loss: 0.791\n[178,     4] loss: 0.803\n[178,     5] loss: 0.784\n[178,     6] loss: 0.785\n[178,     7] loss: 0.773\n[178,     8] loss: 0.763\n[179,     1] loss: 0.738\n[179,     2] loss: 0.678\n[179,     3] loss: 0.678\n[179,     4] loss: 0.693\n[179,     5] loss: 0.691\n[179,     6] loss: 0.707\n[179,     7] loss: 0.706\n[179,     8] loss: 0.699\n[180,     1] loss: 0.835\n[180,     2] loss: 0.725\n[180,     3] loss: 0.739\n[180,     4] loss: 0.755\n[180,     5] loss: 0.748\n[180,     6] loss: 0.750\n[180,     7] loss: 0.745\n[180,     8] loss: 0.732\n[181,     1] loss: 0.692\n[181,     2] loss: 0.692\n[181,     3] loss: 0.704\n[181,     4] loss: 0.721\n[181,     5] loss: 0.709\n[181,     6] loss: 0.721\n[181,     7] loss: 0.734\n[181,     8] loss: 0.731\n[182,     1] loss: 0.685\n[182,     2] loss: 0.684\n[182,     3] loss: 0.695\n[182,     4] loss: 0.700\n[182,     5] loss: 0.697\n[182,     6] loss: 0.700\n[182,     7] loss: 0.715\n[182,     8] loss: 0.702\n[183,     1] loss: 0.685\n[183,     2] loss: 0.678\n[183,     3] loss: 0.696\n[183,     4] loss: 0.710\n[183,     5] loss: 0.723\n[183,     6] loss: 0.727\n[183,     7] loss: 0.728\n[183,     8] loss: 0.734\n[184,     1] loss: 0.793\n[184,     2] loss: 0.747\n[184,     3] loss: 0.757\n[184,     4] loss: 0.750\n[184,     5] loss: 0.751\n[184,     6] loss: 0.751\n[184,     7] loss: 0.739\n[184,     8] loss: 0.733\n[185,     1] loss: 0.772\n[185,     2] loss: 0.745\n[185,     3] loss: 0.740\n[185,     4] loss: 0.752\n[185,     5] loss: 0.741\n[185,     6] loss: 0.756\n[185,     7] loss: 0.756\n[185,     8] loss: 0.746\n[186,     1] loss: 0.757\n[186,     2] loss: 0.699\n[186,     3] loss: 0.683\n[186,     4] loss: 0.697\n[186,     5] loss: 0.706\n[186,     6] loss: 0.714\n[186,     7] loss: 0.722\n[186,     8] loss: 0.715\n[187,     1] loss: 0.699\n[187,     2] loss: 0.725\n[187,     3] loss: 0.727\n[187,     4] loss: 0.716\n[187,     5] loss: 0.706\n[187,     6] loss: 0.706\n[187,     7] loss: 0.716\n[187,     8] loss: 0.728\n[188,     1] loss: 0.756\n[188,     2] loss: 0.755\n[188,     3] loss: 0.745\n[188,     4] loss: 0.757\n[188,     5] loss: 0.757\n[188,     6] loss: 0.749\n[188,     7] loss: 0.734\n[188,     8] loss: 0.753\n[189,     1] loss: 0.782\n[189,     2] loss: 0.738\n[189,     3] loss: 0.722\n[189,     4] loss: 0.704\n[189,     5] loss: 0.702\n[189,     6] loss: 0.713\n[189,     7] loss: 0.710\n[189,     8] loss: 0.735\n[190,     1] loss: 0.764\n[190,     2] loss: 0.750\n[190,     3] loss: 0.765\n[190,     4] loss: 0.757\n[190,     5] loss: 0.735\n[190,     6] loss: 0.751\n[190,     7] loss: 0.757\n[190,     8] loss: 0.752\n[191,     1] loss: 0.759\n[191,     2] loss: 0.752\n[191,     3] loss: 0.718\n[191,     4] loss: 0.752\n[191,     5] loss: 0.753\n[191,     6] loss: 0.749\n[191,     7] loss: 0.753\n[191,     8] loss: 0.754\n[192,     1] loss: 0.783\n[192,     2] loss: 0.781\n[192,     3] loss: 0.775\n[192,     4] loss: 0.766\n[192,     5] loss: 0.752\n[192,     6] loss: 0.763\n[192,     7] loss: 0.756\n[192,     8] loss: 0.754\n[193,     1] loss: 0.701\n[193,     2] loss: 0.728\n[193,     3] loss: 0.739\n[193,     4] loss: 0.736\n[193,     5] loss: 0.741\n[193,     6] loss: 0.749\n[193,     7] loss: 0.767\n[193,     8] loss: 0.753\n[194,     1] loss: 0.766\n[194,     2] loss: 0.710\n[194,     3] loss: 0.725\n[194,     4] loss: 0.730\n[194,     5] loss: 0.727\n[194,     6] loss: 0.731\n[194,     7] loss: 0.723\n[194,     8] loss: 0.723\n[195,     1] loss: 0.787\n[195,     2] loss: 0.730\n[195,     3] loss: 0.731\n[195,     4] loss: 0.746\n[195,     5] loss: 0.749\n[195,     6] loss: 0.748\n[195,     7] loss: 0.747\n[195,     8] loss: 0.745\n[196,     1] loss: 0.791\n[196,     2] loss: 0.726\n[196,     3] loss: 0.770\n[196,     4] loss: 0.775\n[196,     5] loss: 0.769\n[196,     6] loss: 0.765\n[196,     7] loss: 0.766\n[196,     8] loss: 0.761\n[197,     1] loss: 0.726\n[197,     2] loss: 0.703\n[197,     3] loss: 0.736\n[197,     4] loss: 0.744\n[197,     5] loss: 0.738\n[197,     6] loss: 0.755\n[197,     7] loss: 0.755\n[197,     8] loss: 0.762\n[198,     1] loss: 0.729\n[198,     2] loss: 0.740\n[198,     3] loss: 0.747\n[198,     4] loss: 0.747\n[198,     5] loss: 0.758\n[198,     6] loss: 0.754\n[198,     7] loss: 0.756\n[198,     8] loss: 0.751\n[199,     1] loss: 0.777\n[199,     2] loss: 0.770\n[199,     3] loss: 0.764\n[199,     4] loss: 0.772\n[199,     5] loss: 0.765\n[199,     6] loss: 0.756\n[199,     7] loss: 0.746\n[199,     8] loss: 0.734\n[200,     1] loss: 0.736\n[200,     2] loss: 0.724\n[200,     3] loss: 0.732\n[200,     4] loss: 0.726\n[200,     5] loss: 0.728\n[200,     6] loss: 0.736\n[200,     7] loss: 0.730\n[200,     8] loss: 0.732\n"
    }
   ],
   "source": [
    "# create writer for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
    "logs_path = \"./logs/\" + dt + \"/\"\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass\n",
    "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)\n",
    "\n",
    "\n",
    "# run training\n",
    "for epoch in range(200):\n",
    "  running_loss = 0.0\n",
    "  i = 0\n",
    "  \n",
    "  for X1, X2, y in train_generator:\n",
    "    X1 = X1.to(device)\n",
    "    X2 = X2.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    X1 = X1.view(-1, 1, 105, 105)\n",
    "    X2 = X2.view(-1, 1, 105, 105)\n",
    "    \n",
    "\n",
    "    outputs = siamese(X1, X2)\n",
    "    # print(outputs, outputs.dtype)\n",
    "    # print(outputs.shape)\n",
    "    # print(y.shape)\n",
    "    # print(outputs)\n",
    "    # outputs_ = torch.cat((outputs.view(-1, 1), (1-outputs).view(-1, 1)), dim=1)\n",
    "    loss = criterion(outputs , y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "      writer.add_scalar(\"training loss (per epoch)\", loss.item(), epoch)\n",
    "      writer.add_scalar(\"learning rate (per epoch)\", optim_scheduler.state_dict()[\"_last_lr\"][0], epoch)\n",
    "    \n",
    "    writer.add_scalar(\"training loss\", loss.item(), epoch * len(train_generator) + i)\n",
    "\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / (i+1)))\n",
    "    i+=1\n",
    "  # Update the learning rate\n",
    "  optim_scheduler.step()\n",
    "\n",
    "  #\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gDe5nYvhFQc"
   },
   "outputs": [],
   "source": [
    "### Test to see if labels are correct\n",
    "\n",
    "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
    "\n",
    "batch_size = 1\n",
    "eval_generator = data.DataLoader(eval_set, **{'batch_size': batch_size})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(5, 5))\n",
    "i=1\n",
    "columns = 2\n",
    "rows = batch_size\n",
    "for x1, x2, y in eval_generator:\n",
    "  print(x1.shape)\n",
    "  print(x2.shape)\n",
    "  fig.add_subplot(rows, columns, i)\n",
    "  # img = np.random.randint(10, size=(h,w))\n",
    "  plt.imshow(x1.squeeze())\n",
    "  i+=1\n",
    "  fig.add_subplot(rows, columns, i)\n",
    "  plt.imshow(x2.squeeze())\n",
    "  i+=1\n",
    "  if (y == 1):\n",
    "    print(\"same letter\")\n",
    "  else:\n",
    "    print(\"different letter\")\n",
    "  break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
    "save_path = \"./saves/\" + dt + \"/\"\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass\n",
    "torch.save(siamese, save_path + \"siamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}