{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python38164bitdlvenv8c880a4f959d41a0bd88c28d905ec3ee",
      "display_name": "Python 3.8.1 64-bit ('DL': venv)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2cT7lOrijlJs"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Twhmbb8kXNQ"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5M1FkjdQluR8"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qac1GqFnl58c"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Mcpj2P3l8So"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone\n",
        "\n",
        "amsterdam = timezone('Europe/Amsterdam')\n",
        "datetime_format = '%Y-%m-%d-T-(%H-%M-%S)'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPlU4gaHlY5t"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the netwok architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = F.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = F.sigmoid(self.prediction_layer(h))\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "SiameseNet(\n  (convnet): ConvNet(\n    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n  )\n  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n)"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4P_10kSyZO0p"
      },
      "source": [
        "### Define the Loss (CrossEntropy) and the Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Learning rate decay per epoch\n",
        "lr_decay_rate = 0.99\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "#optimizer = optim.SGD(siamese.parameters(), lr = 0.01, momentum=0.5, weight_decay=2e-4)\n",
        "optimizer = optim.SGD(siamese.parameters(), lr = 0.1, weight_decay=2e-4)\n",
        "optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = lr_decay_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OtINTkF9mZUC"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_GgaTl2Tmvg0"
      },
      "source": [
        "---\n",
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, data_path):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_path: str\n",
        "      Path to the pickle file\n",
        "    \"\"\"\n",
        "    self.data = None\n",
        "    self.alphabet_index = None\n",
        "    with open(data_path, \"rb\") as f:\n",
        "      X, i = pickle.load(f)\n",
        "      self.data = X.astype(\"float32\")\n",
        "      self.alphabet_index = i\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    index: int\n",
        "      index from which to get the data\n",
        "    \"\"\"\n",
        "    # get dimensions of the data\n",
        "    num_letters, num_drawings, height, width = self.data.shape\n",
        "\n",
        "    # initialize index2\n",
        "    # index of second letter from pair same as first\n",
        "    index2 = index\n",
        "\n",
        "    # randomly choose a variant of the letter\n",
        "    drawing_index = np.random.choice(num_drawings)\n",
        "    drawing_index2 = np.random.choice(num_drawings)\n",
        "\n",
        "    # choose image for first letter\n",
        "    X1 = self.data[index, drawing_index, :, :].reshape(width, height)\n",
        "    \n",
        "    # set label to be 1, i.e. same letter\n",
        "    y = np.array([1.0], dtype=\"float32\")\n",
        "\n",
        "    # with 50% probability,\n",
        "    # pick an image of a different letter\n",
        "    # and change the label to 0, i.e. different letter\n",
        "    if np.random.uniform() >= 0.5:\n",
        "      index2 = (index + np.random.randint(1, num_letters)) % num_letters\n",
        "      y = np.array([0.0], dtype=\"float32\")\n",
        "    \n",
        "    # choose image for the second letter\n",
        "    X2 = self.data[index2, drawing_index2, :, :].reshape(width, height)\n",
        "\n",
        "    return X1, X2, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# create the dataset object\n",
        "data_path = \"../../data/processed/\"\n",
        "\n",
        "train_set = Dataset(os.path.join(data_path, \"train.pkl\"))\n",
        "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
        "\n",
        "device = torch.device('cuda')\n",
        "siamese.to(device, dtype=torch.float32)\n",
        "\n",
        "# set parameters for data creation\n",
        "batch_size = 128\n",
        "num_workers = 1\n",
        "\n",
        "params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': num_workers}\n",
        "\n",
        "# create the dataloader object which returns a generator over the data\n",
        "train_generator = data.DataLoader(train_set, **params)\n",
        "eval_generator = data.DataLoader(eval_set, **params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bj8kcukdmc5b"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create writer for tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter \n",
        "\n",
        "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
        "logs_path = \"./logs/\" + dt + \"/\"\n",
        "try:\n",
        "    os.makedirs(save_path)\n",
        "except:\n",
        "    pass\n",
        "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[101,     1] loss: 0.773\n[101,     2] loss: 0.794\n[101,     3] loss: 0.770\n[101,     4] loss: 0.777\n[101,     5] loss: 0.767\n[101,     6] loss: 0.762\n[101,     7] loss: 0.759\n[101,     8] loss: 0.758\n[102,     1] loss: 0.875\n[102,     2] loss: 0.780\n[102,     3] loss: 0.761\n[102,     4] loss: 0.777\n[102,     5] loss: 0.783\n[102,     6] loss: 0.786\n[102,     7] loss: 0.780\n[102,     8] loss: 0.775\n[103,     1] loss: 0.885\n[103,     2] loss: 0.836\n[103,     3] loss: 0.779\n[103,     4] loss: 0.769\n[103,     5] loss: 0.807\n[103,     6] loss: 0.803\n[103,     7] loss: 0.808\n[103,     8] loss: 0.820\n[104,     1] loss: 0.756\n[104,     2] loss: 0.817\n[104,     3] loss: 0.799\n[104,     4] loss: 0.807\n[104,     5] loss: 0.821\n[104,     6] loss: 0.826\n[104,     7] loss: 0.827\n[104,     8] loss: 0.810\n[105,     1] loss: 0.844\n[105,     2] loss: 0.818\n[105,     3] loss: 0.810\n[105,     4] loss: 0.793\n[105,     5] loss: 0.782\n[105,     6] loss: 0.787\n[105,     7] loss: 0.791\n[105,     8] loss: 0.798\n[106,     1] loss: 0.813\n[106,     2] loss: 0.852\n[106,     3] loss: 0.798\n[106,     4] loss: 0.810\n[106,     5] loss: 0.788\n[106,     6] loss: 0.796\n[106,     7] loss: 0.794\n[106,     8] loss: 0.787\n[107,     1] loss: 0.914\n[107,     2] loss: 0.860\n[107,     3] loss: 0.862\n[107,     4] loss: 0.854\n[107,     5] loss: 0.863\n[107,     6] loss: 0.856\n[107,     7] loss: 0.859\n[107,     8] loss: 0.865\n[108,     1] loss: 0.877\n[108,     2] loss: 0.853\n[108,     3] loss: 0.838\n[108,     4] loss: 0.814\n[108,     5] loss: 0.801\n[108,     6] loss: 0.809\n[108,     7] loss: 0.813\n[108,     8] loss: 0.816\n[109,     1] loss: 0.805\n[109,     2] loss: 0.806\n[109,     3] loss: 0.791\n[109,     4] loss: 0.811\n[109,     5] loss: 0.810\n[109,     6] loss: 0.799\n[109,     7] loss: 0.794\n[109,     8] loss: 0.778\n[110,     1] loss: 0.847\n[110,     2] loss: 0.798\n[110,     3] loss: 0.766\n[110,     4] loss: 0.791\n[110,     5] loss: 0.774\n[110,     6] loss: 0.762\n[110,     7] loss: 0.759\n[110,     8] loss: 0.757\n[111,     1] loss: 0.899\n[111,     2] loss: 0.864\n[111,     3] loss: 0.835\n[111,     4] loss: 0.829\n[111,     5] loss: 0.816\n[111,     6] loss: 0.825\n[111,     7] loss: 0.834\n[111,     8] loss: 0.840\n[112,     1] loss: 0.798\n[112,     2] loss: 0.778\n[112,     3] loss: 0.804\n[112,     4] loss: 0.790\n[112,     5] loss: 0.814\n[112,     6] loss: 0.822\n[112,     7] loss: 0.841\n[112,     8] loss: 0.842\n[113,     1] loss: 0.886\n[113,     2] loss: 0.791\n[113,     3] loss: 0.805\n[113,     4] loss: 0.811\n[113,     5] loss: 0.810\n[113,     6] loss: 0.808\n[113,     7] loss: 0.809\n[113,     8] loss: 0.811\n[114,     1] loss: 0.871\n[114,     2] loss: 0.751\n[114,     3] loss: 0.737\n[114,     4] loss: 0.748\n[114,     5] loss: 0.746\n[114,     6] loss: 0.762\n[114,     7] loss: 0.768\n[114,     8] loss: 0.781\n[115,     1] loss: 0.867\n[115,     2] loss: 0.784\n[115,     3] loss: 0.779\n[115,     4] loss: 0.812\n[115,     5] loss: 0.814\n[115,     6] loss: 0.823\n[115,     7] loss: 0.844\n[115,     8] loss: 0.860\n[116,     1] loss: 1.026\n[116,     2] loss: 1.026\n[116,     3] loss: 0.984\n[116,     4] loss: 0.986\n[116,     5] loss: 0.976\n[116,     6] loss: 0.973\n[116,     7] loss: 0.971\n[116,     8] loss: 0.954\n[117,     1] loss: 0.897\n[117,     2] loss: 0.941\n[117,     3] loss: 0.906\n[117,     4] loss: 0.872\n[117,     5] loss: 0.859\n[117,     6] loss: 0.846\n[117,     7] loss: 0.858\n[117,     8] loss: 0.860\n[118,     1] loss: 0.834\n[118,     2] loss: 0.942\n[118,     3] loss: 0.939\n[118,     4] loss: 0.903\n[118,     5] loss: 0.907\n[118,     6] loss: 0.886\n[118,     7] loss: 0.899\n[118,     8] loss: 0.908\n[119,     1] loss: 0.906\n[119,     2] loss: 0.952\n[119,     3] loss: 0.930\n[119,     4] loss: 0.911\n[119,     5] loss: 0.881\n[119,     6] loss: 0.880\n[119,     7] loss: 0.866\n[119,     8] loss: 0.884\n[120,     1] loss: 0.852\n[120,     2] loss: 0.897\n[120,     3] loss: 0.922\n[120,     4] loss: 0.908\n[120,     5] loss: 0.920\n[120,     6] loss: 0.918\n[120,     7] loss: 0.909\n[120,     8] loss: 0.922\n[121,     1] loss: 0.997\n[121,     2] loss: 0.953\n[121,     3] loss: 0.920\n[121,     4] loss: 0.914\n[121,     5] loss: 0.922\n[121,     6] loss: 0.902\n[121,     7] loss: 0.906\n[121,     8] loss: 0.925\n[122,     1] loss: 0.878\n[122,     2] loss: 0.883\n[122,     3] loss: 0.863\n[122,     4] loss: 0.905\n[122,     5] loss: 0.891\n[122,     6] loss: 0.895\n[122,     7] loss: 0.884\n[122,     8] loss: 0.873\n[123,     1] loss: 0.954\n[123,     2] loss: 0.971\n[123,     3] loss: 0.936\n[123,     4] loss: 0.891\n[123,     5] loss: 0.881\n[123,     6] loss: 0.877\n[123,     7] loss: 0.876\n[123,     8] loss: 0.880\n[124,     1] loss: 0.944\n[124,     2] loss: 0.930\n[124,     3] loss: 0.889\n[124,     4] loss: 0.895\n[124,     5] loss: 0.908\n[124,     6] loss: 0.912\n[124,     7] loss: 0.888\n[124,     8] loss: 0.895\n[125,     1] loss: 0.844\n[125,     2] loss: 0.859\n[125,     3] loss: 0.824\n[125,     4] loss: 0.838\n[125,     5] loss: 0.850\n[125,     6] loss: 0.876\n[125,     7] loss: 0.881\n[125,     8] loss: 0.873\n[126,     1] loss: 0.857\n[126,     2] loss: 0.842\n[126,     3] loss: 0.850\n[126,     4] loss: 0.864\n[126,     5] loss: 0.868\n[126,     6] loss: 0.866\n[126,     7] loss: 0.872\n[126,     8] loss: 0.862\n[127,     1] loss: 0.915\n[127,     2] loss: 0.931\n[127,     3] loss: 0.867\n[127,     4] loss: 0.848\n[127,     5] loss: 0.844\n[127,     6] loss: 0.840\n[127,     7] loss: 0.829\n[127,     8] loss: 0.844\n[128,     1] loss: 0.780\n[128,     2] loss: 0.833\n[128,     3] loss: 0.854\n[128,     4] loss: 0.842\n[128,     5] loss: 0.828\n[128,     6] loss: 0.821\n[128,     7] loss: 0.815\n[128,     8] loss: 0.829\n[129,     1] loss: 0.820\n[129,     2] loss: 0.791\n[129,     3] loss: 0.775\n[129,     4] loss: 0.765\n[129,     5] loss: 0.775\n[129,     6] loss: 0.780\n[129,     7] loss: 0.775\n[129,     8] loss: 0.787\n[130,     1] loss: 0.857\n[130,     2] loss: 0.872\n[130,     3] loss: 0.883\n[130,     4] loss: 0.866\n[130,     5] loss: 0.856\n[130,     6] loss: 0.841\n[130,     7] loss: 0.852\n[130,     8] loss: 0.865\n[131,     1] loss: 0.839\n[131,     2] loss: 0.846\n[131,     3] loss: 0.844\n[131,     4] loss: 0.842\n[131,     5] loss: 0.823\n[131,     6] loss: 0.831\n[131,     7] loss: 0.833\n[131,     8] loss: 0.832\n[132,     1] loss: 0.711\n[132,     2] loss: 0.724\n[132,     3] loss: 0.737\n[132,     4] loss: 0.762\n[132,     5] loss: 0.748\n[132,     6] loss: 0.744\n[132,     7] loss: 0.757\n[132,     8] loss: 0.755\n[133,     1] loss: 0.794\n[133,     2] loss: 0.854\n[133,     3] loss: 0.820\n[133,     4] loss: 0.835\n[133,     5] loss: 0.830\n[133,     6] loss: 0.840\n[133,     7] loss: 0.835\n[133,     8] loss: 0.834\n[134,     1] loss: 0.862\n[134,     2] loss: 0.821\n[134,     3] loss: 0.876\n[134,     4] loss: 0.855\n[134,     5] loss: 0.836\n[134,     6] loss: 0.834\n[134,     7] loss: 0.822\n[134,     8] loss: 0.831\n[135,     1] loss: 0.862\n[135,     2] loss: 0.750\n[135,     3] loss: 0.733\n[135,     4] loss: 0.755\n[135,     5] loss: 0.738\n[135,     6] loss: 0.751\n[135,     7] loss: 0.750\n[135,     8] loss: 0.736\n[136,     1] loss: 0.829\n[136,     2] loss: 0.798\n[136,     3] loss: 0.827\n[136,     4] loss: 0.803\n[136,     5] loss: 0.792\n[136,     6] loss: 0.813\n[136,     7] loss: 0.796\n[136,     8] loss: 0.776\n[137,     1] loss: 0.823\n[137,     2] loss: 0.779\n[137,     3] loss: 0.796\n[137,     4] loss: 0.786\n[137,     5] loss: 0.778\n[137,     6] loss: 0.795\n[137,     7] loss: 0.798\n[137,     8] loss: 0.813\n[138,     1] loss: 0.961\n[138,     2] loss: 0.832\n[138,     3] loss: 0.804\n[138,     4] loss: 0.818\n[138,     5] loss: 0.813\n[138,     6] loss: 0.812\n[138,     7] loss: 0.807\n[138,     8] loss: 0.792\n[139,     1] loss: 0.814\n[139,     2] loss: 0.754\n[139,     3] loss: 0.768\n[139,     4] loss: 0.764\n[139,     5] loss: 0.771\n[139,     6] loss: 0.778\n[139,     7] loss: 0.783\n[139,     8] loss: 0.774\n[140,     1] loss: 0.888\n[140,     2] loss: 0.809\n[140,     3] loss: 0.780\n[140,     4] loss: 0.807\n[140,     5] loss: 0.836\n[140,     6] loss: 0.827\n[140,     7] loss: 0.820\n[140,     8] loss: 0.812\n[141,     1] loss: 0.771\n[141,     2] loss: 0.754\n[141,     3] loss: 0.791\n[141,     4] loss: 0.787\n[141,     5] loss: 0.770\n[141,     6] loss: 0.771\n[141,     7] loss: 0.776\n[141,     8] loss: 0.782\n[142,     1] loss: 0.768\n[142,     2] loss: 0.822\n[142,     3] loss: 0.815\n[142,     4] loss: 0.836\n[142,     5] loss: 0.837\n[142,     6] loss: 0.829\n[142,     7] loss: 0.816\n[142,     8] loss: 0.821\n[143,     1] loss: 0.758\n[143,     2] loss: 0.776\n[143,     3] loss: 0.766\n[143,     4] loss: 0.747\n[143,     5] loss: 0.767\n[143,     6] loss: 0.760\n[143,     7] loss: 0.749\n[143,     8] loss: 0.743\n[144,     1] loss: 0.776\n[144,     2] loss: 0.784\n[144,     3] loss: 0.777\n[144,     4] loss: 0.794\n[144,     5] loss: 0.789\n[144,     6] loss: 0.789\n[144,     7] loss: 0.795\n[144,     8] loss: 0.801\n[145,     1] loss: 0.829\n[145,     2] loss: 0.771\n[145,     3] loss: 0.790\n[145,     4] loss: 0.800\n[145,     5] loss: 0.833\n[145,     6] loss: 0.827\n[145,     7] loss: 0.840\n[145,     8] loss: 0.832\n[146,     1] loss: 0.945\n[146,     2] loss: 0.856\n[146,     3] loss: 0.855\n[146,     4] loss: 0.827\n[146,     5] loss: 0.829\n[146,     6] loss: 0.836\n[146,     7] loss: 0.832\n[146,     8] loss: 0.823\n[147,     1] loss: 0.888\n[147,     2] loss: 0.862\n[147,     3] loss: 0.800\n[147,     4] loss: 0.773\n[147,     5] loss: 0.792\n[147,     6] loss: 0.786\n[147,     7] loss: 0.784\n[147,     8] loss: 0.793\n[148,     1] loss: 0.764\n[148,     2] loss: 0.742\n[148,     3] loss: 0.760\n[148,     4] loss: 0.739\n[148,     5] loss: 0.755\n[148,     6] loss: 0.780\n[148,     7] loss: 0.805\n[148,     8] loss: 0.834\n[149,     1] loss: 0.980\n[149,     2] loss: 0.970\n[149,     3] loss: 0.958\n[149,     4] loss: 0.926\n[149,     5] loss: 0.900\n[149,     6] loss: 0.887\n[149,     7] loss: 0.865\n[149,     8] loss: 0.859\n[150,     1] loss: 0.722\n[150,     2] loss: 0.723\n[150,     3] loss: 0.748\n[150,     4] loss: 0.769\n[150,     5] loss: 0.781\n[150,     6] loss: 0.769\n[150,     7] loss: 0.769\n[150,     8] loss: 0.776\n[151,     1] loss: 0.781\n[151,     2] loss: 0.774\n[151,     3] loss: 0.751\n[151,     4] loss: 0.770\n[151,     5] loss: 0.777\n[151,     6] loss: 0.772\n[151,     7] loss: 0.766\n[151,     8] loss: 0.763\n[152,     1] loss: 0.917\n[152,     2] loss: 0.819\n[152,     3] loss: 0.807\n[152,     4] loss: 0.793\n[152,     5] loss: 0.802\n[152,     6] loss: 0.792\n[152,     7] loss: 0.792\n[152,     8] loss: 0.780\n[153,     1] loss: 0.810\n[153,     2] loss: 0.794\n[153,     3] loss: 0.808\n[153,     4] loss: 0.810\n[153,     5] loss: 0.807\n[153,     6] loss: 0.812\n[153,     7] loss: 0.803\n[153,     8] loss: 0.809\n[154,     1] loss: 0.681\n[154,     2] loss: 0.731\n[154,     3] loss: 0.739\n[154,     4] loss: 0.735\n[154,     5] loss: 0.742\n[154,     6] loss: 0.749\n[154,     7] loss: 0.757\n[154,     8] loss: 0.756\n[155,     1] loss: 0.883\n[155,     2] loss: 0.812\n[155,     3] loss: 0.826\n[155,     4] loss: 0.816\n[155,     5] loss: 0.793\n[155,     6] loss: 0.795\n[155,     7] loss: 0.790\n[155,     8] loss: 0.793\n[156,     1] loss: 0.744\n[156,     2] loss: 0.786\n[156,     3] loss: 0.781\n[156,     4] loss: 0.790\n[156,     5] loss: 0.787\n[156,     6] loss: 0.786\n[156,     7] loss: 0.796\n[156,     8] loss: 0.812\n[157,     1] loss: 0.810\n[157,     2] loss: 0.790\n[157,     3] loss: 0.799\n[157,     4] loss: 0.790\n[157,     5] loss: 0.781\n[157,     6] loss: 0.774\n[157,     7] loss: 0.788\n[157,     8] loss: 0.789\n[158,     1] loss: 0.700\n[158,     2] loss: 0.675\n[158,     3] loss: 0.719\n[158,     4] loss: 0.739\n[158,     5] loss: 0.748\n[158,     6] loss: 0.741\n[158,     7] loss: 0.752\n[158,     8] loss: 0.765\n[159,     1] loss: 0.820\n[159,     2] loss: 0.743\n[159,     3] loss: 0.771\n[159,     4] loss: 0.787\n[159,     5] loss: 0.763\n[159,     6] loss: 0.771\n[159,     7] loss: 0.772\n[159,     8] loss: 0.770\n[160,     1] loss: 0.838\n[160,     2] loss: 0.772\n[160,     3] loss: 0.776\n[160,     4] loss: 0.776\n[160,     5] loss: 0.755\n[160,     6] loss: 0.767\n[160,     7] loss: 0.762\n[160,     8] loss: 0.766\n[161,     1] loss: 0.756\n[161,     2] loss: 0.761\n[161,     3] loss: 0.737\n[161,     4] loss: 0.712\n[161,     5] loss: 0.719\n[161,     6] loss: 0.730\n[161,     7] loss: 0.733\n[161,     8] loss: 0.738\n[162,     1] loss: 0.824\n[162,     2] loss: 0.798\n[162,     3] loss: 0.821\n[162,     4] loss: 0.797\n[162,     5] loss: 0.786\n[162,     6] loss: 0.790\n[162,     7] loss: 0.792\n[162,     8] loss: 0.779\n[163,     1] loss: 0.739\n[163,     2] loss: 0.743\n[163,     3] loss: 0.753\n[163,     4] loss: 0.786\n[163,     5] loss: 0.758\n[163,     6] loss: 0.758\n[163,     7] loss: 0.772\n[163,     8] loss: 0.777\n[164,     1] loss: 0.712\n[164,     2] loss: 0.697\n[164,     3] loss: 0.737\n[164,     4] loss: 0.725\n[164,     5] loss: 0.722\n[164,     6] loss: 0.726\n[164,     7] loss: 0.737\n[164,     8] loss: 0.744\n[165,     1] loss: 0.796\n[165,     2] loss: 0.741\n[165,     3] loss: 0.749\n[165,     4] loss: 0.741\n[165,     5] loss: 0.757\n[165,     6] loss: 0.765\n[165,     7] loss: 0.775\n[165,     8] loss: 0.767\n[166,     1] loss: 0.788\n[166,     2] loss: 0.771\n[166,     3] loss: 0.790\n[166,     4] loss: 0.793\n[166,     5] loss: 0.787\n[166,     6] loss: 0.811\n[166,     7] loss: 0.808\n[166,     8] loss: 0.783\n[167,     1] loss: 0.780\n[167,     2] loss: 0.721\n[167,     3] loss: 0.707\n[167,     4] loss: 0.731\n[167,     5] loss: 0.737\n[167,     6] loss: 0.750\n[167,     7] loss: 0.750\n[167,     8] loss: 0.747\n[168,     1] loss: 0.748\n[168,     2] loss: 0.726\n[168,     3] loss: 0.732\n[168,     4] loss: 0.748\n[168,     5] loss: 0.742\n[168,     6] loss: 0.764\n[168,     7] loss: 0.783\n[168,     8] loss: 0.780\n[169,     1] loss: 0.762\n[169,     2] loss: 0.721\n[169,     3] loss: 0.749\n[169,     4] loss: 0.755\n[169,     5] loss: 0.746\n[169,     6] loss: 0.760\n[169,     7] loss: 0.768\n[169,     8] loss: 0.775\n[170,     1] loss: 0.813\n[170,     2] loss: 0.757\n[170,     3] loss: 0.731\n[170,     4] loss: 0.768\n[170,     5] loss: 0.772\n[170,     6] loss: 0.781\n[170,     7] loss: 0.776\n[170,     8] loss: 0.779\n[171,     1] loss: 0.736\n[171,     2] loss: 0.708\n[171,     3] loss: 0.734\n[171,     4] loss: 0.734\n[171,     5] loss: 0.742\n[171,     6] loss: 0.748\n[171,     7] loss: 0.768\n[171,     8] loss: 0.789\n[172,     1] loss: 0.826\n[172,     2] loss: 0.754\n[172,     3] loss: 0.752\n[172,     4] loss: 0.789\n[172,     5] loss: 0.777\n[172,     6] loss: 0.776\n[172,     7] loss: 0.771\n[172,     8] loss: 0.768\n[173,     1] loss: 0.830\n[173,     2] loss: 0.769\n[173,     3] loss: 0.775\n[173,     4] loss: 0.781\n[173,     5] loss: 0.770\n[173,     6] loss: 0.752\n[173,     7] loss: 0.759\n[173,     8] loss: 0.768\n[174,     1] loss: 0.770\n[174,     2] loss: 0.706\n[174,     3] loss: 0.718\n[174,     4] loss: 0.714\n[174,     5] loss: 0.723\n[174,     6] loss: 0.739\n[174,     7] loss: 0.747\n[174,     8] loss: 0.738\n[175,     1] loss: 0.804\n[175,     2] loss: 0.733\n[175,     3] loss: 0.731\n[175,     4] loss: 0.737\n[175,     5] loss: 0.737\n[175,     6] loss: 0.747\n[175,     7] loss: 0.740\n[175,     8] loss: 0.756\n[176,     1] loss: 0.720\n[176,     2] loss: 0.742\n[176,     3] loss: 0.753\n[176,     4] loss: 0.765\n[176,     5] loss: 0.754\n[176,     6] loss: 0.757\n[176,     7] loss: 0.755\n[176,     8] loss: 0.760\n[177,     1] loss: 0.708\n[177,     2] loss: 0.715\n[177,     3] loss: 0.720\n[177,     4] loss: 0.734\n[177,     5] loss: 0.741\n[177,     6] loss: 0.743\n[177,     7] loss: 0.744\n[177,     8] loss: 0.748\n[178,     1] loss: 0.804\n[178,     2] loss: 0.783\n[178,     3] loss: 0.791\n[178,     4] loss: 0.803\n[178,     5] loss: 0.784\n[178,     6] loss: 0.785\n[178,     7] loss: 0.773\n[178,     8] loss: 0.763\n[179,     1] loss: 0.738\n[179,     2] loss: 0.678\n[179,     3] loss: 0.678\n[179,     4] loss: 0.693\n[179,     5] loss: 0.691\n[179,     6] loss: 0.707\n[179,     7] loss: 0.706\n[179,     8] loss: 0.699\n[180,     1] loss: 0.835\n[180,     2] loss: 0.725\n[180,     3] loss: 0.739\n[180,     4] loss: 0.755\n[180,     5] loss: 0.748\n[180,     6] loss: 0.750\n[180,     7] loss: 0.745\n[180,     8] loss: 0.732\n[181,     1] loss: 0.692\n[181,     2] loss: 0.692\n[181,     3] loss: 0.704\n[181,     4] loss: 0.721\n[181,     5] loss: 0.709\n[181,     6] loss: 0.721\n[181,     7] loss: 0.734\n[181,     8] loss: 0.731\n[182,     1] loss: 0.685\n[182,     2] loss: 0.684\n[182,     3] loss: 0.695\n[182,     4] loss: 0.700\n[182,     5] loss: 0.697\n[182,     6] loss: 0.700\n[182,     7] loss: 0.715\n[182,     8] loss: 0.702\n[183,     1] loss: 0.685\n[183,     2] loss: 0.678\n[183,     3] loss: 0.696\n[183,     4] loss: 0.710\n[183,     5] loss: 0.723\n[183,     6] loss: 0.727\n[183,     7] loss: 0.728\n[183,     8] loss: 0.734\n[184,     1] loss: 0.793\n[184,     2] loss: 0.747\n[184,     3] loss: 0.757\n[184,     4] loss: 0.750\n[184,     5] loss: 0.751\n[184,     6] loss: 0.751\n[184,     7] loss: 0.739\n[184,     8] loss: 0.733\n[185,     1] loss: 0.772\n[185,     2] loss: 0.745\n[185,     3] loss: 0.740\n[185,     4] loss: 0.752\n[185,     5] loss: 0.741\n[185,     6] loss: 0.756\n[185,     7] loss: 0.756\n[185,     8] loss: 0.746\n[186,     1] loss: 0.757\n[186,     2] loss: 0.699\n[186,     3] loss: 0.683\n[186,     4] loss: 0.697\n[186,     5] loss: 0.706\n[186,     6] loss: 0.714\n[186,     7] loss: 0.722\n[186,     8] loss: 0.715\n[187,     1] loss: 0.699\n[187,     2] loss: 0.725\n[187,     3] loss: 0.727\n[187,     4] loss: 0.716\n[187,     5] loss: 0.706\n[187,     6] loss: 0.706\n[187,     7] loss: 0.716\n[187,     8] loss: 0.728\n[188,     1] loss: 0.756\n[188,     2] loss: 0.755\n[188,     3] loss: 0.745\n[188,     4] loss: 0.757\n[188,     5] loss: 0.757\n[188,     6] loss: 0.749\n[188,     7] loss: 0.734\n[188,     8] loss: 0.753\n[189,     1] loss: 0.782\n[189,     2] loss: 0.738\n[189,     3] loss: 0.722\n[189,     4] loss: 0.704\n[189,     5] loss: 0.702\n[189,     6] loss: 0.713\n[189,     7] loss: 0.710\n[189,     8] loss: 0.735\n[190,     1] loss: 0.764\n[190,     2] loss: 0.750\n[190,     3] loss: 0.765\n[190,     4] loss: 0.757\n[190,     5] loss: 0.735\n[190,     6] loss: 0.751\n[190,     7] loss: 0.757\n[190,     8] loss: 0.752\n[191,     1] loss: 0.759\n[191,     2] loss: 0.752\n[191,     3] loss: 0.718\n[191,     4] loss: 0.752\n[191,     5] loss: 0.753\n[191,     6] loss: 0.749\n[191,     7] loss: 0.753\n[191,     8] loss: 0.754\n[192,     1] loss: 0.783\n[192,     2] loss: 0.781\n[192,     3] loss: 0.775\n[192,     4] loss: 0.766\n[192,     5] loss: 0.752\n[192,     6] loss: 0.763\n[192,     7] loss: 0.756\n[192,     8] loss: 0.754\n[193,     1] loss: 0.701\n[193,     2] loss: 0.728\n[193,     3] loss: 0.739\n[193,     4] loss: 0.736\n[193,     5] loss: 0.741\n[193,     6] loss: 0.749\n[193,     7] loss: 0.767\n[193,     8] loss: 0.753\n[194,     1] loss: 0.766\n[194,     2] loss: 0.710\n[194,     3] loss: 0.725\n[194,     4] loss: 0.730\n[194,     5] loss: 0.727\n[194,     6] loss: 0.731\n[194,     7] loss: 0.723\n[194,     8] loss: 0.723\n[195,     1] loss: 0.787\n[195,     2] loss: 0.730\n[195,     3] loss: 0.731\n[195,     4] loss: 0.746\n[195,     5] loss: 0.749\n[195,     6] loss: 0.748\n[195,     7] loss: 0.747\n[195,     8] loss: 0.745\n[196,     1] loss: 0.791\n[196,     2] loss: 0.726\n[196,     3] loss: 0.770\n[196,     4] loss: 0.775\n[196,     5] loss: 0.769\n[196,     6] loss: 0.765\n[196,     7] loss: 0.766\n[196,     8] loss: 0.761\n[197,     1] loss: 0.726\n[197,     2] loss: 0.703\n[197,     3] loss: 0.736\n[197,     4] loss: 0.744\n[197,     5] loss: 0.738\n[197,     6] loss: 0.755\n[197,     7] loss: 0.755\n[197,     8] loss: 0.762\n[198,     1] loss: 0.729\n[198,     2] loss: 0.740\n[198,     3] loss: 0.747\n[198,     4] loss: 0.747\n[198,     5] loss: 0.758\n[198,     6] loss: 0.754\n[198,     7] loss: 0.756\n[198,     8] loss: 0.751\n[199,     1] loss: 0.777\n[199,     2] loss: 0.770\n[199,     3] loss: 0.764\n[199,     4] loss: 0.772\n[199,     5] loss: 0.765\n[199,     6] loss: 0.756\n[199,     7] loss: 0.746\n[199,     8] loss: 0.734\n[200,     1] loss: 0.736\n[200,     2] loss: 0.724\n[200,     3] loss: 0.732\n[200,     4] loss: 0.726\n[200,     5] loss: 0.728\n[200,     6] loss: 0.736\n[200,     7] loss: 0.730\n[200,     8] loss: 0.732\n"
        }
      ],
      "source": [
        "# create writer for tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter \n",
        "\n",
        "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
        "logs_path = \"./logs/\" + dt + \"/\"\n",
        "try:\n",
        "    os.makedirs(save_path)\n",
        "except:\n",
        "    pass\n",
        "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)\n",
        "\n",
        "\n",
        "# run training\n",
        "for epoch in range(200):\n",
        "  running_loss = 0.0\n",
        "  i = 0\n",
        "  \n",
        "  for X1, X2, y in train_generator:\n",
        "    X1 = X1.to(device)\n",
        "    X2 = X2.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    X1 = X1.view(-1, 1, 105, 105)\n",
        "    X2 = X2.view(-1, 1, 105, 105)\n",
        "    \n",
        "\n",
        "    outputs = siamese(X1, X2)\n",
        "    # print(outputs, outputs.dtype)\n",
        "    # print(outputs.shape)\n",
        "    # print(y.shape)\n",
        "    # print(outputs)\n",
        "    # outputs_ = torch.cat((outputs.view(-1, 1), (1-outputs).view(-1, 1)), dim=1)\n",
        "    loss = criterion(outputs , y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "\n",
        "    if i == 0:\n",
        "      writer.add_scalar(\"training loss (per epoch)\", loss.item(), epoch)\n",
        "      writer.add_scalar(\"learning rate (per epoch)\", optim_scheduler.state_dict()[\"_last_lr\"][0], epoch)\n",
        "    \n",
        "    writer.add_scalar(\"training loss\", loss.item(), epoch * len(train_generator) + i)\n",
        "\n",
        "    print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss / (i+1)))\n",
        "    i+=1\n",
        "  # Update the learning rate\n",
        "  optim_scheduler.step()\n",
        "\n",
        "  #\n",
        "    \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "torch.Size([1, 105, 105])\ntorch.Size([1, 105, 105])\ndifferent letter\n"
        },
        {
          "data": {
            "image/png": "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\n",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"161.091629pt\" version=\"1.1\" viewBox=\"0 0 323.596185 161.091629\" width=\"323.596185pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 161.091629 \nL 323.596185 161.091629 \nL 323.596185 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 137.213504 \nL 160.105682 137.213504 \nL 160.105682 10.395323 \nL 33.2875 10.395323 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p577101f59f)\">\n    <image height=\"127\" id=\"image901edfc97d\" transform=\"scale(1 -1)translate(0 -127)\" width=\"127\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAH8AAAB/CAYAAADGvR0TAAAABHNCSVQICAgIfAhkiAAAAtxJREFUeJztndttAjEQAEmUKvhOAXRBzekiVSRVRMkXUsTjOLDX3r2Z+UWcDs+ud7HN8fLz/f67EySvs29A5qF8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoH8zb7BkZy3B9Crvvx9Rly3WheqjyWJUpcNJkDw2k/mOP+kDZwS8jPOnjVKSFfYlA+mNBu/zRdZ2l6Iu6jckkK6fZvDcizg9/7elHcC4Rs99st8ytnwCiO+0OqALDmg1md+Wb2fU5ZXWWszPwAMk3tS4Sv7VcZCCJ3M79leVLxl2QqCTflt65J08VX+PzWfDAXNd8pnsNF5reswmXevpRLrk77H1+fTZlsICyTZVxWr+33uOHea/st1xxF5n2J1Q1f62yw2z3/DSLDQG0Ru30wD6/wnWfhM5mcbXeLSvPy7jWJS3Xu9JoBMJ+Qaf+W1POg8BvBXMJq/qlBXJPdBsAchjR8a4LAWWA8dvtghv5W73/Dd4vz120K4xie+Y/KtBzEMWXaN5tzMK3m9wgAg6gNGz4wUx/O0GOpWJ4n1ZM5SMGQ4XeMqaf9LdT0HlvhUaSWv0TWAa1EqmmfSOvuZsspp7KZL+0ofxBLWTirsVU+mOE1P/Np1miWNrZmnGwy8weTKchTd/tbXuTpRUswmflg0mT+I1meaersyei6b+ZPIEvwpj/J0/v92Rl5cqnMSZ7MGyRVKXGSZ4vSMwSzNR9MmX/a2Coznz1g5k/GkzxyleiuX/lglA9G+QmYddBD+QWICgDlJ2FG159mV49GhrMKZn4RIjZ8lA9G+cXomf3KB6P8gvTKfnf1wJj5YJQPRvlglA9G+WCUD0b5YJQPRvlglA9G+WCUD0b5YJQPRvlglA9G+WCUD0b5YJQPRvlglA9G+WCUD0b5YJQPRvlg/gD+beohHMU2zAAAAABJRU5ErkJggg==\" y=\"-10.213504\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf83a97c31c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.891396\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(30.710146 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.281006\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(87.918506 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.670617\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(145.126867 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb8ef8e3044\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"35.155063\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(13.5625 38.954282)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"59.310907\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(13.5625 63.110126)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"83.466751\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(13.5625 87.26597)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"107.622595\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(13.5625 111.421814)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"131.77844\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 135.577658)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 137.213504 \nL 33.2875 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 160.105682 137.213504 \nL 160.105682 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 137.213504 \nL 160.105682 137.213504 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.395323 \nL 160.105682 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 185.469318 137.213504 \nL 312.2875 137.213504 \nL 312.2875 10.395323 \nL 185.469318 10.395323 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd07201a05d)\">\n    <image height=\"127\" id=\"image7d39380896\" transform=\"scale(1 -1)translate(0 -127)\" width=\"127\" x=\"185.469318\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAH8AAAB/CAYAAADGvR0TAAAABHNCSVQICAgIfAhkiAAAAsJJREFUeJzt3c1t20AURWE6cBVapwB1oZrVhaqQqzCSlQBBIKk/aubdOefbO6F55g0nhGN9/f78/TcJ6U/vC1A/xgczPpjxwYwPZnww44MZH8z4YMYHMz6Y8cGMD2Z8MOODGR/M+GDGBzM+mPHBjA9mfDDjgxkfzPhgxgczPpjxwYwPZnww44MZH8z4YN+9L4DusNu/9HXH8+ntv/vLX8vSxquR17y7AJz8N30iaivGf0By4DUe+MDQk5880Vsc+CLjJ0e7tUXEV5WOP1Lkaeobek63+COFrRb1UR74wIz/gLXJTp36aSr+zP+05HBbiIx/HW3p7NAq7GG3j11E5eOn3tgE3eKnRT2eT0P9C2WaPPChGR/M+BtIfRxEx29909POKfdEx18yWqRPGTJ+D4lbv/HBjP+kkR4psfF7v9YdQWz8ig67fdSz3/gvOJ5PQ+wwkfGrT1f167uIjK9tDBV/hK24paHit7a22BK2/rj4CTc1RVz8Jb22/OTpHya+nmf8DaQeNMv/AOdF9S00UUz8JalTV0HEtp889ZXf90fEX+LUvyc6vt5j/I0k7kLlD3xVn5fXEq5xTun41W9q9eu7p3T8NT222fTYt2Ljt7BF7MpnAQ98YJGTX3maLhKuMTJ+RQmxb8Vt+xVvcsVreoST/4LU2Ldi4ve44aNEXhK37Ws7xgeL2PYTtt9nXwhV+J6axB/tf9Ru8ebv8mf4K9dXVFggo73Tv/CZD9Z18q8nqsKET9O4Uz6nzLY/d9NbLIhesSss9jLx56RNYYWgz2j6iZppMe9Ji32ry8epJi6C9NBzPO2Dlfgg5Wo7wYhTPqdE/CUtFgUl9JzS8fVZPvPBjA9mfDDjgxkfzPhgxgczPpjxwYwPZnww44MZH8z4YMYHMz6Y8cGMD2Z8MOODGR/M+GDGBzM+mPHBjA9mfDDjg/0HZiau+Z2XyLoAAAAASUVORK5CYII=\" y=\"-10.213504\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.073214\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(182.891964 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.462825\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 50 -->\n      <g transform=\"translate(240.100325 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.852435\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 100 -->\n      <g transform=\"translate(297.308685 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g transform=\"translate(172.106818 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"35.155063\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(165.744318 38.954282)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"59.310907\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 40 -->\n      <g transform=\"translate(165.744318 63.110126)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"83.466751\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 60 -->\n      <g transform=\"translate(165.744318 87.26597)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"107.622595\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 80 -->\n      <g transform=\"translate(165.744318 111.421814)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"131.77844\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 100 -->\n      <g transform=\"translate(159.381818 135.577658)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 185.469318 137.213504 \nL 185.469318 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 312.2875 137.213504 \nL 312.2875 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 185.469318 137.213504 \nL 312.2875 137.213504 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 185.469318 10.395323 \nL 312.2875 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p577101f59f\">\n   <rect height=\"126.818182\" width=\"126.818182\" x=\"33.2875\" y=\"10.395323\"/>\n  </clipPath>\n  <clipPath id=\"pd07201a05d\">\n   <rect height=\"126.818182\" width=\"126.818182\" x=\"185.469318\" y=\"10.395323\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": "<Figure size 360x360 with 2 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Test to see if labels are correct\n",
        "\n",
        "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
        "\n",
        "batch_size = 1\n",
        "eval_generator = data.DataLoader(eval_set, **{'batch_size': batch_size})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure(figsize=(5, 5))\n",
        "i=1\n",
        "columns = 2\n",
        "rows = batch_size\n",
        "for x1, x2, y in eval_generator:\n",
        "  print(x1.shape)\n",
        "  print(x2.shape)\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  # img = np.random.randint(10, size=(h,w))\n",
        "  plt.imshow(x1.squeeze())\n",
        "  i+=1\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.imshow(x2.squeeze())\n",
        "  i+=1\n",
        "  if (y == 1):\n",
        "    print(\"same letter\")\n",
        "  else:\n",
        "    print(\"different letter\")\n",
        "  break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cI4oWq6bmz5Z"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8gDe5nYvhFQc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
        "save_path = \"./saves/\" + dt + \"/\"\n",
        "try:\n",
        "    os.makedirs(save_path)\n",
        "except:\n",
        "    pass\n",
        "torch.save(siamese, save_path + \"siamese\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}