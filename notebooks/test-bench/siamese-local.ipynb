{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python38164bitdlvenv8c880a4f959d41a0bd88c28d905ec3ee",
      "display_name": "Python 3.8.1 64-bit ('DL': venv)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2cT7lOrijlJs"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Twhmbb8kXNQ"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5M1FkjdQluR8"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qac1GqFnl58c"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Mcpj2P3l8So"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone\n",
        "\n",
        "amsterdam = timezone('Europe/Amsterdam')\n",
        "datetime_format = '%Y-%m-%d-T-(%H-%M-%S)'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPlU4gaHlY5t"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the netwok architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = F.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = F.sigmoid(self.prediction_layer(h))\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "SiameseNet(\n  (convnet): ConvNet(\n    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n  )\n  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n)"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4P_10kSyZO0p"
      },
      "source": [
        "### Define the Loss (CrossEntropy) and the Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Learning rate decay per epoch\n",
        "lr_decay_rate = 0.99\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "#optimizer = optim.SGD(siamese.parameters(), lr = 0.01, momentum=0.5, weight_decay=2e-4)\n",
        "optimizer = optim.SGD(siamese.parameters(), lr = 0.01, weight_decay=2e-4)\n",
        "optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = lr_decay_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OtINTkF9mZUC"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_GgaTl2Tmvg0"
      },
      "source": [
        "---\n",
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, data_path):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_path: str\n",
        "      Path to the pickle file\n",
        "    \"\"\"\n",
        "    self.data = None\n",
        "    self.alphabet_index = None\n",
        "    with open(data_path, \"rb\") as f:\n",
        "      X, i = pickle.load(f)\n",
        "      self.data = X.astype(\"float32\")\n",
        "      self.alphabet_index = i\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    index: int\n",
        "      index from which to get the data\n",
        "    \"\"\"\n",
        "    # get dimensions of the data\n",
        "    num_letters, num_drawings, height, width = self.data.shape\n",
        "\n",
        "    # initialize index2\n",
        "    # index of second letter from pair same as first\n",
        "    index2 = index\n",
        "\n",
        "    # randomly choose a variant of the letter\n",
        "    drawing_index = np.random.choice(num_drawings)\n",
        "    drawing_index2 = np.random.choice(num_drawings)\n",
        "\n",
        "    # choose image for first letter\n",
        "    X1 = self.data[index, drawing_index, :, :].reshape(width, height)\n",
        "    \n",
        "    # set label to be 1, i.e. same letter\n",
        "    y = np.array([1.0], dtype=\"float32\")\n",
        "\n",
        "    # with 50% probability,\n",
        "    # pick an image of a different letter\n",
        "    # and change the label to 0, i.e. different letter\n",
        "    if np.random.uniform() >= 0.5:\n",
        "      index2 = (index + np.random.randint(1, num_letters)) % num_letters\n",
        "      y = np.array([0.0], dtype=\"float32\")\n",
        "    \n",
        "    # choose image for the second letter\n",
        "    X2 = self.data[index2, drawing_index2, :, :].reshape(width, height)\n",
        "\n",
        "    return X1, X2, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# create the dataset object\n",
        "data_path = \"../../data/processed/\"\n",
        "\n",
        "train_set = Dataset(os.path.join(data_path, \"train.pkl\"))\n",
        "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
        "\n",
        "device = torch.device('cuda')\n",
        "siamese.to(device, dtype=torch.float32)\n",
        "\n",
        "# set parameters for data creation\n",
        "batch_size = 128\n",
        "num_workers = 1\n",
        "\n",
        "params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': num_workers}\n",
        "\n",
        "# create the dataloader object which returns a generator over the data\n",
        "train_generator = data.DataLoader(train_set, **params)\n",
        "eval_generator = data.DataLoader(eval_set, **params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bj8kcukdmc5b"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create writer for tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter \n",
        "\n",
        "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
        "logs_path = \"./logs/\" + dt + \"/\"\n",
        "try:\n",
        "    os.makedirs(save_path)\n",
        "except:\n",
        "    pass\n",
        "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[0.009043820750088045]\n[1,     1] loss: 0.969\n[0.009043820750088045]\n[1,     2] loss: 0.909\n[0.009043820750088045]\n[1,     3] loss: 0.889\n[0.009043820750088045]\n[1,     4] loss: 0.908\n[0.009043820750088045]\n[1,     5] loss: 0.903\n[0.009043820750088045]\n[1,     6] loss: 0.866\n[0.009043820750088045]\n[1,     7] loss: 0.890\n[0.009043820750088045]\n[1,     8] loss: 0.905\n[0.008953382542587164]\n[2,     1] loss: 0.999\n[0.008953382542587164]\n[2,     2] loss: 0.955\n[0.008953382542587164]\n[2,     3] loss: 0.943\n[0.008953382542587164]\n[2,     4] loss: 0.956\n[0.008953382542587164]\n[2,     5] loss: 0.938\n[0.008953382542587164]\n[2,     6] loss: 0.920\n[0.008953382542587164]\n[2,     7] loss: 0.908\n[0.008953382542587164]\n[2,     8] loss: 0.911\n[0.008863848717161293]\n[3,     1] loss: 0.866\n[0.008863848717161293]\n[3,     2] loss: 0.869\n[0.008863848717161293]\n[3,     3] loss: 0.836\n[0.008863848717161293]\n[3,     4] loss: 0.840\n[0.008863848717161293]\n[3,     5] loss: 0.851\n[0.008863848717161293]\n[3,     6] loss: 0.853\n[0.008863848717161293]\n[3,     7] loss: 0.852\n[0.008863848717161293]\n[3,     8] loss: 0.837\n[0.00877521022998968]\n[4,     1] loss: 0.967\n[0.00877521022998968]\n[4,     2] loss: 0.892\n[0.00877521022998968]\n[4,     3] loss: 0.875\n[0.00877521022998968]\n[4,     4] loss: 0.881\n[0.00877521022998968]\n[4,     5] loss: 0.890\n[0.00877521022998968]\n[4,     6] loss: 0.878\n[0.00877521022998968]\n[4,     7] loss: 0.897\n[0.00877521022998968]\n[4,     8] loss: 0.901\n[0.008687458127689783]\n[5,     1] loss: 0.925\n[0.008687458127689783]\n[5,     2] loss: 0.918\n[0.008687458127689783]\n[5,     3] loss: 0.888\n[0.008687458127689783]\n[5,     4] loss: 0.896\n[0.008687458127689783]\n[5,     5] loss: 0.911\n[0.008687458127689783]\n[5,     6] loss: 0.899\n[0.008687458127689783]\n[5,     7] loss: 0.875\n[0.008687458127689783]\n[5,     8] loss: 0.889\n[0.008600583546412886]\n[6,     1] loss: 0.858\n[0.008600583546412886]\n[6,     2] loss: 0.889\n[0.008600583546412886]\n[6,     3] loss: 0.844\n[0.008600583546412886]\n[6,     4] loss: 0.852\n[0.008600583546412886]\n[6,     5] loss: 0.849\n[0.008600583546412886]\n[6,     6] loss: 0.825\n[0.008600583546412886]\n[6,     7] loss: 0.811\n[0.008600583546412886]\n[6,     8] loss: 0.802\n[0.008514577710948757]\n[7,     1] loss: 0.827\n[0.008514577710948757]\n[7,     2] loss: 0.902\n[0.008514577710948757]\n[7,     3] loss: 0.880\n[0.008514577710948757]\n[7,     4] loss: 0.852\n[0.008514577710948757]\n[7,     5] loss: 0.856\n[0.008514577710948757]\n[7,     6] loss: 0.837\n[0.008514577710948757]\n[7,     7] loss: 0.825\n[0.008514577710948757]\n[7,     8] loss: 0.824\n[0.00842943193383927]\n[8,     1] loss: 0.872\n[0.00842943193383927]\n[8,     2] loss: 0.873\n[0.00842943193383927]\n[8,     3] loss: 0.834\n[0.00842943193383927]\n[8,     4] loss: 0.836\n[0.00842943193383927]\n[8,     5] loss: 0.832\n[0.00842943193383927]\n[8,     6] loss: 0.815\n[0.00842943193383927]\n[8,     7] loss: 0.808\n[0.00842943193383927]\n[8,     8] loss: 0.804\n[0.008345137614500876]\n[9,     1] loss: 0.771\n[0.008345137614500876]\n[9,     2] loss: 0.759\n[0.008345137614500876]\n[9,     3] loss: 0.751\n[0.008345137614500876]\n[9,     4] loss: 0.750\n[0.008345137614500876]\n[9,     5] loss: 0.757\n[0.008345137614500876]\n[9,     6] loss: 0.730\n[0.008345137614500876]\n[9,     7] loss: 0.734\n[0.008345137614500876]\n[9,     8] loss: 0.746\n[0.008261686238355867]\n[10,     1] loss: 0.761\n[0.008261686238355867]\n[10,     2] loss: 0.729\n[0.008261686238355867]\n[10,     3] loss: 0.727\n[0.008261686238355867]\n[10,     4] loss: 0.724\n[0.008261686238355867]\n[10,     5] loss: 0.748\n[0.008261686238355867]\n[10,     6] loss: 0.752\n[0.008261686238355867]\n[10,     7] loss: 0.751\n[0.008261686238355867]\n[10,     8] loss: 0.749\n[0.008179069375972308]\n[11,     1] loss: 0.751\n[0.008179069375972308]\n[11,     2] loss: 0.768\n[0.008179069375972308]\n[11,     3] loss: 0.763\n[0.008179069375972308]\n[11,     4] loss: 0.792\n[0.008179069375972308]\n[11,     5] loss: 0.814\n[0.008179069375972308]\n[11,     6] loss: 0.808\n[0.008179069375972308]\n[11,     7] loss: 0.788\n[0.008179069375972308]\n[11,     8] loss: 0.781\n[0.008097278682212584]\n[12,     1] loss: 0.911\n[0.008097278682212584]\n[12,     2] loss: 0.876\n[0.008097278682212584]\n[12,     3] loss: 0.826\n[0.008097278682212584]\n[12,     4] loss: 0.822\n[0.008097278682212584]\n[12,     5] loss: 0.816\n[0.008097278682212584]\n[12,     6] loss: 0.802\n[0.008097278682212584]\n[12,     7] loss: 0.791\n[0.008097278682212584]\n[12,     8] loss: 0.797\n[0.00801630589539046]\n[13,     1] loss: 0.804\n[0.00801630589539046]\n[13,     2] loss: 0.801\n[0.00801630589539046]\n[13,     3] loss: 0.811\n[0.00801630589539046]\n[13,     4] loss: 0.791\n[0.00801630589539046]\n[13,     5] loss: 0.779\n[0.00801630589539046]\n[13,     6] loss: 0.766\n[0.00801630589539046]\n[13,     7] loss: 0.753\n[0.00801630589539046]\n[13,     8] loss: 0.783\n[0.007936142836436554]\n[14,     1] loss: 0.847\n[0.007936142836436554]\n[14,     2] loss: 0.825\n[0.007936142836436554]\n[14,     3] loss: 0.783\n[0.007936142836436554]\n[14,     4] loss: 0.775\n[0.007936142836436554]\n[14,     5] loss: 0.773\n[0.007936142836436554]\n[14,     6] loss: 0.776\n[0.007936142836436554]\n[14,     7] loss: 0.778\n[0.007936142836436554]\n[14,     8] loss: 0.784\n[0.007856781408072189]\n[15,     1] loss: 0.800\n[0.007856781408072189]\n[15,     2] loss: 0.813\n[0.007856781408072189]\n[15,     3] loss: 0.801\n[0.007856781408072189]\n[15,     4] loss: 0.809\n[0.007856781408072189]\n[15,     5] loss: 0.822\n[0.007856781408072189]\n[15,     6] loss: 0.794\n[0.007856781408072189]\n[15,     7] loss: 0.791\n[0.007856781408072189]\n[15,     8] loss: 0.795\n[0.007778213593991467]\n[16,     1] loss: 0.822\n[0.007778213593991467]\n[16,     2] loss: 0.820\n[0.007778213593991467]\n[16,     3] loss: 0.776\n[0.007778213593991467]\n[16,     4] loss: 0.766\n[0.007778213593991467]\n[16,     5] loss: 0.774\n[0.007778213593991467]\n[16,     6] loss: 0.777\n[0.007778213593991467]\n[16,     7] loss: 0.774\n[0.007778213593991467]\n[16,     8] loss: 0.781\n[0.007700431458051552]\n[17,     1] loss: 0.789\n[0.007700431458051552]\n[17,     2] loss: 0.790\n[0.007700431458051552]\n[17,     3] loss: 0.782\n[0.007700431458051552]\n[17,     4] loss: 0.775\n[0.007700431458051552]\n[17,     5] loss: 0.797\n[0.007700431458051552]\n[17,     6] loss: 0.814\n[0.007700431458051552]\n[17,     7] loss: 0.817\n[0.007700431458051552]\n[17,     8] loss: 0.835\n[0.007623427143471037]\n[18,     1] loss: 0.790\n[0.007623427143471037]\n[18,     2] loss: 0.775\n[0.007623427143471037]\n[18,     3] loss: 0.783\n[0.007623427143471037]\n[18,     4] loss: 0.774\n[0.007623427143471037]\n[18,     5] loss: 0.797\n[0.007623427143471037]\n[18,     6] loss: 0.804\n[0.007623427143471037]\n[18,     7] loss: 0.803\n[0.007623427143471037]\n[18,     8] loss: 0.815\n[0.007547192872036326]\n[19,     1] loss: 0.775\n[0.007547192872036326]\n[19,     2] loss: 0.785\n[0.007547192872036326]\n[19,     3] loss: 0.800\n[0.007547192872036326]\n[19,     4] loss: 0.817\n[0.007547192872036326]\n[19,     5] loss: 0.818\n[0.007547192872036326]\n[19,     6] loss: 0.815\n[0.007547192872036326]\n[19,     7] loss: 0.792\n[0.007547192872036326]\n[19,     8] loss: 0.802\n[0.007471720943315963]\n[20,     1] loss: 1.026\n[0.007471720943315963]\n[20,     2] loss: 0.949\n[0.007471720943315963]\n[20,     3] loss: 0.932\n[0.007471720943315963]\n[20,     4] loss: 0.914\n[0.007471720943315963]\n[20,     5] loss: 0.932\n[0.007471720943315963]\n[20,     6] loss: 0.906\n[0.007471720943315963]\n[20,     7] loss: 0.899\n[0.007471720943315963]\n[20,     8] loss: 0.877\n[0.007397003733882804]\n[21,     1] loss: 0.858\n[0.007397003733882804]\n[21,     2] loss: 0.839\n[0.007397003733882804]\n[21,     3] loss: 0.837\n[0.007397003733882804]\n[21,     4] loss: 0.828\n[0.007397003733882804]\n[21,     5] loss: 0.834\n[0.007397003733882804]\n[21,     6] loss: 0.812\n[0.007397003733882804]\n[21,     7] loss: 0.806\n[0.007397003733882804]\n[21,     8] loss: 0.804\n[0.007323033696543976]\n[22,     1] loss: 0.885\n[0.007323033696543976]\n[22,     2] loss: 0.833\n[0.007323033696543976]\n[22,     3] loss: 0.828\n[0.007323033696543976]\n[22,     4] loss: 0.814\n[0.007323033696543976]\n[22,     5] loss: 0.823\n[0.007323033696543976]\n[22,     6] loss: 0.824\n[0.007323033696543976]\n[22,     7] loss: 0.825\n[0.007323033696543976]\n[22,     8] loss: 0.831\n[0.007249803359578536]\n[23,     1] loss: 0.791\n[0.007249803359578536]\n[23,     2] loss: 0.823\n[0.007249803359578536]\n[23,     3] loss: 0.838\n[0.007249803359578536]\n[23,     4] loss: 0.820\n[0.007249803359578536]\n[23,     5] loss: 0.833\n[0.007249803359578536]\n[23,     6] loss: 0.820\n[0.007249803359578536]\n[23,     7] loss: 0.814\n[0.007249803359578536]\n[23,     8] loss: 0.810\n[0.0071773053259827505]\n[24,     1] loss: 0.904\n[0.0071773053259827505]\n[24,     2] loss: 0.866\n[0.0071773053259827505]\n[24,     3] loss: 0.851\n[0.0071773053259827505]\n[24,     4] loss: 0.856\n[0.0071773053259827505]\n[24,     5] loss: 0.838\n[0.0071773053259827505]\n[24,     6] loss: 0.823\n[0.0071773053259827505]\n[24,     7] loss: 0.828\n[0.0071773053259827505]\n[24,     8] loss: 0.838\n[0.007105532272722923]\n[25,     1] loss: 0.831\n[0.007105532272722923]\n[25,     2] loss: 0.893\n[0.007105532272722923]\n[25,     3] loss: 0.863\n[0.007105532272722923]\n[25,     4] loss: 0.844\n[0.007105532272722923]\n[25,     5] loss: 0.860\n[0.007105532272722923]\n[25,     6] loss: 0.840\n[0.007105532272722923]\n[25,     7] loss: 0.846\n[0.007105532272722923]\n[25,     8] loss: 0.821\n[0.007034476949995694]\n[26,     1] loss: 0.876\n[0.007034476949995694]\n[26,     2] loss: 0.887\n[0.007034476949995694]\n[26,     3] loss: 0.902\n[0.007034476949995694]\n[26,     4] loss: 0.886\n[0.007034476949995694]\n[26,     5] loss: 0.872\n[0.007034476949995694]\n[26,     6] loss: 0.852\n[0.007034476949995694]\n[26,     7] loss: 0.844\n[0.007034476949995694]\n[26,     8] loss: 0.827\n[0.006964132180495737]\n[27,     1] loss: 0.827\n[0.006964132180495737]\n[27,     2] loss: 0.812\n[0.006964132180495737]\n[27,     3] loss: 0.832\n[0.006964132180495737]\n[27,     4] loss: 0.813\n[0.006964132180495737]\n[27,     5] loss: 0.813\n[0.006964132180495737]\n[27,     6] loss: 0.829\n[0.006964132180495737]\n[27,     7] loss: 0.811\n[0.006964132180495737]\n[27,     8] loss: 0.798\n[0.00689449085869078]\n[28,     1] loss: 0.775\n[0.00689449085869078]\n[28,     2] loss: 0.811\n[0.00689449085869078]\n[28,     3] loss: 0.794\n[0.00689449085869078]\n[28,     4] loss: 0.803\n[0.00689449085869078]\n[28,     5] loss: 0.786\n[0.00689449085869078]\n[28,     6] loss: 0.780\n[0.00689449085869078]\n[28,     7] loss: 0.788\n[0.00689449085869078]\n[28,     8] loss: 0.781\n[0.006825545950103872]\n[29,     1] loss: 0.777\n[0.006825545950103872]\n[29,     2] loss: 0.787\n[0.006825545950103872]\n[29,     3] loss: 0.796\n[0.006825545950103872]\n[29,     4] loss: 0.816\n[0.006825545950103872]\n[29,     5] loss: 0.810\n[0.006825545950103872]\n[29,     6] loss: 0.789\n[0.006825545950103872]\n[29,     7] loss: 0.782\n[0.006825545950103872]\n[29,     8] loss: 0.779\n[0.0067572904906028335]\n[30,     1] loss: 0.769\n[0.0067572904906028335]\n[30,     2] loss: 0.792\n[0.0067572904906028335]\n[30,     3] loss: 0.811\n[0.0067572904906028335]\n[30,     4] loss: 0.806\n[0.0067572904906028335]\n[30,     5] loss: 0.806\n[0.0067572904906028335]\n[30,     6] loss: 0.804\n[0.0067572904906028335]\n[30,     7] loss: 0.804\n[0.0067572904906028335]\n[30,     8] loss: 0.798\n[0.006689717585696805]\n[31,     1] loss: 0.851\n[0.006689717585696805]\n[31,     2] loss: 0.863\n[0.006689717585696805]\n[31,     3] loss: 0.823\n[0.006689717585696805]\n[31,     4] loss: 0.810\n[0.006689717585696805]\n[31,     5] loss: 0.849\n[0.006689717585696805]\n[31,     6] loss: 0.904\n[0.006689717585696805]\n[31,     7] loss: 0.914\n[0.006689717585696805]\n[31,     8] loss: 0.910\n[0.006622820409839836]\n[32,     1] loss: 0.925\n[0.006622820409839836]\n[32,     2] loss: 0.908\n[0.006622820409839836]\n[32,     3] loss: 0.919\n[0.006622820409839836]\n[32,     4] loss: 0.955\n[0.006622820409839836]\n[32,     5] loss: 0.950\n[0.006622820409839836]\n[32,     6] loss: 0.956\n[0.006622820409839836]\n[32,     7] loss: 0.968\n[0.006622820409839836]\n[32,     8] loss: 0.966\n[0.006556592205741438]\n[33,     1] loss: 1.028\n[0.006556592205741438]\n[33,     2] loss: 0.942\n[0.006556592205741438]\n[33,     3] loss: 0.908\n[0.006556592205741438]\n[33,     4] loss: 0.917\n[0.006556592205741438]\n[33,     5] loss: 0.904\n[0.006556592205741438]\n[33,     6] loss: 0.881\n[0.006556592205741438]\n[33,     7] loss: 0.886\n[0.006556592205741438]\n[33,     8] loss: 0.868\n[0.006491026283684023]\n[34,     1] loss: 0.781\n[0.006491026283684023]\n[34,     2] loss: 0.838\n[0.006491026283684023]\n[34,     3] loss: 0.802\n[0.006491026283684023]\n[34,     4] loss: 0.803\n[0.006491026283684023]\n[34,     5] loss: 0.796\n[0.006491026283684023]\n[34,     6] loss: 0.798\n[0.006491026283684023]\n[34,     7] loss: 0.817\n[0.006491026283684023]\n[34,     8] loss: 0.799\n[0.006426116020847182]\n[35,     1] loss: 0.902\n[0.006426116020847182]\n[35,     2] loss: 0.878\n[0.006426116020847182]\n[35,     3] loss: 0.860\n[0.006426116020847182]\n[35,     4] loss: 0.849\n[0.006426116020847182]\n[35,     5] loss: 0.815\n[0.006426116020847182]\n[35,     6] loss: 0.810\n[0.006426116020847182]\n[35,     7] loss: 0.820\n[0.006426116020847182]\n[35,     8] loss: 0.813\n[0.006361854860638711]\n[36,     1] loss: 0.785\n[0.006361854860638711]\n[36,     2] loss: 0.790\n[0.006361854860638711]\n[36,     3] loss: 0.760\n[0.006361854860638711]\n[36,     4] loss: 0.769\n[0.006361854860638711]\n[36,     5] loss: 0.782\n[0.006361854860638711]\n[36,     6] loss: 0.768\n[0.006361854860638711]\n[36,     7] loss: 0.791\n[0.006361854860638711]\n[36,     8] loss: 0.778\n[0.006298236312032323]\n[37,     1] loss: 0.838\n[0.006298236312032323]\n[37,     2] loss: 0.820\n[0.006298236312032323]\n[37,     3] loss: 0.807\n[0.006298236312032323]\n[37,     4] loss: 0.820\n[0.006298236312032323]\n[37,     5] loss: 0.810\n[0.006298236312032323]\n[37,     6] loss: 0.807\n[0.006298236312032323]\n[37,     7] loss: 0.805\n[0.006298236312032323]\n[37,     8] loss: 0.800\n[0.006235253948912]\n[38,     1] loss: 0.889\n[0.006235253948912]\n[38,     2] loss: 0.895\n[0.006235253948912]\n[38,     3] loss: 0.857\n[0.006235253948912]\n[38,     4] loss: 0.846\n[0.006235253948912]\n[38,     5] loss: 0.848\n[0.006235253948912]\n[38,     6] loss: 0.848\n[0.006235253948912]\n[38,     7] loss: 0.855\n[0.006235253948912]\n[38,     8] loss: 0.842\n[0.00617290140942288]\n[39,     1] loss: 0.864\n[0.00617290140942288]\n[39,     2] loss: 0.810\n[0.00617290140942288]\n[39,     3] loss: 0.780\n[0.00617290140942288]\n[39,     4] loss: 0.792\n[0.00617290140942288]\n[39,     5] loss: 0.795\n[0.00617290140942288]\n[39,     6] loss: 0.819\n[0.00617290140942288]\n[39,     7] loss: 0.830\n[0.00617290140942288]\n[39,     8] loss: 0.835\n[0.006111172395328651]\n[40,     1] loss: 0.804\n[0.006111172395328651]\n[40,     2] loss: 0.764\n[0.006111172395328651]\n[40,     3] loss: 0.759\n[0.006111172395328651]\n[40,     4] loss: 0.758\n[0.006111172395328651]\n[40,     5] loss: 0.775\n[0.006111172395328651]\n[40,     6] loss: 0.771\n[0.006111172395328651]\n[40,     7] loss: 0.774\n[0.006111172395328651]\n[40,     8] loss: 0.778\n[0.0060500606713753645]\n[41,     1] loss: 0.856\n[0.0060500606713753645]\n[41,     2] loss: 0.837\n[0.0060500606713753645]\n[41,     3] loss: 0.824\n[0.0060500606713753645]\n[41,     4] loss: 0.859\n[0.0060500606713753645]\n[41,     5] loss: 0.942\n[0.0060500606713753645]\n[41,     6] loss: 0.988\n[0.0060500606713753645]\n[41,     7] loss: 1.047\n[0.0060500606713753645]\n[41,     8] loss: 1.065\n[0.005989560064661611]\n[42,     1] loss: 1.082\n[0.005989560064661611]\n[42,     2] loss: 1.143\n[0.005989560064661611]\n[42,     3] loss: 1.073\n[0.005989560064661611]\n[42,     4] loss: 1.095\n[0.005989560064661611]\n[42,     5] loss: 1.069\n[0.005989560064661611]\n[42,     6] loss: 1.051\n[0.005989560064661611]\n[42,     7] loss: 1.054\n[0.005989560064661611]\n[42,     8] loss: 1.061\n[0.005929664464014994]\n[43,     1] loss: 1.065\n[0.005929664464014994]\n[43,     2] loss: 1.056\n[0.005929664464014994]\n[43,     3] loss: 0.996\n[0.005929664464014994]\n[43,     4] loss: 0.983\n[0.005929664464014994]\n[43,     5] loss: 0.965\n[0.005929664464014994]\n[43,     6] loss: 0.966\n[0.005929664464014994]\n[43,     7] loss: 0.953\n[0.005929664464014994]\n[43,     8] loss: 0.957\n[0.0058703678193748445]\n[44,     1] loss: 0.909\n[0.0058703678193748445]\n[44,     2] loss: 0.892\n[0.0058703678193748445]\n[44,     3] loss: 0.884\n[0.0058703678193748445]\n[44,     4] loss: 0.914\n[0.0058703678193748445]\n[44,     5] loss: 0.886\n[0.0058703678193748445]\n[44,     6] loss: 0.891\n[0.0058703678193748445]\n[44,     7] loss: 0.896\n[0.0058703678193748445]\n[44,     8] loss: 0.901\n[0.005811664141181096]\n[45,     1] loss: 1.006\n[0.005811664141181096]\n[45,     2] loss: 0.962\n[0.005811664141181096]\n[45,     3] loss: 0.927\n[0.005811664141181096]\n[45,     4] loss: 0.938\n[0.005811664141181096]\n[45,     5] loss: 0.904\n[0.005811664141181096]\n[45,     6] loss: 0.896\n[0.005811664141181096]\n[45,     7] loss: 0.903\n[0.005811664141181096]\n[45,     8] loss: 0.916\n[0.0057535474997692845]\n[46,     1] loss: 0.910\n[0.0057535474997692845]\n[46,     2] loss: 0.856\n[0.0057535474997692845]\n[46,     3] loss: 0.848\n[0.0057535474997692845]\n[46,     4] loss: 0.863\n[0.0057535474997692845]\n[46,     5] loss: 0.862\n[0.0057535474997692845]\n[46,     6] loss: 0.862\n[0.0057535474997692845]\n[46,     7] loss: 0.847\n[0.0057535474997692845]\n[46,     8] loss: 0.851\n[0.005696012024771591]\n[47,     1] loss: 0.830\n[0.005696012024771591]\n[47,     2] loss: 0.868\n[0.005696012024771591]\n[47,     3] loss: 0.857\n[0.005696012024771591]\n[47,     4] loss: 0.875\n[0.005696012024771591]\n[47,     5] loss: 0.856\n[0.005696012024771591]\n[47,     6] loss: 0.870\n[0.005696012024771591]\n[47,     7] loss: 0.873\n[0.005696012024771591]\n[47,     8] loss: 0.870\n[0.005639051904523875]\n[48,     1] loss: 0.949\n[0.005639051904523875]\n[48,     2] loss: 0.912\n[0.005639051904523875]\n[48,     3] loss: 0.876\n[0.005639051904523875]\n[48,     4] loss: 0.901\n[0.005639051904523875]\n[48,     5] loss: 0.863\n[0.005639051904523875]\n[48,     6] loss: 0.858\n[0.005639051904523875]\n[48,     7] loss: 0.857\n[0.005639051904523875]\n[48,     8] loss: 0.844\n[0.005582661385478636]\n[49,     1] loss: 0.874\n[0.005582661385478636]\n[49,     2] loss: 0.848\n[0.005582661385478636]\n[49,     3] loss: 0.825\n[0.005582661385478636]\n[49,     4] loss: 0.829\n[0.005582661385478636]\n[49,     5] loss: 0.837\n[0.005582661385478636]\n[49,     6] loss: 0.830\n[0.005582661385478636]\n[49,     7] loss: 0.833\n[0.005582661385478636]\n[49,     8] loss: 0.814\n[0.0055268347716238495]\n[50,     1] loss: 0.874\n"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-090361dfa8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# outputs_ = torch.cat((outputs.view(-1, 1), (1-outputs).view(-1, 1)), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/envs/DL/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/envs/DL/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2074\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2076\u001b[0;31m     return torch._C._nn.binary_cross_entropy(\n\u001b[0m\u001b[1;32m   2077\u001b[0m         input, target, weight, reduction_enum)\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(200):\n",
        "  running_loss = 0.0\n",
        "  i = 0\n",
        "  \n",
        "  for X1, X2, y in train_generator:\n",
        "    X1 = X1.to(device)\n",
        "    X2 = X2.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    X1 = X1.view(-1, 1, 105, 105)\n",
        "    X2 = X2.view(-1, 1, 105, 105)\n",
        "    \n",
        "\n",
        "    outputs = siamese(X1, X2)\n",
        "    # print(outputs, outputs.dtype)\n",
        "    # print(outputs.shape)\n",
        "    # print(y.shape)\n",
        "    # print(outputs)\n",
        "    # outputs_ = torch.cat((outputs.view(-1, 1), (1-outputs).view(-1, 1)), dim=1)\n",
        "    loss = criterion(outputs , y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "\n",
        "\n",
        "    if i == 0:\n",
        "      writer.add_scalar(\"training loss (per epoch)\", loss.item(), epoch)\n",
        "      writer.add_scalar(\"learning rate (per epoch)\", optim_scheduler.state_dict()[\"_last_lr\"][0], epoch)\n",
        "    \n",
        "    writer.add_scalar(\"training loss\", loss.item(), epoch * len(train_generator) + i)\n",
        "\n",
        "    print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss / (i+1)))\n",
        "    i+=1\n",
        "  # Update the learning rate\n",
        "  optim_scheduler.step()\n",
        "\n",
        "  #\n",
        "    \n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "torch.Size([1, 105, 105])\ntorch.Size([1, 105, 105])\ndifferent letter\n"
        },
        {
          "data": {
            "image/png": "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\n",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"161.091629pt\" version=\"1.1\" viewBox=\"0 0 323.596185 161.091629\" width=\"323.596185pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 161.091629 \nL 323.596185 161.091629 \nL 323.596185 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 137.213504 \nL 160.105682 137.213504 \nL 160.105682 10.395323 \nL 33.2875 10.395323 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p577101f59f)\">\n    <image height=\"127\" id=\"image901edfc97d\" transform=\"scale(1 -1)translate(0 -127)\" width=\"127\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAH8AAAB/CAYAAADGvR0TAAAABHNCSVQICAgIfAhkiAAAAtxJREFUeJztndttAjEQAEmUKvhOAXRBzekiVSRVRMkXUsTjOLDX3r2Z+UWcDs+ud7HN8fLz/f67EySvs29A5qF8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoHo3wwygejfDDKB6N8MMoH8zb7BkZy3B9Crvvx9Rly3WheqjyWJUpcNJkDw2k/mOP+kDZwS8jPOnjVKSFfYlA+mNBu/zRdZ2l6Iu6jckkK6fZvDcizg9/7elHcC4Rs99st8ytnwCiO+0OqALDmg1md+Wb2fU5ZXWWszPwAMk3tS4Sv7VcZCCJ3M79leVLxl2QqCTflt65J08VX+PzWfDAXNd8pnsNF5reswmXevpRLrk77H1+fTZlsICyTZVxWr+33uOHea/st1xxF5n2J1Q1f62yw2z3/DSLDQG0Ru30wD6/wnWfhM5mcbXeLSvPy7jWJS3Xu9JoBMJ+Qaf+W1POg8BvBXMJq/qlBXJPdBsAchjR8a4LAWWA8dvtghv5W73/Dd4vz120K4xie+Y/KtBzEMWXaN5tzMK3m9wgAg6gNGz4wUx/O0GOpWJ4n1ZM5SMGQ4XeMqaf9LdT0HlvhUaSWv0TWAa1EqmmfSOvuZsspp7KZL+0ofxBLWTirsVU+mOE1P/Np1miWNrZmnGwy8weTKchTd/tbXuTpRUswmflg0mT+I1meaersyei6b+ZPIEvwpj/J0/v92Rl5cqnMSZ7MGyRVKXGSZ4vSMwSzNR9MmX/a2Coznz1g5k/GkzxyleiuX/lglA9G+QmYddBD+QWICgDlJ2FG159mV49GhrMKZn4RIjZ8lA9G+cXomf3KB6P8gvTKfnf1wJj5YJQPRvlglA9G+WCUD0b5YJQPRvlglA9G+WCUD0b5YJQPRvlglA9G+WCUD0b5YJQPRvlglA9G+WCUD0b5YJQPRvlg/gD+beohHMU2zAAAAABJRU5ErkJggg==\" y=\"-10.213504\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf83a97c31c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.891396\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(30.710146 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.281006\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(87.918506 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.670617\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(145.126867 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb8ef8e3044\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"35.155063\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(13.5625 38.954282)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"59.310907\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(13.5625 63.110126)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"83.466751\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(13.5625 87.26597)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"107.622595\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(13.5625 111.421814)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb8ef8e3044\" y=\"131.77844\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 135.577658)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 137.213504 \nL 33.2875 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 160.105682 137.213504 \nL 160.105682 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 137.213504 \nL 160.105682 137.213504 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.395323 \nL 160.105682 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 185.469318 137.213504 \nL 312.2875 137.213504 \nL 312.2875 10.395323 \nL 185.469318 10.395323 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd07201a05d)\">\n    <image height=\"127\" id=\"image7d39380896\" transform=\"scale(1 -1)translate(0 -127)\" width=\"127\" x=\"185.469318\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAH8AAAB/CAYAAADGvR0TAAAABHNCSVQICAgIfAhkiAAAAsJJREFUeJzt3c1t20AURWE6cBVapwB1oZrVhaqQqzCSlQBBIKk/aubdOefbO6F55g0nhGN9/f78/TcJ6U/vC1A/xgczPpjxwYwPZnww44MZH8z4YMYHMz6Y8cGMD2Z8MOODGR/M+GDGBzM+mPHBjA9mfDDjgxkfzPhgxgczPpjxwYwPZnww44MZH8z4YN+9L4DusNu/9HXH8+ntv/vLX8vSxquR17y7AJz8N30iaivGf0By4DUe+MDQk5880Vsc+CLjJ0e7tUXEV5WOP1Lkaeobek63+COFrRb1UR74wIz/gLXJTp36aSr+zP+05HBbiIx/HW3p7NAq7GG3j11E5eOn3tgE3eKnRT2eT0P9C2WaPPChGR/M+BtIfRxEx29909POKfdEx18yWqRPGTJ+D4lbv/HBjP+kkR4psfF7v9YdQWz8ig67fdSz3/gvOJ5PQ+wwkfGrT1f167uIjK9tDBV/hK24paHit7a22BK2/rj4CTc1RVz8Jb22/OTpHya+nmf8DaQeNMv/AOdF9S00UUz8JalTV0HEtp889ZXf90fEX+LUvyc6vt5j/I0k7kLlD3xVn5fXEq5xTun41W9q9eu7p3T8NT222fTYt2Ljt7BF7MpnAQ98YJGTX3maLhKuMTJ+RQmxb8Vt+xVvcsVreoST/4LU2Ldi4ve44aNEXhK37Ws7xgeL2PYTtt9nXwhV+J6axB/tf9Ru8ebv8mf4K9dXVFggo73Tv/CZD9Z18q8nqsKET9O4Uz6nzLY/d9NbLIhesSss9jLx56RNYYWgz2j6iZppMe9Ji32ry8epJi6C9NBzPO2Dlfgg5Wo7wYhTPqdE/CUtFgUl9JzS8fVZPvPBjA9mfDDjgxkfzPhgxgczPpjxwYwPZnww44MZH8z4YMYHMz6Y8cGMD2Z8MOODGR/M+GDGBzM+mPHBjA9mfDDjg/0HZiau+Z2XyLoAAAAASUVORK5CYII=\" y=\"-10.213504\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.073214\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(182.891964 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.462825\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 50 -->\n      <g transform=\"translate(240.100325 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.852435\" xlink:href=\"#mf83a97c31c\" y=\"137.213504\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 100 -->\n      <g transform=\"translate(297.308685 151.811942)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g transform=\"translate(172.106818 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"35.155063\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(165.744318 38.954282)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"59.310907\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 40 -->\n      <g transform=\"translate(165.744318 63.110126)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"83.466751\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 60 -->\n      <g transform=\"translate(165.744318 87.26597)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"107.622595\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 80 -->\n      <g transform=\"translate(165.744318 111.421814)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.469318\" xlink:href=\"#mb8ef8e3044\" y=\"131.77844\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 100 -->\n      <g transform=\"translate(159.381818 135.577658)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 185.469318 137.213504 \nL 185.469318 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 312.2875 137.213504 \nL 312.2875 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 185.469318 137.213504 \nL 312.2875 137.213504 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 185.469318 10.395323 \nL 312.2875 10.395323 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p577101f59f\">\n   <rect height=\"126.818182\" width=\"126.818182\" x=\"33.2875\" y=\"10.395323\"/>\n  </clipPath>\n  <clipPath id=\"pd07201a05d\">\n   <rect height=\"126.818182\" width=\"126.818182\" x=\"185.469318\" y=\"10.395323\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": "<Figure size 360x360 with 2 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Test to see if labels are correct\n",
        "\n",
        "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
        "\n",
        "batch_size = 1\n",
        "eval_generator = data.DataLoader(eval_set, **{'batch_size': batch_size})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure(figsize=(5, 5))\n",
        "i=1\n",
        "columns = 2\n",
        "rows = batch_size\n",
        "for x1, x2, y in eval_generator:\n",
        "  print(x1.shape)\n",
        "  print(x2.shape)\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  # img = np.random.randint(10, size=(h,w))\n",
        "  plt.imshow(x1.squeeze())\n",
        "  i+=1\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.imshow(x2.squeeze())\n",
        "  i+=1\n",
        "  if (y == 1):\n",
        "    print(\"same letter\")\n",
        "  else:\n",
        "    print(\"different letter\")\n",
        "  break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cI4oWq6bmz5Z"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8gDe5nYvhFQc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
        "save_path = \"./saves/\" + dt + \"/\"\n",
        "try:\n",
        "    os.makedirs(save_path)\n",
        "except:\n",
        "    pass\n",
        "torch.save(siamese, save_path + \"siamese\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}