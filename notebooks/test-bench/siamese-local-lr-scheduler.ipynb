{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python38164bitdlvenv8c880a4f959d41a0bd88c28d905ec3ee",
      "display_name": "Python 3.8.1 64-bit ('DL': venv)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharwinbobde/siamese-nn-oneshot-reproduction/blob/validation-and-fix/notebooks/test-bench/siamese-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cT7lOrijlJs",
        "colab_type": "text"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Twhmbb8kXNQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M1FkjdQluR8",
        "colab_type": "text"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qac1GqFnl58c",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mcpj2P3l8So",
        "colab_type": "text"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uAN_OrVe3HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone\n",
        "amsterdam = timezone('Europe/Amsterdam')\n",
        "datetime_format = '%Y-%m-%d-T-(%H-%M-%S)'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoqpb1oxoEpU",
        "colab_type": "text"
      },
      "source": [
        "## Definition of the dataset class that will hold our examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtx4GZl_oJy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    \"\"\"Dataset that reads the data from an npy file and \n",
        "    returns a pair to the loader\"\"\"\n",
        "    def __init__(self, data_path=None, labels_path=None, \n",
        "                 transform=None, dataset: data.Dataset =None, \n",
        "                 data : np.ndarray = None, labels: np.ndarray = None,\n",
        "                 mean : float = None, std : float = None,\n",
        "                 transform_data=False):\n",
        "        self.transform_data = transform_data\n",
        "\n",
        "        # If we're given another dataset, just take that\n",
        "        if dataset is not None:\n",
        "            self.data = dataset.data\n",
        "            self.labels = dataset.labels\n",
        "            self.transforms = dataset.transforms\n",
        "\n",
        "        # We can also pass the data and labels as an array\n",
        "        elif data is not None:\n",
        "            self.data = data\n",
        "            self.labels = labels\n",
        "\n",
        "            self.mean = mean\n",
        "            self.std = std \n",
        "\n",
        "            \n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                self.normalize\n",
        "            ])\n",
        "\n",
        "        # If not, that means that we load it from a file\n",
        "        else:\n",
        "            # Load the data and labels\n",
        "            self.data = np.load(data_path)\n",
        "            self.labels = np.load(labels_path)\n",
        "\n",
        "            # for training set, calculate mean and std\n",
        "            # to normalize\n",
        "            if mean == None and std == None:\n",
        "                # stats of the dataset\n",
        "                self.mean = np.mean(self.data[:,:,:])\n",
        "                self.std = np.std(self.data[:,:,:])\n",
        "            # for test set, use mean and std from\n",
        "            # the train set to normalize\n",
        "            else:\n",
        "                self.mean = mean\n",
        "                self.std = std\n",
        "            # Normalize by default!\n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            # We apply the transformations that are given, so we can \n",
        "            # join the datasets\n",
        "\n",
        "            if transform is not None:\n",
        "              # If we're given transforms it means\n",
        "              # that we're trying to apply the affine transformations\n",
        "              self.transforms = transforms.Compose([\n",
        "                  transform\n",
        "              ])\n",
        "            else:\n",
        "              # If we're not given transforms just return the\n",
        "              # normalized tensor\n",
        "              print(\"Using the default transformations\")\n",
        "              self.transforms = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    self.normalize                                \n",
        "              ])\n",
        "              \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def get_images(self, index):\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        return Image.fromarray(_x1), Image.fromarray(_x2), label\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Gets the next pair from \n",
        "        the dataset and its corresponding label\n",
        "        (0 or 1 depending on if they're the same\n",
        "        or a different letter)\"\"\"\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        # Convert to PIL Images so \n",
        "        # we can transform them with affine transforms\n",
        "        # Just needed to generate the dataset\n",
        "        if self.transform_data:\n",
        "            _x1 = Image.fromarray(_x1)\n",
        "            _x2 = Image.fromarray(_x2)\n",
        "            \n",
        "            # we need to convert the x's to images to apply the transforms\n",
        "            return np.array(self.transforms(_x1)), np.array(self.transforms(_x2)), label\n",
        "        else:\n",
        "          # We're trying to train the dataset, so give\n",
        "          # the data in float32 version that's better for training\n",
        "          # and apply the ToTensor and normalization transformations\n",
        "            _x1 = _x1.astype(np.float32)\n",
        "            _x2 = _x2.astype(np.float32)\n",
        "            label = label.astype(np.float32)\n",
        "            return self.transforms(_x1), self.transforms(_x2), label\n",
        "    \n",
        "# Some easy functions to visualize the data \n",
        "def show_pair(x1, x2, lab):\n",
        "    \"\"\"Function to show two images of the dataset side by side\"\"\"\n",
        "    # x1 = x1.numpy()\n",
        "    # x2 = x2.numpy()\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(x1.squeeze())\n",
        "    ax2.imshow(x2.squeeze())\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    \n",
        "def show_image_pair(i1, i2, lab):\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(i1)\n",
        "    ax2.imshow(i2)\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSgUcaaokfH",
        "colab_type": "text"
      },
      "source": [
        "### Set up the folder in Google Drive and define the data path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ioF-p-sxTg",
        "colab_type": "text"
      },
      "source": [
        "## In case we want to augment the dataset we can run the function defined below\n",
        "\n",
        "For each sample the fucntion generates 8x affine transformed samples of that pair and returns a new dataset with the original images and the augmented samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGt6Iyi7rSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case we want to create affine transformations...\n",
        "import gc\n",
        "\n",
        "\n",
        "def augment_dataset(d: SiameseDataset) -> SiameseDataset:\n",
        "  \"\"\" Augments the dataset and returns a siamese dataset\n",
        "  with 9x as much data, the original data in the argument dataset\n",
        "  plus 8 affine transformations of that input data\"\"\"\n",
        "  # Create a data loader of the dataset\n",
        "  loader = data.DataLoader(d, batch_size=15000)\n",
        "\n",
        "  # Altered samples of the input data\n",
        "  _altered = None\n",
        "  mean = None\n",
        "  std = None\n",
        "\n",
        "  # Check the size of the batches and so on\n",
        "  # Read in batches of 15000, and do it \n",
        "  for j in range(8):\n",
        "      gc.collect()\n",
        "      print(\"starting with round \",j)\n",
        "      for i, (x1, x2, _) in enumerate(loader):\n",
        "          if i % 1 == 0:\n",
        "              print(i)\n",
        "          x1 = np.expand_dims(x1, 1)\n",
        "          x2 = np.expand_dims(x2, 1)\n",
        "          # concatenate the arrays by their second axis\n",
        "          _data = np.concatenate((x1,x2), axis = 1)\n",
        "          _mean = np.mean(_data)\n",
        "          _std = np.std(_data)\n",
        "          if mean is None:\n",
        "            mean = _mean\n",
        "            std = _std\n",
        "          else:\n",
        "            mean = (mean*len(_altered) +  _mean*len(_data))/(len(_altered)+len(_data))\n",
        "            std = (std*len(_altered) +  _std*len(_data))/(len(_altered)+len(_data))\n",
        "          # add them to the dataset\n",
        "          if _altered is None:\n",
        "              _altered = _data\n",
        "          else:\n",
        "              # Concatenate the existing data and the new batch\n",
        "              _altered = np.concatenate((_altered, _data), axis = 0)\n",
        "      \n",
        "      print(f'Size of the datasets -> {_altered.shape}')\n",
        "\n",
        "  # Now create a new dataset with the newly defined data\n",
        "  # Concatenate the original dataset with the new one\n",
        "  all_data = np.concatenate((d.data, _altered), axis = 0)\n",
        "  labels = np.tile(d.labels, 9)\n",
        "  # Add mean of the original datset\n",
        "  mean = (mean*len(_altered) +  d.mean*len(d))/(len(_altered)+len(d))\n",
        "  std = (std*len(_altered) +  d.std*len(d))/(len(_altered)+len(d))\n",
        "  d = SiameseDataset(data = all_data, labels = labels, mean = mean, std = std)\n",
        "  return d"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkzkzXACLghZ",
        "colab_type": "text"
      },
      "source": [
        "## Load the training and validation data\n",
        "\n",
        "In case we want to work with the affine transformations we use the part of the code that calls transform dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bug6aTZxosI4",
        "colab_type": "code",
        "outputId": "9f5020bd-fb05-497b-8e92-fc806d22e131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# define the paths of the training data\n",
        "train_data_path = \"../../data/processed/datasets/trainX_30k.npy\"\n",
        "train_labels_path = \"../../data/processed/datasets/trainY_30k.npy\"\n",
        "\n",
        "# Affine transformations to be done on the train data\n",
        "affine = transforms.RandomAffine(degrees = (-10,10), \n",
        "                                 translate=(0.1,0.1),\n",
        "                                 scale = (0.8, 1.2),\n",
        "                                 shear = (-0.3, 0.3), \n",
        "                                 fillcolor=255)\n",
        "\n",
        "# In order to augment the dataset the dataset has to be created with\n",
        "# some extra parameters and call the function defined below\n",
        "train_d = SiameseDataset(train_data_path, train_labels_path)#, transform_data=True, transform=affine)\n",
        "print(f\"Mean of {train_d.mean} and {train_d.std}\")\n",
        "print(\"Loaded data\")\n",
        "#train_d = augment_dataset(train_d)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using the default transformations\nMean of 236.38998936507937 and 66.32661770415308\nLoaded data\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L09YjjbD9Ia1",
        "colab_type": "code",
        "outputId": "a5b491d4-f0fd-477e-b13c-7bdafab3e2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# validation data\n",
        "valid_data_path = \"../../data/processed/datasets/validationX.npy\"\n",
        "valid_labels_path = \"../../data/processed/datasets/validationY.npy\"\n",
        "valid_d = SiameseDataset(valid_data_path, valid_labels_path)\n",
        "print(\"Loaded validation set with shape \",valid_d.data.shape)\n",
        "print(f\"Mean of {valid_d.mean} and {valid_d.std}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using the default transformations\nLoaded validation set with shape  (10000, 2, 105, 105)\nMean of 234.00379959183672 and 70.09415576566917\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7HZCDcU5aVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "test_data_path = \"../../data/processed/datasets/testX.npy\"\n",
        "test_labels_path = \"../../data/processed/datasets/testY.npy\"\n",
        "test_d = SiameseDataset(test_data_path, test_labels_path)\n",
        "# print(\"Loaded test set with shape \",test_d.data.shape)\n",
        "# print(f\"Mean of {test_d.mean} and {test_d.std}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using the default transformations\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlU4gaHlY5t",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsEL_whslWv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = torch.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = self.prediction_layer(h)\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "torch.manual_seed(12)\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  np.random.seed(12)\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n",
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj8AwypRYxFe",
        "colab_type": "text"
      },
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPtfP3AUfhj",
        "colab_type": "code",
        "outputId": "204e4159-f71d-47a8-ecda-48a04f15bee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)\n",
        "\n",
        "# Send the network to the GPU if available\n",
        "#device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "siamese.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "SiameseNet(\n  (convnet): ConvNet(\n    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n  )\n  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n)"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8kcukdmc5b",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfozeCIjo1iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_data_loader, validate_data_loader,\n",
        "          model_save_path, checkpoint_path,\n",
        "          validate_every = 10 , save_every = 50):\n",
        "  \"\"\" Train the network with two parameters, one is how often should we validate\n",
        "  and the other is how often should we save a checkpoint\"\"\"\n",
        "\n",
        "  best_accuracy = 0\n",
        "\n",
        "  # define the loader of the dataset\n",
        "\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    \n",
        "    for X1, X2, y in train_data_loader:\n",
        "      # set network to learning mode\n",
        "      model.train()\n",
        "\n",
        "      # send to gpu\n",
        "      X1 = X1.to(device)\n",
        "      X2 = X2.to(device)\n",
        "      y = y.to(device)\n",
        "      \n",
        "      # make gradients zero before forward prop\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # reshape inputs\n",
        "      X1 = X1.view(-1, 1, 105, 105)\n",
        "      X2 = X2.view(-1, 1, 105, 105)\n",
        "      y = y.view(-1, 1)\n",
        "\n",
        "      # forward prop\n",
        "      outputs = model(X1, X2)\n",
        "\n",
        "      # compute loss\n",
        "      loss = criterion(outputs, y)\n",
        "\n",
        "      # backprop and gradient descent step\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      # For cyclic LR update for every batch size\n",
        "      writer.add_scalar(\"learning rate\", optimizer.param_groups[0]['lr'], epoch * len(train_data_loader) + i)\n",
        "      scheduler.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      if i % 50 == 0:\n",
        "        print('[%d, %5d] loss: %.3f lr: %.14f' %\n",
        "                  (epoch + 1, i + 1, running_loss / (i+1), optimizer.param_groups[0]['lr']))\n",
        "      if i % 10 == 0:\n",
        "        writer.add_scalar(\"training loss\", running_loss / (i+1), epoch * len(train_data_loader) + i)\n",
        "\n",
        "\n",
        "      # Author's schedular\n",
        "      '''\n",
        "      if i==0 and epoch > 0:\n",
        "        writer.add_scalar(\"learning rate\", optimizer.param_groups[0]['lr'], epoch * len(train_data_loader) + i)\n",
        "        scheduler.step()\n",
        "      '''\n",
        "      i+=1\n",
        "\n",
        "    # Update the learning rate\n",
        "    # optim_scheduler.step()\n",
        "\n",
        "    # every `validate_every` epochs,\n",
        "    # get metrics from validation set\n",
        "    if epoch % validate_every == 0:\n",
        "      accuracy = validate(model, validate_data_loader)\n",
        "      writer.add_scalar(\"validation accuracy\", accuracy, epoch+1)\n",
        "\n",
        "      # if accuracy is higest till now,\n",
        "      # save model\n",
        "      if accuracy > best_accuracy:\n",
        "        print(\"Saving best model\")\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    # save model every `save_every` epochs\n",
        "    if epoch > 0 and epoch % save_every == 0:\n",
        "      torch.save(model.state_dict(), checkpoint_path)\n",
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkbTiqs7MlZ2",
        "colab_type": "text"
      },
      "source": [
        "## Validation Loop\n",
        "\n",
        "In this validation loop we loop through the validation set and calculate the accuracy of the model.\n",
        "\n",
        "We can do this as often as it is said in the training loop. For a thorough evaluation we can use validate_every= 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABqSLMQRCUys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, data_loader):\n",
        "  \"\"\" Validates the model and computes the accuracy\"\"\"\n",
        "  \n",
        "  # set network to validation mode\n",
        "  model.eval()\n",
        "  print(\"Validating model!\")\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for x1, x2, y in data_loader:\n",
        "\n",
        "      # Send data to device\n",
        "      x1 = x1.to(device)\n",
        "      x2= x2.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # Appropriate view\n",
        "      x1 = x1.view(-1, 1, 105, 105)\n",
        "      x2 = x2.view(-1, 1, 105, 105)\n",
        "      y = y.view(-1,1)\n",
        "\n",
        "      # forward prop\n",
        "      outputs = model(x1, x2)\n",
        "      # Translate the outputs to 0 or 1\n",
        "      predicted = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "      total += y.size(0)\n",
        "      correct += (predicted == y).sum().item()\n",
        "    \n",
        "    # return the accuracy\n",
        "    print(\"Accuracy of the network on the val set %.3f %%\" % (100*correct /total))\n",
        "    return 100*correct/total"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtINTkF9mZUC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting \n",
        "\n",
        "### Define the Loss (CrossEntropy) and the Adam optimizer\n",
        "\n",
        "We set two different weight decay rates as done in the keras code, as it certainly shows really good results this way, as well as a fixed (could be reduced in the future) learning rate of 3e-4 using the Adam Optimizer\n",
        "\n",
        "We choose BCEWithLogits in order to improve the stability of teh network compared to when we use just BCE, since it makes use of the log sum exp trick thus avoiding underflow.\n",
        "\n",
        "### Recorded Stable settings\n",
        "None  \n",
        "Authors) optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.99) updated every epoch  \n",
        "Test Accuracy 90.43    \n",
        "\n",
        "optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=3e-4, cycle_momentum=False, mode=\"triangular2\", step_size_up=500) \n",
        "Test Accuracy 87.9 \n",
        "  \n",
        "optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=3e-4, cycle_momentum=False, mode=\"triangular\", step_size_up=500)  \n",
        "Test Accuracy 91.06  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjJzC3Q3lgRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = 3e-4\n",
        "regularization = 2e-4\n",
        "# Learning rate decay per epoch\n",
        "lr_decay_rate = 0.999\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    [\n",
        "     {\"params\": siamese.convnet.parameters()},\n",
        "     {\"params\": siamese.prediction_layer.parameters(), \"weight_decay\": 1e-3}\n",
        "    ],\n",
        "    lr = learning_rate,\n",
        "    weight_decay = regularization\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=3e-4, cycle_momentum=False, mode=\"triangular\", step_size_up=500) \n",
        "\n",
        "n_epochs = 25\n",
        "# momentum = 0.7\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# We shouls change the momentum as the network trains, right now it's to low\n",
        "#optimizer = optim.SGD(siamese.parameters(), lr = 0.1, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "# optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=lr_decay_rate)\n",
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4oWq6bmz5Z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XANF2TNnfJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create writer for tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter \n",
        "\n",
        "dt = amsterdam.localize(datetime.now()).strftime(datetime_format)\n",
        "logs_path = \"./logs/\" + dt + \"/\"\n",
        "save_path = \"./saves/\" + dt + \"/\"\n",
        "try:\n",
        "    os.makedirs(save_path)\n",
        "except:\n",
        "    pass\n",
        "writer = SummaryWriter(log_dir=logs_path, comment=\"Simese local testbench\", flush_secs=1)\n",
        "\n",
        "\n",
        "model_save_path = os.path.join(save_path, \"best.th\")\n",
        "checkpoint_path = os.path.join(save_path, \"checkpoint.th\")\n",
        "\n",
        "## Time to train now :)\n",
        "train_loader = data.DataLoader(train_d, batch_size=128, shuffle=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(valid_d, shuffle=True, batch_size=128, pin_memory=True, num_workers=4)\n",
        "\n",
        "train(siamese, train_loader, val_loader, model_save_path, checkpoint_path, validate_every=1) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[1,     1] loss: 0.689 lr: 0.00000159800000\n[1,    51] loss: 0.636 lr: 0.00003149800000\n[1,   101] loss: 0.594 lr: 0.00006139800000\n[1,   151] loss: 0.580 lr: 0.00009129800000\n[1,   201] loss: 0.572 lr: 0.00012119800000\nValidating model!\nAccuracy of the network on the val set 65.430 %\nSaving best model\n[2,     1] loss: 0.585 lr: 0.00014212800000\n[2,    51] loss: 0.534 lr: 0.00017202800000\n[2,   101] loss: 0.532 lr: 0.00020192800000\n[2,   151] loss: 0.533 lr: 0.00023182800000\n[2,   201] loss: 0.527 lr: 0.00026172800000\nValidating model!\nAccuracy of the network on the val set 74.660 %\nSaving best model\n[3,     1] loss: 0.540 lr: 0.00028265800000\n[3,    51] loss: 0.503 lr: 0.00028744200000\n[3,   101] loss: 0.492 lr: 0.00025754200000\n[3,   151] loss: 0.482 lr: 0.00022764200000\n[3,   201] loss: 0.471 lr: 0.00019774200000\nValidating model!\nAccuracy of the network on the val set 78.790 %\nSaving best model\n[4,     1] loss: 0.444 lr: 0.00017681200000\n[4,    51] loss: 0.413 lr: 0.00014691200000\n[4,   101] loss: 0.402 lr: 0.00011701200000\n[4,   151] loss: 0.398 lr: 0.00008711200000\n[4,   201] loss: 0.388 lr: 0.00005721200000\nValidating model!\nAccuracy of the network on the val set 82.880 %\nSaving best model\n[5,     1] loss: 0.364 lr: 0.00003628200000\n[5,    51] loss: 0.313 lr: 0.00000638200000\n[5,   101] loss: 0.316 lr: 0.00002551800000\n[5,   151] loss: 0.316 lr: 0.00005541800000\n[5,   201] loss: 0.314 lr: 0.00008531800000\nValidating model!\nAccuracy of the network on the val set 82.360 %\n[6,     1] loss: 0.302 lr: 0.00010624800000\n[6,    51] loss: 0.319 lr: 0.00013614800000\n[6,   101] loss: 0.325 lr: 0.00016604800000\n[6,   151] loss: 0.328 lr: 0.00019594800000\n[6,   201] loss: 0.336 lr: 0.00022584800000\nValidating model!\nAccuracy of the network on the val set 80.030 %\n[7,     1] loss: 0.408 lr: 0.00024677800000\n[7,    51] loss: 0.354 lr: 0.00027667800000\n[7,   101] loss: 0.353 lr: 0.00029342200000\n[7,   151] loss: 0.348 lr: 0.00026352200000\n[7,   201] loss: 0.344 lr: 0.00023362200000\nValidating model!\nAccuracy of the network on the val set 82.010 %\n[8,     1] loss: 0.285 lr: 0.00021269200000\n[8,    51] loss: 0.275 lr: 0.00018279200000\n[8,   101] loss: 0.265 lr: 0.00015289200000\n[8,   151] loss: 0.259 lr: 0.00012299200000\n[8,   201] loss: 0.255 lr: 0.00009309200000\nValidating model!\nAccuracy of the network on the val set 87.170 %\nSaving best model\n[9,     1] loss: 0.164 lr: 0.00007216200000\n[9,    51] loss: 0.164 lr: 0.00004226200000\n[9,   101] loss: 0.167 lr: 0.00001236200000\n[9,   151] loss: 0.165 lr: 0.00001953800000\n[9,   201] loss: 0.165 lr: 0.00004943800000\nValidating model!\nAccuracy of the network on the val set 87.200 %\nSaving best model\n[10,     1] loss: 0.152 lr: 0.00007036800000\n[10,    51] loss: 0.146 lr: 0.00010026800000\n[10,   101] loss: 0.152 lr: 0.00013016800000\n[10,   151] loss: 0.160 lr: 0.00016006800000\n[10,   201] loss: 0.168 lr: 0.00018996800000\nValidating model!\nAccuracy of the network on the val set 86.220 %\n[11,     1] loss: 0.208 lr: 0.00021089800000\n[11,    51] loss: 0.189 lr: 0.00024079800000\n[11,   101] loss: 0.195 lr: 0.00027069800000\n[11,   151] loss: 0.205 lr: 0.00029940200000\n[11,   201] loss: 0.211 lr: 0.00026950200000\nValidating model!\nAccuracy of the network on the val set 85.940 %\n[12,     1] loss: 0.201 lr: 0.00024857200000\n[12,    51] loss: 0.168 lr: 0.00021867200000\n[12,   101] loss: 0.159 lr: 0.00018877200000\n[12,   151] loss: 0.161 lr: 0.00015887200000\n[12,   201] loss: 0.157 lr: 0.00012897200000\nValidating model!\nAccuracy of the network on the val set 89.020 %\nSaving best model\n[13,     1] loss: 0.097 lr: 0.00010804200000\n[13,    51] loss: 0.081 lr: 0.00007814200000\n[13,   101] loss: 0.079 lr: 0.00004824200000\n[13,   151] loss: 0.079 lr: 0.00001834200000\n[13,   201] loss: 0.077 lr: 0.00001355800000\nValidating model!\nAccuracy of the network on the val set 89.540 %\nSaving best model\n[14,     1] loss: 0.073 lr: 0.00003448800000\n[14,    51] loss: 0.056 lr: 0.00006438800000\n[14,   101] loss: 0.059 lr: 0.00009428800000\n[14,   151] loss: 0.061 lr: 0.00012418800000\n[14,   201] loss: 0.064 lr: 0.00015408800000\nValidating model!\nAccuracy of the network on the val set 89.220 %\n[15,     1] loss: 0.088 lr: 0.00017501800000\n[15,    51] loss: 0.063 lr: 0.00020491800000\n[15,   101] loss: 0.069 lr: 0.00023481800000\n[15,   151] loss: 0.077 lr: 0.00026471800000\n[15,   201] loss: 0.093 lr: 0.00029461800000\nValidating model!\nAccuracy of the network on the val set 87.810 %\n[16,     1] loss: 0.163 lr: 0.00028445200000\n[16,    51] loss: 0.131 lr: 0.00025455200000\n[16,   101] loss: 0.119 lr: 0.00022465200000\n[16,   151] loss: 0.116 lr: 0.00019475200000\n[16,   201] loss: 0.113 lr: 0.00016485200000\nValidating model!\nAccuracy of the network on the val set 89.750 %\nSaving best model\n[17,     1] loss: 0.040 lr: 0.00014392200000\n[17,    51] loss: 0.048 lr: 0.00011402200000\n[17,   101] loss: 0.047 lr: 0.00008412200000\n[17,   151] loss: 0.046 lr: 0.00005422200000\n[17,   201] loss: 0.045 lr: 0.00002432200000\nValidating model!\nAccuracy of the network on the val set 90.810 %\nSaving best model\n[18,     1] loss: 0.032 lr: 0.00000339200000\n[18,    51] loss: 0.030 lr: 0.00002850800000\n[18,   101] loss: 0.030 lr: 0.00005840800000\n[18,   151] loss: 0.030 lr: 0.00008830800000\n[18,   201] loss: 0.030 lr: 0.00011820800000\nValidating model!\nAccuracy of the network on the val set 90.650 %\n[19,     1] loss: 0.031 lr: 0.00013913800000\n[19,    51] loss: 0.028 lr: 0.00016903800000\n[19,   101] loss: 0.028 lr: 0.00019893800000\n[19,   151] loss: 0.030 lr: 0.00022883800000\n[19,   201] loss: 0.035 lr: 0.00025873800000\nValidating model!\nAccuracy of the network on the val set 88.710 %\n[20,     1] loss: 0.074 lr: 0.00027966800000\n[20,    51] loss: 0.100 lr: 0.00029043200000\n[20,   101] loss: 0.109 lr: 0.00026053200000\n[20,   151] loss: 0.113 lr: 0.00023063200000\n[20,   201] loss: 0.112 lr: 0.00020073200000\nValidating model!\nAccuracy of the network on the val set 89.840 %\n[21,     1] loss: 0.073 lr: 0.00017980200000\n[21,    51] loss: 0.052 lr: 0.00014990200000\n[21,   101] loss: 0.047 lr: 0.00012000200000\n[21,   151] loss: 0.044 lr: 0.00009010200000\n[21,   201] loss: 0.042 lr: 0.00006020200000\nValidating model!\nAccuracy of the network on the val set 91.020 %\nSaving best model\n[22,     1] loss: 0.024 lr: 0.00003927200000\n[22,    51] loss: 0.023 lr: 0.00000937200000\n[22,   101] loss: 0.023 lr: 0.00002252800000\n[22,   151] loss: 0.023 lr: 0.00005242800000\n[22,   201] loss: 0.023 lr: 0.00008232800000\nValidating model!\nAccuracy of the network on the val set 91.160 %\nSaving best model\n[23,     1] loss: 0.022 lr: 0.00010325800000\n[23,    51] loss: 0.020 lr: 0.00013315800000\n[23,   101] loss: 0.020 lr: 0.00016305800000\n[23,   151] loss: 0.021 lr: 0.00019295800000\n[23,   201] loss: 0.021 lr: 0.00022285800000\nValidating model!\nAccuracy of the network on the val set 89.610 %\n[24,     1] loss: 0.042 lr: 0.00024378800000\n[24,    51] loss: 0.029 lr: 0.00027368800000\n[24,   101] loss: 0.035 lr: 0.00029641200000\n[24,   151] loss: 0.044 lr: 0.00026651200000\n[24,   201] loss: 0.058 lr: 0.00023661200000\nValidating model!\nAccuracy of the network on the val set 89.750 %\n[25,     1] loss: 0.065 lr: 0.00021568200000\n[25,    51] loss: 0.068 lr: 0.00018578200000\n[25,   101] loss: 0.060 lr: 0.00015588200000\n[25,   151] loss: 0.056 lr: 0.00012598200000\n[25,   201] loss: 0.052 lr: 0.00009608200000\nValidating model!\nAccuracy of the network on the val set 91.290 %\nSaving best model\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sebOdYR32Pg",
        "colab_type": "text"
      },
      "source": [
        "# Validating on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf68u8z0q8XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load best model for testing\n",
        "#siamese.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "siamese.load_state_dict(torch.load(\"./saves/2020-04-14-T-(14-35-49)/best.th\", map_location=device))\n",
        "# create data loader for test set\n",
        "test_loader = data.DataLoader(test_d, shuffle=True, batch_size=128, pin_memory=True, num_workers=4)\n",
        "validate(siamese, test_loader)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Validating model!\nAccuracy of the network on the val set 87.900 %\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "87.9"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}