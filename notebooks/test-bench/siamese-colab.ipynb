{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cT7lOrijlJs",
        "colab_type": "text"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Twhmbb8kXNQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M1FkjdQluR8",
        "colab_type": "text"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qac1GqFnl58c",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mcpj2P3l8So",
        "colab_type": "text"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uAN_OrVe3HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoqpb1oxoEpU",
        "colab_type": "text"
      },
      "source": [
        "## Definition of the dataset class that will hold our examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtx4GZl_oJy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    \"\"\"Dataset that reads the data from an npy file and \n",
        "    returns a pair to the loader\"\"\"\n",
        "    def __init__(self, data_path=None, labels_path=None, \n",
        "                 transform=None, dataset: data.Dataset =None, \n",
        "                 data : np.ndarray = None, labels: np.ndarray = None,\n",
        "                 mean : float = None, std : float = None,\n",
        "                 transform_data=False):\n",
        "        self.transform_data = transform_data\n",
        "\n",
        "        # If we're given another dataset, just take that\n",
        "        if dataset is not None:\n",
        "            self.data = dataset.data\n",
        "            self.labels = dataset.labels\n",
        "            self.transforms = dataset.transforms\n",
        "\n",
        "        # We can also pass the data and labels as an array\n",
        "        elif data is not None:\n",
        "            self.data = data\n",
        "            self.labels = labels\n",
        "\n",
        "            self.mean = mean\n",
        "            self.std = std \n",
        "\n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                self.normalize\n",
        "            ])\n",
        "\n",
        "        # If not, that means that we load it from a file\n",
        "        else:\n",
        "            # Load the data and labels\n",
        "            self.data = np.load(data_path)\n",
        "            self.labels = np.load(labels_path)\n",
        "\n",
        "            # for training set, calculate mean and std\n",
        "            # to normalize\n",
        "            if mean == None and std == None:\n",
        "                # stats of the dataset\n",
        "                self.mean = np.mean(self.data[:,:,:])\n",
        "                self.std = np.std(self.data[:,:,:])\n",
        "            # for test set, use mean and std from\n",
        "            # the train set to normalize\n",
        "            else:\n",
        "                self.mean = mean\n",
        "                self.std = std\n",
        "            # Normalize by default!\n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            # We apply the transformations that are given, so we can \n",
        "            # join the datasets\n",
        "\n",
        "            if transform is not None:\n",
        "              # If we're given transforms it means\n",
        "              # that we're trying to apply the affine transformations\n",
        "              self.transforms = transforms.Compose([\n",
        "                  transform\n",
        "              ])\n",
        "            else:\n",
        "              # If we're not given transforms just return the\n",
        "              # normalized tensor\n",
        "              print(\"Using the default transformations\")\n",
        "              self.transforms = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    self.normalize                                \n",
        "              ])\n",
        "              \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def get_images(self, index):\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        return Image.fromarray(_x1), Image.fromarray(_x2), label\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Gets the next pair from \n",
        "        the dataset and its corresponding label\n",
        "        (0 or 1 depending on if they're the same\n",
        "        or a different letter)\"\"\"\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        # Convert to PIL Images so \n",
        "        # we can transform them with affine transforms\n",
        "        # Just needed to generate the dataset\n",
        "        if self.transform_data:\n",
        "            _x1 = Image.fromarray(_x1)\n",
        "            _x2 = Image.fromarray(_x2)\n",
        "            \n",
        "            # we need to convert the x's to images to apply the transforms\n",
        "            return np.array(self.transforms(_x1)), np.array(self.transforms(_x2)), label\n",
        "        else:\n",
        "          # We're trying to train the dataset, so give\n",
        "          # the data in float32 version that's better for training\n",
        "          # and apply the ToTensor and normalization transformations\n",
        "            _x1 = _x1.astype(np.float32)\n",
        "            _x2 = _x2.astype(np.float32)\n",
        "            label = label.astype(np.float32)\n",
        "            return self.transforms(_x1), self.transforms(_x2), label\n",
        "    \n",
        "# Some easy functions to visualize the data \n",
        "def show_pair(x1, x2, lab):\n",
        "    \"\"\"Function to show two images of the dataset side by side\"\"\"\n",
        "    # x1 = x1.numpy()\n",
        "    # x2 = x2.numpy()\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(x1.squeeze())\n",
        "    ax2.imshow(x2.squeeze())\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    \n",
        "def show_image_pair(i1, i2, lab):\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(i1)\n",
        "    ax2.imshow(i2)\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSgUcaaokfH",
        "colab_type": "text"
      },
      "source": [
        "### Set up the folder in Google Drive and define the data path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6auieDfogWa",
        "colab_type": "code",
        "outputId": "32d03a5c-1efa-4c22-ef1f-870f938002d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/Siamese/\"\n",
        "\n",
        "# Change the current directory to the path so it's more comfortable to work\n",
        "path = \"/content/drive/My Drive/Siamese/\"\n",
        "os.chdir(path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "saved_models\t trainX_30k.npy  validationX.npy\n",
            "trained_net.pkl  trainY_30k.npy  validationY.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ioF-p-sxTg",
        "colab_type": "text"
      },
      "source": [
        "## In case we want to augment the dataset we can run the function defined below\n",
        "\n",
        "For each sample the fucntion generates 8x affine transformed samples of that pair and returns a new dataset with the original images and the augmented samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGt6Iyi7rSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case we want to create affine transformations...\n",
        "import gc\n",
        "\n",
        "\n",
        "def augment_dataset(d: SiameseDataset) -> SiameseDataset:\n",
        "  \"\"\" Augments the dataset and returns a siamese dataset\n",
        "  with 9x as much data, the original data in the argument dataset\n",
        "  plus 8 affine transformations of that input data\"\"\"\n",
        "  # Create a data loader of the dataset\n",
        "  loader = data.DataLoader(d, batch_size=15000)\n",
        "\n",
        "  # Altered samples of the input data\n",
        "  _altered = None\n",
        "  mean = None\n",
        "  std = None\n",
        "\n",
        "  # Check the size of the batches and so on\n",
        "  # Read in batches of 15000, and do it \n",
        "  for j in range(8):\n",
        "      gc.collect()\n",
        "      print(\"starting with round \",j)\n",
        "      for i, (x1, x2, _) in enumerate(loader):\n",
        "          if i % 1 == 0:\n",
        "              print(i)\n",
        "          x1 = np.expand_dims(x1, 1)\n",
        "          x2 = np.expand_dims(x2, 1)\n",
        "          # concatenate the arrays by their second axis\n",
        "          _data = np.concatenate((x1,x2), axis = 1)\n",
        "          _mean = np.mean(_data)\n",
        "          _std = np.std(_data)\n",
        "          if mean is None:\n",
        "            mean = _mean\n",
        "            std = _std\n",
        "          else:\n",
        "            mean = (mean*len(_altered) +  _mean*len(_data))/(len(_altered)+len(_data))\n",
        "            std = (std*len(_altered) +  _std*len(_data))/(len(_altered)+len(_data))\n",
        "          # add them to the dataset\n",
        "          if _altered is None:\n",
        "              _altered = _data\n",
        "          else:\n",
        "              # Concatenate the existing data and the new batch\n",
        "              _altered = np.concatenate((_altered, _data), axis = 0)\n",
        "      \n",
        "      print(f'Size of the datasets -> {_altered.shape}')\n",
        "\n",
        "  # Now create a new dataset with the newly defined data\n",
        "  # Concatenate the original dataset with the new one\n",
        "  all_data = np.concatenate((d.data, _altered), axis = 0)\n",
        "  labels = np.tile(d.labels, 9)\n",
        "  # Add mean of the original datset\n",
        "  mean = (mean*len(_altered) +  d.mean*len(d))/(len(_altered)+len(d))\n",
        "  std = (std*len(_altered) +  d.std*len(d))/(len(_altered)+len(d))\n",
        "  d = SiameseDataset(data = all_data, labels = labels, mean = mean, std = std)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkzkzXACLghZ",
        "colab_type": "text"
      },
      "source": [
        "## Load the training and validation data\n",
        "\n",
        "In case we want to work with the affine transformations we use the part of the code that calls transform dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bug6aTZxosI4",
        "colab_type": "code",
        "outputId": "deaa4730-fa8a-4b63-90b9-69b7e1b2e1d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# define the paths of the data from the work dir\n",
        "data_path = \"trainX_30k.npy\"\n",
        "labels_path = \"trainY_30k.npy\"\n",
        "\n",
        "# Affine transformations to be done on the data\n",
        "affine = transforms.RandomAffine(degrees = (-10,10), \n",
        "                                 translate=(0.1,0.1),\n",
        "                                 scale = (0.8, 1.2),\n",
        "                                 shear = (-0.3, 0.3), \n",
        "                                 fillcolor=255)\n",
        "\n",
        "\n",
        "# Create a dataset with the 30K examples without\n",
        "# affine tranbsformations \n",
        "# d = SiameseDataset(data_path, labels_path)\n",
        "\n",
        "validation = SiameseDataset(\"validationX.npy\", \"validationY.npy\")\n",
        "print(f\"Mean of {validation.mean} and {validation.std}\")\n",
        "\n",
        "# In order to augment the dataset the dataset has to be created with\n",
        "# some extra parameters and call the function defined below\n",
        "d = SiameseDataset(data_path, labels_path, transform_data=True, transform=affine)\n",
        "print(f\"Mean of {d.mean} and {d.std}\")\n",
        "print(\"Loaded data\")\n",
        "d = augment_dataset(d)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default transformations\n",
            "Mean of 237.0979106122449 and 65.15019562081416\n",
            "Mean of 233.89711437641722 and 70.25598944126823\n",
            "Loaded data\n",
            "starting with round  0\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.90603224489797 and std is 70.24248217352327\n",
            "New Mean is 233.90603224489797 and std is 70.24248217352327\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.5728121995465 and std is 70.7446712627871\n",
            "New Mean is 233.73942222222223 and std is 70.49357671815518\n",
            "Size of the datasets -> (30000, 2, 105, 105)\n",
            "starting with round  1\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.8679198639456 and std is 70.30018224597457\n",
            "New Mean is 233.78225476946335 and std is 70.42911189409499\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.53556176870748 and std is 70.80049180898854\n",
            "New Mean is 233.7205815192744 and std is 70.52195687281836\n",
            "Size of the datasets -> (60000, 2, 105, 105)\n",
            "starting with round  2\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.83446802721087 and std is 70.35077049592383\n",
            "New Mean is 233.7433588208617 and std is 70.48771959743947\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.5629676190476 and std is 70.7594297591554\n",
            "New Mean is 233.7132936205593 and std is 70.53300462439212\n",
            "Size of the datasets -> (90000, 2, 105, 105)\n",
            "starting with round  3\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.89080702947845 and std is 70.26554048476616\n",
            "New Mean is 233.73865267897634 and std is 70.49479546158841\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.53889315192743 and std is 70.79550225200829\n",
            "New Mean is 233.71368273809523 and std is 70.53238381039088\n",
            "Size of the datasets -> (120000, 2, 105, 105)\n",
            "starting with round  4\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.92174009070294 and std is 70.21868153181768\n",
            "New Mean is 233.7368002217183 and std is 70.49752800166054\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.5723688888889 and std is 70.74533594732179\n",
            "New Mean is 233.72035708843538 and std is 70.52230879622665\n",
            "Size of the datasets -> (150000, 2, 105, 105)\n",
            "starting with round  5\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.84513369614513 and std is 70.33464679053294\n",
            "New Mean is 233.731700416409 and std is 70.50524861389086\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.53812217687076 and std is 70.7966570199683\n",
            "New Mean is 233.7155688964475 and std is 70.52953264773065\n",
            "Size of the datasets -> (180000, 2, 105, 105)\n",
            "starting with round  6\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.8714262585034 and std is 70.29487659660381\n",
            "New Mean is 233.72755792429794 and std is 70.51148218225936\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.56062154195013 and std is 70.76294622283095\n",
            "New Mean is 233.71563389698738 and std is 70.52944389944304\n",
            "Size of the datasets -> (210000, 2, 105, 105)\n",
            "starting with round  7\n",
            "0\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.87383324263038 and std is 70.29123417090545\n",
            "New Mean is 233.72618052003025 and std is 70.51356325087387\n",
            "1\n",
            "Shapes:  torch.Size([15000, 105, 105]) torch.Size([15000, 105, 105])\n",
            "Mean is 233.554731292517 and std is 70.77177382595382\n",
            "New Mean is 233.71546494331068 and std is 70.52970141181638\n",
            "Size of the datasets -> (240000, 2, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlU4gaHlY5t",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsEL_whslWv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = torch.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = self.prediction_layer(h)\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "torch.manual_seed(12)\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  np.random.seed(12)\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj8AwypRYxFe",
        "colab_type": "text"
      },
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPtfP3AUfhj",
        "colab_type": "code",
        "outputId": "b8e6dcf2-b922-42e6-9e45-863bc84b1fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNet(\n",
              "  (convnet): ConvNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
              "    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  )\n",
              "  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtINTkF9mZUC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_10kSyZO0p",
        "colab_type": "text"
      },
      "source": [
        "### Define the Loss (CrossEntropy) and the Adam optimizer\n",
        "\n",
        "We set two different weight decay rates as done in the keras code, as it certainly shows really good results this way, as well as a fixed (could be reduced in the future) learning rate of 3e-4 using the Adam Optimizer\n",
        "\n",
        "We choose BCEWithLogits in order to improve the stability of teh network compared to when we use just BCE, since it makes use of the log sum exp trick thus avoiding underflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlxZADgYuMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "# Learning rate decay per epoch\n",
        "# lr_decay_rate = 0.9\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# We shouls change the momentum as the network trains, right now it's to low\n",
        "#optimizer = optim.SGD(siamese.parameters(), lr = 0.1, momentum=0.7, weight_decay=2e-4)\n",
        "optimizer = optim.Adam([\n",
        "                        {\"params\": siamese.convnet.parameters()},\n",
        "                        {\"params\": siamese.prediction_layer.parameters(), \"weight_decay\": 1e-3},\n",
        "],lr = 3e-4, weight_decay = 2e-4)\n",
        "# optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = lr_decay_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8kcukdmc5b",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfozeCIjo1iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Send the network to the GPU if available\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "siamese.to(device)\n",
        "\n",
        "\n",
        "def train(validate_every = 10 , save_every = 50):\n",
        "  \"\"\" Train the network with two parameters, one is how often should we validate\n",
        "  and the other is how often should we save a checkpoint\"\"\"\n",
        "\n",
        "  best_accuracy = 0\n",
        "  siamese.train()\n",
        "\n",
        "  # define the loader of the dataset\n",
        "\n",
        "\n",
        "  for epoch in range(200):\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    \n",
        "    for X1, X2, y in train_loader:\n",
        "      X1 = X1.to(device)\n",
        "      X2 = X2.to(device)\n",
        "      y = y.to(device)\n",
        "      \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      X1 = X1.view(-1, 1, 105, 105)\n",
        "      X2 = X2.view(-1, 1, 105, 105)\n",
        "      y = y.view(-1,1)\n",
        "\n",
        "      \n",
        "\n",
        "      outputs = siamese(X1, X2)\n",
        "      # print(torch.max(outputs))\n",
        "      # print(torch.min(outputs))\n",
        "      #print(outputs.max(), outputs.min())\n",
        "      \n",
        "      loss = criterion(outputs , y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      if i % 50 == 0:\n",
        "        print('[%d, %5d] loss: %.3f lr: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / (i+1), optimizer.param_groups[0]['lr']))\n",
        "      i+=1\n",
        "      \n",
        "    # Update the learning rate\n",
        "    # optim_scheduler.step()\n",
        "\n",
        "    if epoch % validate_every == 0:\n",
        "      accuracy = validate(siamese, criterion)\n",
        "      # Put the model back in training mode\n",
        "      siamese.train()\n",
        "      if accuracy > best_accuracy:\n",
        "        print(\"Saving best model\")\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(siamese.state_dict(), os.path.join(\"saved_models\", \"best.th\"))\n",
        "      \n",
        "\n",
        "    if epoch > 0 and epoch % save_every == 0:\n",
        "      torch.save(siamese.state_dict(), os.path.join(\"saved_models\", \"checkpoint.th\"))\n",
        "        \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkbTiqs7MlZ2",
        "colab_type": "text"
      },
      "source": [
        "## Validation Loop\n",
        "\n",
        "In this validation loop we loop through the validation set and calculate the accuracy of the model.\n",
        "\n",
        "We can do this as often as it is said in the training loop. For a thorough evaluation we can use validate_every= 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABqSLMQRCUys",
        "colab_type": "code",
        "outputId": "7d0f8a82-7bd8-4456-c43b-cb0b2ec91fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation set and checkpoint save\n",
        "\n",
        "\n",
        "print(\"Loaded validation set with shape \",validation.data.shape)\n",
        "\n",
        "def validate(model, criterion):\n",
        "  \"\"\" Validates the model and computes the accuracy\"\"\"\n",
        "  model.eval()\n",
        "  print(\"Validating model!\")\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for x1, x2, y in val_loader:\n",
        "\n",
        "      # Send data to device\n",
        "      x1 = x1.to(device)\n",
        "      x2= x2.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # Appropriate view\n",
        "      x1 = x1.view(-1, 1, 105, 105)\n",
        "      x2 = x2.view(-1, 1, 105, 105)\n",
        "      y = y.view(-1,1)\n",
        "\n",
        "      outputs = model(x1, x2)\n",
        "      # Translate the outputs to 0 or 1\n",
        "      predicted = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "      total += y.size(0)\n",
        "      correct += (predicted == y).sum().item()\n",
        "    \n",
        "    # return the accuracy\n",
        "    print(\"Accuracy of the network on the val set %.3f %%\" % (100*correct /total))\n",
        "    return 100*correct/total\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded validation set with shape  (10000, 2, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4oWq6bmz5Z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XANF2TNnfJb",
        "colab_type": "code",
        "outputId": "e0eb1c5b-dd0b-4760-ab06-16cf43a81bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_loader = data.DataLoader(validation, shuffle=True, batch_size=128, pin_memory=True, num_workers=4)\n",
        "train_loader = data.DataLoader(d, batch_size=128, shuffle=True, pin_memory=True, num_workers=4)\n",
        "print(d.data.shape)\n",
        "train(validate_every=1) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(270000, 2, 105, 105)\n",
            "[1,     1] loss: 0.727 lr: 0.00030\n",
            "[1,    51] loss: 0.694 lr: 0.00030\n",
            "[1,   101] loss: 0.694 lr: 0.00030\n",
            "[1,   151] loss: 0.692 lr: 0.00030\n",
            "[1,   201] loss: 0.690 lr: 0.00030\n",
            "[1,   251] loss: 0.689 lr: 0.00030\n",
            "[1,   301] loss: 0.687 lr: 0.00030\n",
            "[1,   351] loss: 0.686 lr: 0.00030\n",
            "[1,   401] loss: 0.686 lr: 0.00030\n",
            "[1,   451] loss: 0.685 lr: 0.00030\n",
            "[1,   501] loss: 0.685 lr: 0.00030\n",
            "[1,   551] loss: 0.685 lr: 0.00030\n",
            "[1,   601] loss: 0.684 lr: 0.00030\n",
            "[1,   651] loss: 0.683 lr: 0.00030\n",
            "[1,   701] loss: 0.682 lr: 0.00030\n",
            "[1,   751] loss: 0.682 lr: 0.00030\n",
            "[1,   801] loss: 0.681 lr: 0.00030\n",
            "[1,   851] loss: 0.680 lr: 0.00030\n",
            "[1,   901] loss: 0.676 lr: 0.00030\n",
            "[1,   951] loss: 0.673 lr: 0.00030\n",
            "[1,  1001] loss: 0.669 lr: 0.00030\n",
            "[1,  1051] loss: 0.665 lr: 0.00030\n",
            "[1,  1101] loss: 0.661 lr: 0.00030\n",
            "[1,  1151] loss: 0.658 lr: 0.00030\n",
            "[1,  1201] loss: 0.654 lr: 0.00030\n",
            "[1,  1251] loss: 0.650 lr: 0.00030\n",
            "[1,  1301] loss: 0.647 lr: 0.00030\n",
            "[1,  1351] loss: 0.644 lr: 0.00030\n",
            "[1,  1401] loss: 0.641 lr: 0.00030\n",
            "[1,  1451] loss: 0.638 lr: 0.00030\n",
            "[1,  1501] loss: 0.635 lr: 0.00030\n",
            "[1,  1551] loss: 0.631 lr: 0.00030\n",
            "[1,  1601] loss: 0.629 lr: 0.00030\n",
            "[1,  1651] loss: 0.626 lr: 0.00030\n",
            "[1,  1701] loss: 0.623 lr: 0.00030\n",
            "[1,  1751] loss: 0.620 lr: 0.00030\n",
            "[1,  1801] loss: 0.617 lr: 0.00030\n",
            "[1,  1851] loss: 0.615 lr: 0.00030\n",
            "[1,  1901] loss: 0.612 lr: 0.00030\n",
            "[1,  1951] loss: 0.609 lr: 0.00030\n",
            "[1,  2001] loss: 0.606 lr: 0.00030\n",
            "[1,  2051] loss: 0.603 lr: 0.00030\n",
            "[1,  2101] loss: 0.600 lr: 0.00030\n",
            "Validating model!\n",
            "Accuracy of the network on the val set 74.090 %\n",
            "Saving best model\n",
            "[2,     1] loss: 0.494 lr: 0.00030\n",
            "[2,    51] loss: 0.471 lr: 0.00030\n",
            "[2,   101] loss: 0.462 lr: 0.00030\n",
            "[2,   151] loss: 0.459 lr: 0.00030\n",
            "[2,   201] loss: 0.453 lr: 0.00030\n",
            "[2,   251] loss: 0.445 lr: 0.00030\n",
            "[2,   301] loss: 0.435 lr: 0.00030\n",
            "[2,   351] loss: 0.425 lr: 0.00030\n",
            "[2,   401] loss: 0.416 lr: 0.00030\n",
            "[2,   451] loss: 0.409 lr: 0.00030\n",
            "[2,   501] loss: 0.401 lr: 0.00030\n",
            "[2,   551] loss: 0.392 lr: 0.00030\n",
            "[2,   601] loss: 0.383 lr: 0.00030\n",
            "[2,   651] loss: 0.376 lr: 0.00030\n",
            "[2,   701] loss: 0.369 lr: 0.00030\n",
            "[2,   751] loss: 0.361 lr: 0.00030\n",
            "[2,   801] loss: 0.354 lr: 0.00030\n",
            "[2,   851] loss: 0.347 lr: 0.00030\n",
            "[2,   901] loss: 0.340 lr: 0.00030\n",
            "[2,   951] loss: 0.334 lr: 0.00030\n",
            "[2,  1001] loss: 0.327 lr: 0.00030\n",
            "[2,  1051] loss: 0.322 lr: 0.00030\n",
            "[2,  1101] loss: 0.317 lr: 0.00030\n",
            "[2,  1151] loss: 0.312 lr: 0.00030\n",
            "[2,  1201] loss: 0.307 lr: 0.00030\n",
            "[2,  1251] loss: 0.302 lr: 0.00030\n",
            "[2,  1301] loss: 0.298 lr: 0.00030\n",
            "[2,  1351] loss: 0.293 lr: 0.00030\n",
            "[2,  1401] loss: 0.288 lr: 0.00030\n",
            "[2,  1451] loss: 0.285 lr: 0.00030\n",
            "[2,  1501] loss: 0.281 lr: 0.00030\n",
            "[2,  1551] loss: 0.277 lr: 0.00030\n",
            "[2,  1601] loss: 0.274 lr: 0.00030\n",
            "[2,  1651] loss: 0.270 lr: 0.00030\n",
            "[2,  1701] loss: 0.267 lr: 0.00030\n",
            "[2,  1751] loss: 0.264 lr: 0.00030\n",
            "[2,  1801] loss: 0.260 lr: 0.00030\n",
            "[2,  1851] loss: 0.257 lr: 0.00030\n",
            "[2,  1901] loss: 0.254 lr: 0.00030\n",
            "[2,  1951] loss: 0.251 lr: 0.00030\n",
            "[2,  2001] loss: 0.248 lr: 0.00030\n",
            "[2,  2051] loss: 0.246 lr: 0.00030\n",
            "[2,  2101] loss: 0.243 lr: 0.00030\n",
            "Validating model!\n",
            "Accuracy of the network on the val set 91.740 %\n",
            "Saving best model\n",
            "[3,     1] loss: 0.120 lr: 0.00030\n",
            "[3,    51] loss: 0.105 lr: 0.00030\n",
            "[3,   101] loss: 0.105 lr: 0.00030\n",
            "[3,   151] loss: 0.106 lr: 0.00030\n",
            "[3,   201] loss: 0.107 lr: 0.00030\n",
            "[3,   251] loss: 0.107 lr: 0.00030\n",
            "[3,   301] loss: 0.108 lr: 0.00030\n",
            "[3,   351] loss: 0.108 lr: 0.00030\n",
            "[3,   401] loss: 0.108 lr: 0.00030\n",
            "[3,   451] loss: 0.108 lr: 0.00030\n",
            "[3,   501] loss: 0.108 lr: 0.00030\n",
            "[3,   551] loss: 0.107 lr: 0.00030\n",
            "[3,   601] loss: 0.106 lr: 0.00030\n",
            "[3,   651] loss: 0.106 lr: 0.00030\n",
            "[3,   701] loss: 0.105 lr: 0.00030\n",
            "[3,   751] loss: 0.105 lr: 0.00030\n",
            "[3,   801] loss: 0.105 lr: 0.00030\n",
            "[3,   851] loss: 0.105 lr: 0.00030\n",
            "[3,   901] loss: 0.105 lr: 0.00030\n",
            "[3,   951] loss: 0.104 lr: 0.00030\n",
            "[3,  1001] loss: 0.103 lr: 0.00030\n",
            "[3,  1051] loss: 0.103 lr: 0.00030\n",
            "[3,  1101] loss: 0.102 lr: 0.00030\n",
            "[3,  1151] loss: 0.102 lr: 0.00030\n",
            "[3,  1201] loss: 0.101 lr: 0.00030\n",
            "[3,  1251] loss: 0.101 lr: 0.00030\n",
            "[3,  1301] loss: 0.100 lr: 0.00030\n",
            "[3,  1351] loss: 0.100 lr: 0.00030\n",
            "[3,  1401] loss: 0.100 lr: 0.00030\n",
            "[3,  1451] loss: 0.099 lr: 0.00030\n",
            "[3,  1501] loss: 0.099 lr: 0.00030\n",
            "[3,  1551] loss: 0.098 lr: 0.00030\n",
            "[3,  1601] loss: 0.098 lr: 0.00030\n",
            "[3,  1651] loss: 0.097 lr: 0.00030\n",
            "[3,  1701] loss: 0.097 lr: 0.00030\n",
            "[3,  1751] loss: 0.097 lr: 0.00030\n",
            "[3,  1801] loss: 0.096 lr: 0.00030\n",
            "[3,  1851] loss: 0.096 lr: 0.00030\n",
            "[3,  1901] loss: 0.095 lr: 0.00030\n",
            "[3,  1951] loss: 0.095 lr: 0.00030\n",
            "[3,  2001] loss: 0.094 lr: 0.00030\n",
            "[3,  2051] loss: 0.094 lr: 0.00030\n",
            "[3,  2101] loss: 0.093 lr: 0.00030\n",
            "Validating model!\n",
            "Accuracy of the network on the val set 92.560 %\n",
            "Saving best model\n",
            "[4,     1] loss: 0.031 lr: 0.00030\n",
            "[4,    51] loss: 0.050 lr: 0.00030\n",
            "[4,   101] loss: 0.049 lr: 0.00030\n",
            "[4,   151] loss: 0.047 lr: 0.00030\n",
            "[4,   201] loss: 0.047 lr: 0.00030\n",
            "[4,   251] loss: 0.048 lr: 0.00030\n",
            "[4,   301] loss: 0.048 lr: 0.00030\n",
            "[4,   351] loss: 0.048 lr: 0.00030\n",
            "[4,   401] loss: 0.048 lr: 0.00030\n",
            "[4,   451] loss: 0.048 lr: 0.00030\n",
            "[4,   501] loss: 0.048 lr: 0.00030\n",
            "[4,   551] loss: 0.049 lr: 0.00030\n",
            "[4,   601] loss: 0.050 lr: 0.00030\n",
            "[4,   651] loss: 0.050 lr: 0.00030\n",
            "[4,   701] loss: 0.050 lr: 0.00030\n",
            "[4,   751] loss: 0.051 lr: 0.00030\n",
            "[4,   801] loss: 0.051 lr: 0.00030\n",
            "[4,   851] loss: 0.051 lr: 0.00030\n",
            "[4,   901] loss: 0.051 lr: 0.00030\n",
            "[4,   951] loss: 0.052 lr: 0.00030\n",
            "[4,  1001] loss: 0.052 lr: 0.00030\n",
            "[4,  1051] loss: 0.052 lr: 0.00030\n",
            "[4,  1101] loss: 0.052 lr: 0.00030\n",
            "[4,  1151] loss: 0.052 lr: 0.00030\n",
            "[4,  1201] loss: 0.052 lr: 0.00030\n",
            "[4,  1251] loss: 0.053 lr: 0.00030\n",
            "[4,  1301] loss: 0.053 lr: 0.00030\n",
            "[4,  1351] loss: 0.052 lr: 0.00030\n",
            "[4,  1401] loss: 0.052 lr: 0.00030\n",
            "[4,  1451] loss: 0.052 lr: 0.00030\n",
            "[4,  1501] loss: 0.052 lr: 0.00030\n",
            "[4,  1551] loss: 0.052 lr: 0.00030\n",
            "[4,  1601] loss: 0.052 lr: 0.00030\n",
            "[4,  1651] loss: 0.052 lr: 0.00030\n",
            "[4,  1701] loss: 0.052 lr: 0.00030\n",
            "[4,  1751] loss: 0.052 lr: 0.00030\n",
            "[4,  1801] loss: 0.052 lr: 0.00030\n",
            "[4,  1851] loss: 0.052 lr: 0.00030\n",
            "[4,  1901] loss: 0.052 lr: 0.00030\n",
            "[4,  1951] loss: 0.052 lr: 0.00030\n",
            "[4,  2001] loss: 0.052 lr: 0.00030\n",
            "[4,  2051] loss: 0.052 lr: 0.00030\n",
            "[4,  2101] loss: 0.053 lr: 0.00030\n",
            "Validating model!\n",
            "Accuracy of the network on the val set 93.480 %\n",
            "Saving best model\n",
            "[5,     1] loss: 0.033 lr: 0.00030\n",
            "[5,    51] loss: 0.032 lr: 0.00030\n",
            "[5,   101] loss: 0.029 lr: 0.00030\n",
            "[5,   151] loss: 0.028 lr: 0.00030\n",
            "[5,   201] loss: 0.027 lr: 0.00030\n",
            "[5,   251] loss: 0.027 lr: 0.00030\n",
            "[5,   301] loss: 0.027 lr: 0.00030\n",
            "[5,   351] loss: 0.027 lr: 0.00030\n",
            "[5,   401] loss: 0.026 lr: 0.00030\n",
            "[5,   451] loss: 0.027 lr: 0.00030\n",
            "[5,   501] loss: 0.027 lr: 0.00030\n",
            "[5,   551] loss: 0.027 lr: 0.00030\n",
            "[5,   601] loss: 0.027 lr: 0.00030\n",
            "[5,   651] loss: 0.028 lr: 0.00030\n",
            "[5,   701] loss: 0.028 lr: 0.00030\n",
            "[5,   751] loss: 0.029 lr: 0.00030\n",
            "[5,   801] loss: 0.029 lr: 0.00030\n",
            "[5,   851] loss: 0.029 lr: 0.00030\n",
            "[5,   901] loss: 0.030 lr: 0.00030\n",
            "[5,   951] loss: 0.030 lr: 0.00030\n",
            "[5,  1001] loss: 0.031 lr: 0.00030\n",
            "[5,  1051] loss: 0.032 lr: 0.00030\n",
            "[5,  1101] loss: 0.032 lr: 0.00030\n",
            "[5,  1151] loss: 0.033 lr: 0.00030\n",
            "[5,  1201] loss: 0.033 lr: 0.00030\n",
            "[5,  1251] loss: 0.033 lr: 0.00030\n",
            "[5,  1301] loss: 0.034 lr: 0.00030\n",
            "[5,  1351] loss: 0.034 lr: 0.00030\n",
            "[5,  1401] loss: 0.034 lr: 0.00030\n",
            "[5,  1451] loss: 0.034 lr: 0.00030\n",
            "[5,  1501] loss: 0.035 lr: 0.00030\n",
            "[5,  1551] loss: 0.035 lr: 0.00030\n",
            "[5,  1601] loss: 0.035 lr: 0.00030\n",
            "[5,  1651] loss: 0.035 lr: 0.00030\n",
            "[5,  1701] loss: 0.035 lr: 0.00030\n",
            "[5,  1751] loss: 0.035 lr: 0.00030\n",
            "[5,  1801] loss: 0.035 lr: 0.00030\n",
            "[5,  1851] loss: 0.035 lr: 0.00030\n",
            "[5,  1901] loss: 0.035 lr: 0.00030\n",
            "[5,  1951] loss: 0.036 lr: 0.00030\n",
            "[5,  2001] loss: 0.036 lr: 0.00030\n",
            "[5,  2051] loss: 0.036 lr: 0.00030\n",
            "[5,  2101] loss: 0.036 lr: 0.00030\n",
            "Validating model!\n",
            "Accuracy of the network on the val set 92.260 %\n",
            "[6,     1] loss: 0.028 lr: 0.00030\n",
            "[6,    51] loss: 0.025 lr: 0.00030\n",
            "[6,   101] loss: 0.024 lr: 0.00030\n",
            "[6,   151] loss: 0.023 lr: 0.00030\n",
            "[6,   201] loss: 0.022 lr: 0.00030\n",
            "[6,   251] loss: 0.021 lr: 0.00030\n",
            "[6,   301] loss: 0.021 lr: 0.00030\n",
            "[6,   351] loss: 0.021 lr: 0.00030\n",
            "[6,   401] loss: 0.021 lr: 0.00030\n",
            "[6,   451] loss: 0.021 lr: 0.00030\n",
            "[6,   501] loss: 0.021 lr: 0.00030\n",
            "[6,   551] loss: 0.021 lr: 0.00030\n",
            "[6,   601] loss: 0.021 lr: 0.00030\n",
            "[6,   651] loss: 0.022 lr: 0.00030\n",
            "[6,   701] loss: 0.022 lr: 0.00030\n",
            "[6,   751] loss: 0.023 lr: 0.00030\n",
            "[6,   801] loss: 0.024 lr: 0.00030\n",
            "[6,   851] loss: 0.024 lr: 0.00030\n",
            "[6,   901] loss: 0.024 lr: 0.00030\n",
            "[6,   951] loss: 0.024 lr: 0.00030\n",
            "[6,  1001] loss: 0.024 lr: 0.00030\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}