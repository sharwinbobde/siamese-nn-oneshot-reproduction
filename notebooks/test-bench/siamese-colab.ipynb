{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharwinbobde/siamese-nn-oneshot-reproduction/blob/validation-and-fix/notebooks/test-bench/siamese-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cT7lOrijlJs",
        "colab_type": "text"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Twhmbb8kXNQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M1FkjdQluR8",
        "colab_type": "text"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qac1GqFnl58c",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mcpj2P3l8So",
        "colab_type": "text"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uAN_OrVe3HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoqpb1oxoEpU",
        "colab_type": "text"
      },
      "source": [
        "## Definition of the dataset class that will hold our examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtx4GZl_oJy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    \"\"\"Dataset that reads the data from an npy file and \n",
        "    returns a pair to the loader\"\"\"\n",
        "    def __init__(self, data_path=None, labels_path=None, \n",
        "                 transform=None, dataset: data.Dataset =None, \n",
        "                 data : np.ndarray = None, labels: np.ndarray = None,\n",
        "                 mean : float = None, std : float = None,\n",
        "                 transform_data=False):\n",
        "        self.transform_data = transform_data\n",
        "\n",
        "        # If we're given another dataset, just take that\n",
        "        if dataset is not None:\n",
        "            self.data = dataset.data\n",
        "            self.labels = dataset.labels\n",
        "            self.transforms = dataset.transforms\n",
        "\n",
        "        # We can also pass the data and labels as an array\n",
        "        elif data is not None:\n",
        "            self.data = data\n",
        "            self.labels = labels\n",
        "\n",
        "            self.mean = mean\n",
        "            self.std = std \n",
        "\n",
        "            \n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                self.normalize\n",
        "            ])\n",
        "\n",
        "        # If not, that means that we load it from a file\n",
        "        else:\n",
        "            # Load the data and labels\n",
        "            self.data = np.load(data_path)\n",
        "            self.labels = np.load(labels_path)\n",
        "\n",
        "            # for training set, calculate mean and std\n",
        "            # to normalize\n",
        "            if mean == None and std == None:\n",
        "                # stats of the dataset\n",
        "                self.mean = np.mean(self.data[:,:,:])\n",
        "                self.std = np.std(self.data[:,:,:])\n",
        "            # for test set, use mean and std from\n",
        "            # the train set to normalize\n",
        "            else:\n",
        "                self.mean = mean\n",
        "                self.std = std\n",
        "            # Normalize by default!\n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            # We apply the transformations that are given, so we can \n",
        "            # join the datasets\n",
        "\n",
        "            if transform is not None:\n",
        "              # If we're given transforms it means\n",
        "              # that we're trying to apply the affine transformations\n",
        "              self.transforms = transforms.Compose([\n",
        "                  transform\n",
        "              ])\n",
        "            else:\n",
        "              # If we're not given transforms just return the\n",
        "              # normalized tensor\n",
        "              print(\"Using the default transformations\")\n",
        "              self.transforms = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    self.normalize                                \n",
        "              ])\n",
        "              \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def get_images(self, index):\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        return Image.fromarray(_x1), Image.fromarray(_x2), label\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Gets the next pair from \n",
        "        the dataset and its corresponding label\n",
        "        (0 or 1 depending on if they're the same\n",
        "        or a different letter)\"\"\"\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        # Convert to PIL Images so \n",
        "        # we can transform them with affine transforms\n",
        "        # Just needed to generate the dataset\n",
        "        if self.transform_data:\n",
        "            _x1 = Image.fromarray(_x1)\n",
        "            _x2 = Image.fromarray(_x2)\n",
        "            \n",
        "            # we need to convert the x's to images to apply the transforms\n",
        "            return np.array(self.transforms(_x1)), np.array(self.transforms(_x2)), label\n",
        "        else:\n",
        "          # We're trying to train the dataset, so give\n",
        "          # the data in float32 version that's better for training\n",
        "          # and apply the ToTensor and normalization transformations\n",
        "            _x1 = _x1.astype(np.float32)\n",
        "            _x2 = _x2.astype(np.float32)\n",
        "            label = label.astype(np.float32)\n",
        "            return self.transforms(_x1), self.transforms(_x2), label\n",
        "    \n",
        "# Some easy functions to visualize the data \n",
        "def show_pair(x1, x2, lab):\n",
        "    \"\"\"Function to show two images of the dataset side by side\"\"\"\n",
        "    # x1 = x1.numpy()\n",
        "    # x2 = x2.numpy()\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(x1.squeeze())\n",
        "    ax2.imshow(x2.squeeze())\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    \n",
        "def show_image_pair(i1, i2, lab):\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(i1)\n",
        "    ax2.imshow(i2)\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSgUcaaokfH",
        "colab_type": "text"
      },
      "source": [
        "### Set up the folder in Google Drive and define the data path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6auieDfogWa",
        "colab_type": "code",
        "outputId": "c7df1a5f-8957-4334-ff3c-dc928404d9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/Deep Learning Q3\"\n",
        "\n",
        "# Change the current directory to the path so it's more comfortable to work\n",
        "path = \"/content/drive/My Drive/Deep Learning Q3\"\n",
        "os.chdir(path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "'Choose a Paper.gdoc'   datasets   saved_models   Seminar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ioF-p-sxTg",
        "colab_type": "text"
      },
      "source": [
        "## In case we want to augment the dataset we can run the function defined below\n",
        "\n",
        "For each sample the fucntion generates 8x affine transformed samples of that pair and returns a new dataset with the original images and the augmented samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGt6Iyi7rSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case we want to create affine transformations...\n",
        "import gc\n",
        "\n",
        "\n",
        "def augment_dataset(d: SiameseDataset) -> SiameseDataset:\n",
        "  \"\"\" Augments the dataset and returns a siamese dataset\n",
        "  with 9x as much data, the original data in the argument dataset\n",
        "  plus 8 affine transformations of that input data\"\"\"\n",
        "  # Create a data loader of the dataset\n",
        "  loader = data.DataLoader(d, batch_size=15000)\n",
        "\n",
        "  # Altered samples of the input data\n",
        "  _altered = None\n",
        "  mean = None\n",
        "  std = None\n",
        "\n",
        "  # Check the size of the batches and so on\n",
        "  # Read in batches of 15000, and do it \n",
        "  for j in range(8):\n",
        "      gc.collect()\n",
        "      print(\"starting with round \",j)\n",
        "      for i, (x1, x2, _) in enumerate(loader):\n",
        "          if i % 1 == 0:\n",
        "              print(i)\n",
        "          x1 = np.expand_dims(x1, 1)\n",
        "          x2 = np.expand_dims(x2, 1)\n",
        "          # concatenate the arrays by their second axis\n",
        "          _data = np.concatenate((x1,x2), axis = 1)\n",
        "          _mean = np.mean(_data)\n",
        "          _std = np.std(_data)\n",
        "          if mean is None:\n",
        "            mean = _mean\n",
        "            std = _std\n",
        "          else:\n",
        "            mean = (mean*len(_altered) +  _mean*len(_data))/(len(_altered)+len(_data))\n",
        "            std = (std*len(_altered) +  _std*len(_data))/(len(_altered)+len(_data))\n",
        "          # add them to the dataset\n",
        "          if _altered is None:\n",
        "              _altered = _data\n",
        "          else:\n",
        "              # Concatenate the existing data and the new batch\n",
        "              _altered = np.concatenate((_altered, _data), axis = 0)\n",
        "      \n",
        "      print(f'Size of the datasets -> {_altered.shape}')\n",
        "\n",
        "  # Now create a new dataset with the newly defined data\n",
        "  # Concatenate the original dataset with the new one\n",
        "  all_data = np.concatenate((d.data, _altered), axis = 0)\n",
        "  labels = np.tile(d.labels, 9)\n",
        "  # Add mean of the original datset\n",
        "  mean = (mean*len(_altered) +  d.mean*len(d))/(len(_altered)+len(d))\n",
        "  std = (std*len(_altered) +  d.std*len(d))/(len(_altered)+len(d))\n",
        "  d = SiameseDataset(data = all_data, labels = labels, mean = mean, std = std)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkzkzXACLghZ",
        "colab_type": "text"
      },
      "source": [
        "## Load the training and validation data\n",
        "\n",
        "In case we want to work with the affine transformations we use the part of the code that calls transform dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bug6aTZxosI4",
        "colab_type": "code",
        "outputId": "9f5020bd-fb05-497b-8e92-fc806d22e131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# define the paths of the training data\n",
        "train_data_path = \"datasets/trainX_30k.npy\"\n",
        "train_labels_path = \"datasets/trainY_30k.npy\"\n",
        "\n",
        "# Affine transformations to be done on the train data\n",
        "affine = transforms.RandomAffine(degrees = (-10,10), \n",
        "                                 translate=(0.1,0.1),\n",
        "                                 scale = (0.8, 1.2),\n",
        "                                 shear = (-0.3, 0.3), \n",
        "                                 fillcolor=255)\n",
        "\n",
        "# In order to augment the dataset the dataset has to be created with\n",
        "# some extra parameters and call the function defined below\n",
        "train_d = SiameseDataset(train_data_path, train_labels_path, transform_data=True, transform=affine)\n",
        "print(f\"Mean of {train_d.mean} and {train_d.std}\")\n",
        "print(\"Loaded data\")\n",
        "train_d = augment_dataset(train_d)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean of 236.38998936507937 and 66.32661770415308\n",
            "Loaded data\n",
            "starting with round  0\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (30000, 2, 105, 105)\n",
            "starting with round  1\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (60000, 2, 105, 105)\n",
            "starting with round  2\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (90000, 2, 105, 105)\n",
            "starting with round  3\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (120000, 2, 105, 105)\n",
            "starting with round  4\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (150000, 2, 105, 105)\n",
            "starting with round  5\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (180000, 2, 105, 105)\n",
            "starting with round  6\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (210000, 2, 105, 105)\n",
            "starting with round  7\n",
            "0\n",
            "1\n",
            "Size of the datasets -> (240000, 2, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L09YjjbD9Ia1",
        "colab_type": "code",
        "outputId": "a5b491d4-f0fd-477e-b13c-7bdafab3e2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# validation data\n",
        "valid_data_path = \"datasets/validationX.npy\"\n",
        "valid_labels_path = \"datasets/validationY.npy\"\n",
        "valid_d = SiameseDataset(valid_data_path, valid_labels_path)\n",
        "print(\"Loaded validation set with shape \",valid_d.data.shape)\n",
        "print(f\"Mean of {valid_d.mean} and {valid_d.std}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default transformations\n",
            "Loaded validation set with shape  (10000, 2, 105, 105)\n",
            "Mean of 234.00379959183672 and 70.09415576566917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7HZCDcU5aVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "test_data_path = \"datasets/testX.npy\"\n",
        "test_labels_path = \"datasets/testY.npy\"\n",
        "# test_d = SiameseDataset(test_data_path, test_labels_path)\n",
        "# print(\"Loaded test set with shape \",test_d.data.shape)\n",
        "# print(f\"Mean of {test_d.mean} and {test_d.std}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlU4gaHlY5t",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsEL_whslWv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = torch.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = self.prediction_layer(h)\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "torch.manual_seed(12)\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  np.random.seed(12)\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj8AwypRYxFe",
        "colab_type": "text"
      },
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPtfP3AUfhj",
        "colab_type": "code",
        "outputId": "204e4159-f71d-47a8-ecda-48a04f15bee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)\n",
        "\n",
        "# Send the network to the GPU if available\n",
        "#device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "siamese.to(device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNet(\n",
              "  (convnet): ConvNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
              "    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  )\n",
              "  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtINTkF9mZUC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjJzC3Q3lgRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = 3e-4\n",
        "regularization = 2e-4\n",
        "# Learning rate decay per epoch\n",
        "# lr_decay_rate = 0.9\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    [\n",
        "     {\"params\": siamese.convnet.parameters()},\n",
        "     {\"params\": siamese.prediction_layer.parameters(), \"weight_decay\": 1e-3}\n",
        "    ],\n",
        "    lr = learning_rate,\n",
        "    weight_decay = regularization\n",
        ")\n",
        "\n",
        "n_epochs = 200\n",
        "# momentum = 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_10kSyZO0p",
        "colab_type": "text"
      },
      "source": [
        "### Define the Loss (CrossEntropy) and the Adam optimizer\n",
        "\n",
        "We set two different weight decay rates as done in the keras code, as it certainly shows really good results this way, as well as a fixed (could be reduced in the future) learning rate of 3e-4 using the Adam Optimizer\n",
        "\n",
        "We choose BCEWithLogits in order to improve the stability of teh network compared to when we use just BCE, since it makes use of the log sum exp trick thus avoiding underflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlxZADgYuMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# We shouls change the momentum as the network trains, right now it's to low\n",
        "#optimizer = optim.SGD(siamese.parameters(), lr = 0.1, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "# optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=lr_decay_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8kcukdmc5b",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfozeCIjo1iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_data_loader, validate_data_loader,\n",
        "          model_save_path, checkpoint_path,\n",
        "          validate_every = 10 , save_every = 50):\n",
        "  \"\"\" Train the network with two parameters, one is how often should we validate\n",
        "  and the other is how often should we save a checkpoint\"\"\"\n",
        "\n",
        "  best_accuracy = 0\n",
        "\n",
        "  # define the loader of the dataset\n",
        "\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    \n",
        "    for X1, X2, y in train_data_loader:\n",
        "      # set network to learning mode\n",
        "      model.train()\n",
        "\n",
        "      # send to gpu\n",
        "      X1 = X1.to(device)\n",
        "      X2 = X2.to(device)\n",
        "      y = y.to(device)\n",
        "      \n",
        "      # make gradients zero before forward prop\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # reshape inputs\n",
        "      X1 = X1.view(-1, 1, 105, 105)\n",
        "      X2 = X2.view(-1, 1, 105, 105)\n",
        "      y = y.view(-1, 1)\n",
        "\n",
        "      # forward prop\n",
        "      outputs = model(X1, X2)\n",
        "\n",
        "      # compute loss\n",
        "      loss = criterion(outputs, y)\n",
        "\n",
        "      # backprop and gradient descent step\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      if i % 50 == 0:\n",
        "        print('[%d, %5d] loss: %.3f lr: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / (i+1), optimizer.param_groups[0]['lr']))\n",
        "      i+=1\n",
        "\n",
        "    # Update the learning rate\n",
        "    # optim_scheduler.step()\n",
        "\n",
        "    # every `validate_every` epochs,\n",
        "    # get metrics from validation set\n",
        "    if epoch % validate_every == 0:\n",
        "      accuracy = validate(model, validate_data_loader)\n",
        "\n",
        "      # if accuracy is higest till now,\n",
        "      # save model\n",
        "      if accuracy > best_accuracy:\n",
        "        print(\"Saving best model\")\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    # save model every `save_every` epochs\n",
        "    if epoch > 0 and epoch % save_every == 0:\n",
        "      torch.save(model.state_dict(), checkpoint_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkbTiqs7MlZ2",
        "colab_type": "text"
      },
      "source": [
        "## Validation Loop\n",
        "\n",
        "In this validation loop we loop through the validation set and calculate the accuracy of the model.\n",
        "\n",
        "We can do this as often as it is said in the training loop. For a thorough evaluation we can use validate_every= 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABqSLMQRCUys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, data_loader):\n",
        "  \"\"\" Validates the model and computes the accuracy\"\"\"\n",
        "  \n",
        "  # set network to validation mode\n",
        "  model.eval()\n",
        "  print(\"Validating model!\")\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for x1, x2, y in data_loader:\n",
        "\n",
        "      # Send data to device\n",
        "      x1 = x1.to(device)\n",
        "      x2= x2.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # Appropriate view\n",
        "      x1 = x1.view(-1, 1, 105, 105)\n",
        "      x2 = x2.view(-1, 1, 105, 105)\n",
        "      y = y.view(-1,1)\n",
        "\n",
        "      # forward prop\n",
        "      outputs = model(x1, x2)\n",
        "      # Translate the outputs to 0 or 1\n",
        "      predicted = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "      total += y.size(0)\n",
        "      correct += (predicted == y).sum().item()\n",
        "    \n",
        "    # return the accuracy\n",
        "    print(\"Accuracy of the network on the val set %.3f %%\" % (100*correct /total))\n",
        "    return 100*correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4oWq6bmz5Z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XANF2TNnfJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_path = os.path.join(\"saved_models\", \"best.th\")\n",
        "checkpoint_path = os.path.join(\"saved_models\", \"checkpoint.th\")\n",
        "\n",
        "train_loader = data.DataLoader(train_d, batch_size=128, shuffle=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(valid_d, shuffle=True, batch_size=128, pin_memory=True, num_workers=4)\n",
        "\n",
        "train(siamese, train_loader, val_loader, model_save_path, checkpoint_path, validate_every=1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sebOdYR32Pg",
        "colab_type": "text"
      },
      "source": [
        "# Validating on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf68u8z0q8XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load best model for testing\n",
        "siamese.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "# create data loader for test set\n",
        "test_loader = data.DataLoader(test_d, shuffle=True, batch_size=128, pin_memory=True, num_workers=4)\n",
        "validate(siamese, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}