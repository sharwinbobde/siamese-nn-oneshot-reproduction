{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cT7lOrijlJs",
        "colab_type": "text"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Twhmbb8kXNQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M1FkjdQluR8",
        "colab_type": "text"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qac1GqFnl58c",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mcpj2P3l8So",
        "colab_type": "text"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uAN_OrVe3HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoqpb1oxoEpU",
        "colab_type": "text"
      },
      "source": [
        "## Definition of the dataset class that will hold our examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtx4GZl_oJy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseDataset(data.Dataset):\n",
        "    \"\"\"Dataset that reads the data from an npy file and \n",
        "    returns a pair to the loader\"\"\"\n",
        "    def __init__(self, data_path=None, labels_path=None, \n",
        "                 transform=None, dataset: data.Dataset =None, \n",
        "                 data : np.ndarray = None, labels: np.ndarray = None,\n",
        "                 mean : float = None, std : float = None,\n",
        "                 transform_data=False):\n",
        "        self.transform_data = transform_data\n",
        "\n",
        "        # If we're given another dataset, just take that\n",
        "        if dataset is not None:\n",
        "            self.data = dataset.data\n",
        "            self.labels = dataset.labels\n",
        "            self.transforms = dataset.transforms\n",
        "\n",
        "        # We can also pass the data and labels as an array\n",
        "        elif data is not None:\n",
        "            self.data = data\n",
        "            self.labels = labels\n",
        "            self.transforms = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "\n",
        "        # If not, that means that we load it from a file\n",
        "        else:\n",
        "            # Load the data and labels\n",
        "            self.data = np.load(data_path)\n",
        "            self.labels = np.load(labels_path)\n",
        "\n",
        "            # for training set, calculate mean and std\n",
        "            # to normalize\n",
        "            if mean == None and std == None:\n",
        "                # stats of the dataset\n",
        "                self.mean = np.mean(self.data[:,:,:])\n",
        "                self.std = np.std(self.data[:,:,:])\n",
        "            # for test set, use mean and std from\n",
        "            # the train set to normalize\n",
        "            else:\n",
        "                self.mean = mean\n",
        "                self.std = std\n",
        "            # Normalize by default!\n",
        "            self.normalize = transforms.Normalize(mean=(self.mean,),\n",
        "                                                std = (self.std,))\n",
        "            # We apply the transformations that are given, so we can \n",
        "            # join the datasets\n",
        "\n",
        "            if transform is not None:\n",
        "              # If we're given transforms it means\n",
        "              # that we're trying to apply the affine transformations\n",
        "              self.transforms = transforms.Compose([\n",
        "                  transform, \n",
        "                  transforms.ToTensor(),\n",
        "              ])\n",
        "            else:\n",
        "              # If we're not given transforms just return the\n",
        "              # normalized tensor\n",
        "              print(\"Using the default transformations\")\n",
        "              self.transforms = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    self.normalize                                \n",
        "              ])\n",
        "              \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def get_images(self, index):\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        return Image.fromarray(_x1), Image.fromarray(_x2), label\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Gets the next pair from \n",
        "        the dataset and its corresponding label\n",
        "        (0 or 1 depending on if they're the same\n",
        "        or a different letter)\"\"\"\n",
        "        _x1 = self.data[index,0,:,:]\n",
        "        _x2 = self.data[index,1,:,:]\n",
        "        label = self.labels[index]\n",
        "        \n",
        "        # Convert to PIL Images so \n",
        "        # we can transform them with affine transforms\n",
        "        # Just needed to generate the dataset\n",
        "        if self.transform_data:\n",
        "            _x1 = Image.fromarray(_x1)\n",
        "            _x2 = Image.fromarray(_x2)\n",
        "            \n",
        "            # we need to convert the x's to images to apply the transforms\n",
        "            return self.transforms(_x1), self.transforms(_x2), label\n",
        "        else:\n",
        "          # We're trying to train the dataset, so give\n",
        "          # the data in float32 version that's better for training\n",
        "          # and apply the ToTensor and normalization transformations\n",
        "            _x1 = _x1.astype(np.float32)\n",
        "            _x2 = _x2.astype(np.float32)\n",
        "            label = label.astype(np.float32)\n",
        "            return self.transforms(_x1), self.transforms(_x2), label\n",
        "    \n",
        "# Some easy functions to visualize the data \n",
        "def show_pair(x1, x2, lab):\n",
        "    \"\"\"Function to show two images of the dataset side by side\"\"\"\n",
        "    # x1 = x1.numpy()\n",
        "    # x2 = x2.numpy()\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(x1.squeeze())\n",
        "    ax2.imshow(x2.squeeze())\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    \n",
        "def show_image_pair(i1, i2, lab):\n",
        "    f ,(ax1, ax2) = plt.subplots(1, 2, sharey= True)\n",
        "    ax1.imshow(i1)\n",
        "    ax2.imshow(i2)\n",
        "    plt.show()\n",
        "    print('same' if lab == 1 else 'different')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSgUcaaokfH",
        "colab_type": "text"
      },
      "source": [
        "### Set up the folder in Google Drive and define the data path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6auieDfogWa",
        "colab_type": "code",
        "outputId": "17b8d3c0-39ca-44b1-c048-8ba1d55a04cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!ls \"/content/drive/My Drive/Siamese/\"\n",
        "\n",
        "# Change the current directory to the path so it's more comfortable to work\n",
        "path = \"/content/drive/My Drive/Siamese/\"\n",
        "os.chdir(path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "trainX_30k.npy\ttrainY_30k.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bug6aTZxosI4",
        "colab_type": "code",
        "outputId": "8ed83f0f-0f47-43b5-882e-1c00a4adfaa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# define the paths of the data from the work dir\n",
        "data_path = \"trainX_30k.npy\"\n",
        "labels_path = \"trainY_30k.npy\"\n",
        "\n",
        "# Affine transformations to be done on the data\n",
        "affine = transforms.RandomAffine(degrees = (-10,10), \n",
        "                                 translate=(0.2,0.2),\n",
        "                                 scale = (0.8, 1.2),\n",
        "                                 shear = (-0.3, 0.3), \n",
        "                                 fillcolor=255)\n",
        "\n",
        "\n",
        "# Create a dataset with the 30K examples without\n",
        "# affine tranbsformations \n",
        "d = SiameseDataset(data_path, labels_path)\n",
        "\n",
        "# In order to augment the dataset the dataset has to be created with\n",
        "# some extra parameters and call the function defined below\n",
        "# d = SiameseDataset(data_path, labels_path, transform_data=True, transform=affine)\n",
        "# print(\"Loaded data\")\n",
        "# d = augment_dataset(d)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default transformations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4ioF-p-sxTg",
        "colab_type": "text"
      },
      "source": [
        "## In case we want to augment the dataset we can run the function defined below\n",
        "\n",
        "For each sample the fucntion generates 8x affine transformed samples of that pair and returns a new dataset with the original images and the augmented samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGt6Iyi7rSLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In case we want to create affine transformations...\n",
        "import gc\n",
        "\n",
        "\n",
        "def augment_dataset(d: SiameseDataset) -> SiameseDataset:\n",
        "  \"\"\" Augments the dataset and returns a siamese dataset\n",
        "  with 9x as much data, the original data in the argument dataset\n",
        "  plus 8 affine transformations of that input data\"\"\"\n",
        "  # Create a data loader of the dataset\n",
        "  loader = data.DataLoader(d, batch_size=15000)\n",
        "\n",
        "  # Altered samples of the input data\n",
        "  _altered = None\n",
        "\n",
        "  # Check the size of the batches and so on\n",
        "  # Read in batches of 15000, and do it \n",
        "  for j in range(8):\n",
        "      gc.collect()\n",
        "      print(\"starting with round \",j)\n",
        "      for i, (x1, x2, _) in enumerate(loader):\n",
        "          if i % 1 == 0:\n",
        "              print(i)\n",
        "          # concatenate the arrays by their second axis\n",
        "          _data = np.concatenate((x1.numpy().astype(np.uint8), x2.numpy().astype(np.uint8)), axis = 1)\n",
        "          # add them to the dataset\n",
        "          if _altered is None:\n",
        "              _altered = _data\n",
        "          else:\n",
        "              # Concatenate the existing data and the new batch\n",
        "              _altered = np.concatenate((_altered, _data), axis = 0)\n",
        "      \n",
        "      print(f'Size of the datasets -> {_altered.shape}')\n",
        "\n",
        "  # Now create a new dataset with the newly defined data\n",
        "  # Concatenate the original dataset with the new one\n",
        "  all_data = np.concatenate((d.data, _altered), axis = 0)\n",
        "  labels = np.tile(d.labels, 9)\n",
        "  d = SiameseDataset(data = all_data, labels = labels)\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlU4gaHlY5t",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsEL_whslWv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = F.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = F.sigmoid(self.prediction_layer(h))\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj8AwypRYxFe",
        "colab_type": "text"
      },
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPtfP3AUfhj",
        "colab_type": "code",
        "outputId": "63b8ef94-63b5-4e9b-9dcc-aa1af8430da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNet(\n",
              "  (convnet): ConvNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
              "    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  )\n",
              "  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_10kSyZO0p",
        "colab_type": "text"
      },
      "source": [
        "### Define the Loss (CrossEntropy) and the Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlxZADgYuMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "# Learning rate decay per epoch\n",
        "lr_decay_rate = 0.99\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# We shouls change the momentum as the network trains, right now it's to low\n",
        "optimizer = optim.SGD(siamese.parameters(), lr = 0.1, momentum=0.7, weight_decay=2e-4)\n",
        "#optimizer = optim.Adam(siamese.parameters(), lr = 0.01, weight_decay = 2e-4)\n",
        "optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = lr_decay_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtINTkF9mZUC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8kcukdmc5b",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfozeCIjo1iZ",
        "colab_type": "code",
        "outputId": "8ce11dde-fecb-4a50-9750-d31679edd383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Send the network to the GPU if available\n",
        "#device = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "siamese.to(device)\n",
        "\n",
        "# define the loader of the dataset\n",
        "train_loader = data.DataLoader(d, batch_size=128, shuffle=True)\n",
        "\n",
        "for epoch in range(100):\n",
        "  running_loss = 0.0\n",
        "  i = 0\n",
        "  \n",
        "  for X1, X2, y in train_loader:\n",
        "    X1 = X1.to(device)\n",
        "    X2 = X2.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    X1 = X1.view(-1, 1, 105, 105)\n",
        "    X2 = X2.view(-1, 1, 105, 105)\n",
        "    y = y.view(-1,1)\n",
        "\n",
        "    \n",
        "\n",
        "    outputs = siamese(X1, X2)\n",
        "    #print(outputs.max(), outputs.min())\n",
        "    \n",
        "    loss = criterion(outputs , y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 20 == 0:\n",
        "      print('[%d, %5d] loss: %.3f lr: %.5f' %\n",
        "                (epoch + 1, i + 1, running_loss / (i+1), optimizer.param_groups[0]['lr']))\n",
        "    i+=1\n",
        "  # Update the learning rate\n",
        "  optim_scheduler.step()\n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,     1] loss: 0.708 lr: 0.09801\n",
            "[1,    21] loss: 0.721 lr: 0.09801\n",
            "[1,    41] loss: 0.742 lr: 0.09801\n",
            "[1,    61] loss: 0.773 lr: 0.09801\n",
            "[1,    81] loss: 0.788 lr: 0.09801\n",
            "[1,   101] loss: 0.767 lr: 0.09801\n",
            "[1,   121] loss: 0.747 lr: 0.09801\n",
            "[1,   141] loss: 0.737 lr: 0.09801\n",
            "[1,   161] loss: 0.730 lr: 0.09801\n",
            "[1,   181] loss: 0.720 lr: 0.09801\n",
            "[1,   201] loss: 0.719 lr: 0.09801\n",
            "[1,   221] loss: 0.729 lr: 0.09801\n",
            "[2,     1] loss: 0.510 lr: 0.09703\n",
            "[2,    21] loss: 0.716 lr: 0.09703\n",
            "[2,    41] loss: 0.665 lr: 0.09703\n",
            "[2,    61] loss: 0.636 lr: 0.09703\n",
            "[2,    81] loss: 0.667 lr: 0.09703\n",
            "[2,   101] loss: 0.679 lr: 0.09703\n",
            "[2,   121] loss: 0.671 lr: 0.09703\n",
            "[2,   141] loss: 0.657 lr: 0.09703\n",
            "[2,   161] loss: 0.645 lr: 0.09703\n",
            "[2,   181] loss: 0.644 lr: 0.09703\n",
            "[2,   201] loss: 0.648 lr: 0.09703\n",
            "[2,   221] loss: 0.644 lr: 0.09703\n",
            "[3,     1] loss: 0.493 lr: 0.09606\n",
            "[3,    21] loss: 0.538 lr: 0.09606\n",
            "[3,    41] loss: 0.597 lr: 0.09606\n",
            "[3,    61] loss: 0.617 lr: 0.09606\n",
            "[3,    81] loss: 0.611 lr: 0.09606\n",
            "[3,   101] loss: 0.600 lr: 0.09606\n",
            "[3,   121] loss: 0.623 lr: 0.09606\n",
            "[3,   141] loss: 0.634 lr: 0.09606\n",
            "[3,   161] loss: 0.621 lr: 0.09606\n",
            "[3,   181] loss: 0.615 lr: 0.09606\n",
            "[3,   201] loss: 0.616 lr: 0.09606\n",
            "[3,   221] loss: 0.630 lr: 0.09606\n",
            "[4,     1] loss: 0.474 lr: 0.09510\n",
            "[4,    21] loss: 0.552 lr: 0.09510\n",
            "[4,    41] loss: 0.534 lr: 0.09510\n",
            "[4,    61] loss: 0.569 lr: 0.09510\n",
            "[4,    81] loss: 0.597 lr: 0.09510\n",
            "[4,   101] loss: 0.637 lr: 0.09510\n",
            "[4,   121] loss: 0.641 lr: 0.09510\n",
            "[4,   141] loss: 0.633 lr: 0.09510\n",
            "[4,   161] loss: 0.651 lr: 0.09510\n",
            "[4,   181] loss: 0.652 lr: 0.09510\n",
            "[4,   201] loss: 0.642 lr: 0.09510\n",
            "[4,   221] loss: 0.641 lr: 0.09510\n",
            "[5,     1] loss: 0.563 lr: 0.09415\n",
            "[5,    21] loss: 0.573 lr: 0.09415\n",
            "[5,    41] loss: 0.673 lr: 0.09415\n",
            "[5,    61] loss: 0.620 lr: 0.09415\n",
            "[5,    81] loss: 0.603 lr: 0.09415\n",
            "[5,   101] loss: 0.597 lr: 0.09415\n",
            "[5,   121] loss: 0.616 lr: 0.09415\n",
            "[5,   141] loss: 0.624 lr: 0.09415\n",
            "[5,   161] loss: 0.617 lr: 0.09415\n",
            "[5,   181] loss: 0.613 lr: 0.09415\n",
            "[5,   201] loss: 0.620 lr: 0.09415\n",
            "[5,   221] loss: 0.633 lr: 0.09415\n",
            "[6,     1] loss: 0.447 lr: 0.09321\n",
            "[6,    21] loss: 0.560 lr: 0.09321\n",
            "[6,    41] loss: 0.516 lr: 0.09321\n",
            "[6,    61] loss: 0.536 lr: 0.09321\n",
            "[6,    81] loss: 0.564 lr: 0.09321\n",
            "[6,   101] loss: 0.574 lr: 0.09321\n",
            "[6,   121] loss: 0.572 lr: 0.09321\n",
            "[6,   141] loss: 0.585 lr: 0.09321\n",
            "[6,   161] loss: 0.579 lr: 0.09321\n",
            "[6,   181] loss: 0.578 lr: 0.09321\n",
            "[6,   201] loss: 0.581 lr: 0.09321\n",
            "[6,   221] loss: 0.582 lr: 0.09321\n",
            "[7,     1] loss: 0.445 lr: 0.09227\n",
            "[7,    21] loss: 0.565 lr: 0.09227\n",
            "[7,    41] loss: 0.530 lr: 0.09227\n",
            "[7,    61] loss: 0.582 lr: 0.09227\n",
            "[7,    81] loss: 0.571 lr: 0.09227\n",
            "[7,   101] loss: 0.569 lr: 0.09227\n",
            "[7,   121] loss: 0.565 lr: 0.09227\n",
            "[7,   141] loss: 0.575 lr: 0.09227\n",
            "[7,   161] loss: 0.575 lr: 0.09227\n",
            "[7,   181] loss: 0.572 lr: 0.09227\n",
            "[7,   201] loss: 0.584 lr: 0.09227\n",
            "[7,   221] loss: 0.590 lr: 0.09227\n",
            "[8,     1] loss: 1.020 lr: 0.09135\n",
            "[8,    21] loss: 0.533 lr: 0.09135\n",
            "[8,    41] loss: 0.514 lr: 0.09135\n",
            "[8,    61] loss: 0.529 lr: 0.09135\n",
            "[8,    81] loss: 0.532 lr: 0.09135\n",
            "[8,   101] loss: 0.539 lr: 0.09135\n",
            "[8,   121] loss: 0.540 lr: 0.09135\n",
            "[8,   141] loss: 0.564 lr: 0.09135\n",
            "[8,   161] loss: 0.560 lr: 0.09135\n",
            "[8,   181] loss: 0.565 lr: 0.09135\n",
            "[8,   201] loss: 0.569 lr: 0.09135\n",
            "[8,   221] loss: 0.583 lr: 0.09135\n",
            "[9,     1] loss: 0.463 lr: 0.09044\n",
            "[9,    21] loss: 0.539 lr: 0.09044\n",
            "[9,    41] loss: 0.534 lr: 0.09044\n",
            "[9,    61] loss: 0.563 lr: 0.09044\n",
            "[9,    81] loss: 0.563 lr: 0.09044\n",
            "[9,   101] loss: 0.587 lr: 0.09044\n",
            "[9,   121] loss: 0.599 lr: 0.09044\n",
            "[9,   141] loss: 0.597 lr: 0.09044\n",
            "[9,   161] loss: 0.592 lr: 0.09044\n",
            "[9,   181] loss: 0.586 lr: 0.09044\n",
            "[9,   201] loss: 0.583 lr: 0.09044\n",
            "[9,   221] loss: 0.583 lr: 0.09044\n",
            "[10,     1] loss: 0.736 lr: 0.08953\n",
            "[10,    21] loss: 0.522 lr: 0.08953\n",
            "[10,    41] loss: 0.594 lr: 0.08953\n",
            "[10,    61] loss: 0.607 lr: 0.08953\n",
            "[10,    81] loss: 0.581 lr: 0.08953\n",
            "[10,   101] loss: 0.573 lr: 0.08953\n",
            "[10,   121] loss: 0.572 lr: 0.08953\n",
            "[10,   141] loss: 0.577 lr: 0.08953\n",
            "[10,   161] loss: 0.573 lr: 0.08953\n",
            "[10,   181] loss: 0.573 lr: 0.08953\n",
            "[10,   201] loss: 0.568 lr: 0.08953\n",
            "[10,   221] loss: 0.566 lr: 0.08953\n",
            "[11,     1] loss: 0.648 lr: 0.08864\n",
            "[11,    21] loss: 0.508 lr: 0.08864\n",
            "[11,    41] loss: 0.510 lr: 0.08864\n",
            "[11,    61] loss: 0.526 lr: 0.08864\n",
            "[11,    81] loss: 0.533 lr: 0.08864\n",
            "[11,   101] loss: 0.529 lr: 0.08864\n",
            "[11,   121] loss: 0.552 lr: 0.08864\n",
            "[11,   141] loss: 0.544 lr: 0.08864\n",
            "[11,   161] loss: 0.544 lr: 0.08864\n",
            "[11,   181] loss: 0.550 lr: 0.08864\n",
            "[11,   201] loss: 0.549 lr: 0.08864\n",
            "[11,   221] loss: 0.549 lr: 0.08864\n",
            "[12,     1] loss: 0.604 lr: 0.08775\n",
            "[12,    21] loss: 0.491 lr: 0.08775\n",
            "[12,    41] loss: 0.497 lr: 0.08775\n",
            "[12,    61] loss: 0.509 lr: 0.08775\n",
            "[12,    81] loss: 0.507 lr: 0.08775\n",
            "[12,   101] loss: 0.517 lr: 0.08775\n",
            "[12,   121] loss: 0.530 lr: 0.08775\n",
            "[12,   141] loss: 0.547 lr: 0.08775\n",
            "[12,   161] loss: 0.545 lr: 0.08775\n",
            "[12,   181] loss: 0.542 lr: 0.08775\n",
            "[12,   201] loss: 0.544 lr: 0.08775\n",
            "[12,   221] loss: 0.545 lr: 0.08775\n",
            "[13,     1] loss: 0.495 lr: 0.08687\n",
            "[13,    21] loss: 0.689 lr: 0.08687\n",
            "[13,    41] loss: 0.631 lr: 0.08687\n",
            "[13,    61] loss: 0.591 lr: 0.08687\n",
            "[13,    81] loss: 0.577 lr: 0.08687\n",
            "[13,   101] loss: 0.583 lr: 0.08687\n",
            "[13,   121] loss: 0.587 lr: 0.08687\n",
            "[13,   141] loss: 0.581 lr: 0.08687\n",
            "[13,   161] loss: 0.582 lr: 0.08687\n",
            "[13,   181] loss: 0.578 lr: 0.08687\n",
            "[13,   201] loss: 0.577 lr: 0.08687\n",
            "[13,   221] loss: 0.576 lr: 0.08687\n",
            "[14,     1] loss: 0.364 lr: 0.08601\n",
            "[14,    21] loss: 0.494 lr: 0.08601\n",
            "[14,    41] loss: 0.489 lr: 0.08601\n",
            "[14,    61] loss: 0.488 lr: 0.08601\n",
            "[14,    81] loss: 0.486 lr: 0.08601\n",
            "[14,   101] loss: 0.501 lr: 0.08601\n",
            "[14,   121] loss: 0.505 lr: 0.08601\n",
            "[14,   141] loss: 0.516 lr: 0.08601\n",
            "[14,   161] loss: 0.525 lr: 0.08601\n",
            "[14,   181] loss: 0.536 lr: 0.08601\n",
            "[14,   201] loss: 0.542 lr: 0.08601\n",
            "[14,   221] loss: 0.545 lr: 0.08601\n",
            "[15,     1] loss: 0.479 lr: 0.08515\n",
            "[15,    21] loss: 0.496 lr: 0.08515\n",
            "[15,    41] loss: 0.501 lr: 0.08515\n",
            "[15,    61] loss: 0.522 lr: 0.08515\n",
            "[15,    81] loss: 0.521 lr: 0.08515\n",
            "[15,   101] loss: 0.535 lr: 0.08515\n",
            "[15,   121] loss: 0.537 lr: 0.08515\n",
            "[15,   141] loss: 0.531 lr: 0.08515\n",
            "[15,   161] loss: 0.528 lr: 0.08515\n",
            "[15,   181] loss: 0.527 lr: 0.08515\n",
            "[15,   201] loss: 0.528 lr: 0.08515\n",
            "[15,   221] loss: 0.533 lr: 0.08515\n",
            "[16,     1] loss: 1.143 lr: 0.08429\n",
            "[16,    21] loss: 0.529 lr: 0.08429\n",
            "[16,    41] loss: 0.537 lr: 0.08429\n",
            "[16,    61] loss: 0.521 lr: 0.08429\n",
            "[16,    81] loss: 0.510 lr: 0.08429\n",
            "[16,   101] loss: 0.515 lr: 0.08429\n",
            "[16,   121] loss: 0.519 lr: 0.08429\n",
            "[16,   141] loss: 0.520 lr: 0.08429\n",
            "[16,   161] loss: 0.523 lr: 0.08429\n",
            "[16,   181] loss: 0.529 lr: 0.08429\n",
            "[16,   201] loss: 0.536 lr: 0.08429\n",
            "[16,   221] loss: 0.536 lr: 0.08429\n",
            "[17,     1] loss: 0.552 lr: 0.08345\n",
            "[17,    21] loss: 0.486 lr: 0.08345\n",
            "[17,    41] loss: 0.497 lr: 0.08345\n",
            "[17,    61] loss: 0.495 lr: 0.08345\n",
            "[17,    81] loss: 0.493 lr: 0.08345\n",
            "[17,   101] loss: 0.523 lr: 0.08345\n",
            "[17,   121] loss: 0.521 lr: 0.08345\n",
            "[17,   141] loss: 0.523 lr: 0.08345\n",
            "[17,   161] loss: 0.524 lr: 0.08345\n",
            "[17,   181] loss: 0.535 lr: 0.08345\n",
            "[17,   201] loss: 0.531 lr: 0.08345\n",
            "[17,   221] loss: 0.529 lr: 0.08345\n",
            "[18,     1] loss: 0.614 lr: 0.08262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4oWq6bmz5Z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjP2BRqhCSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gDe5nYvhFQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}