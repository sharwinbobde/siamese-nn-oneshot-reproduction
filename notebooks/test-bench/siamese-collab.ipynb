{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cT7lOrijlJs",
        "colab_type": "text"
      },
      "source": [
        "# Reproducing Omniglot experiment in the Siamese NNs for One Shot Recognition Paper\n",
        "\n",
        "In this notebook we reproduce Table 1 in the original \n",
        "[Siamese NN Paper](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "\n",
        "[Original MSc Thesis](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf).\n",
        "\n",
        "We start from this [code](https://github.com/sorenbouma/keras-oneshot) implemented in Keras and try to translate it to use the PyTorch library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Twhmbb8kXNQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "--------------------------------\n",
        "# How/Why Siamese Networks Work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M1FkjdQluR8",
        "colab_type": "text"
      },
      "source": [
        "# One-Shot Image Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qac1GqFnl58c",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mcpj2P3l8So",
        "colab_type": "text"
      },
      "source": [
        "# Running the experiment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uAN_OrVe3HD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlU4gaHlY5t",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------\n",
        "## Definition of the netwok architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsEL_whslWv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\" Convolutional NN used in pair inside the siamese Network \"\"\"\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 64, 10)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "    self.conv3 = nn.Conv2d(128,128,4)\n",
        "    self.conv4 = nn.Conv2d(128,256, 4)\n",
        "    self.fc1 = nn.Linear(256*6*6, 4096)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    out = self.pool(F.relu(self.conv3(out)))\n",
        "    out = F.relu(self.conv4(out))\n",
        "    out = out.view(-1, 256*6*6)\n",
        "    # We get the h feature vectors\n",
        "    out = F.sigmoid(self.fc1(out))\n",
        "    return out\n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "  \"\"\"Siamese Net combining two ConvNets\"\"\"\n",
        "  def __init__(self, net):\n",
        "    # Receives a net as a parameter, we can just have 1 net \n",
        "    # but do the forward pass twice! and then just update once, much more \n",
        "    # elegant\n",
        "    super(SiameseNet, self).__init__()\n",
        "    # Instantiate two of the same class\n",
        "    self.convnet = net\n",
        "    # Final layer and output\n",
        "    self.prediction_layer = nn.Linear(4096,1)\n",
        "\n",
        "  def forward(self,x1, x2):\n",
        "    \"\"\"Computes the forward given two images\"\"\"\n",
        "    h1 = self.convnet(x1)\n",
        "    h2 = self.convnet(x2)\n",
        "    h = self.calculate_l1_distance(h1, h2)\n",
        "    out = F.sigmoid(self.prediction_layer(h))\n",
        "    return out\n",
        "  \n",
        "  def calculate_l1_distance(self, h1, h2):\n",
        "    \"\"\"Calculates l1 distance between the two given vectors\"\"\"\n",
        "    return torch.abs(h1-h2)\n",
        "\n",
        "\n",
        "# How to initialize the weights according to the paper\n",
        "def weights_init(model):\n",
        "  if isinstance(model, nn.Conv2d):\n",
        "    nn.init.normal_(model.weight, mean = 0.0, std = 1e-2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "  elif isinstance(model, nn.Linear):\n",
        "    nn.init.normal_(model.weight, mean= 0.0, std = 0.2)\n",
        "    nn.init.normal_(model.bias, mean=0.5, std = 1e-2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj8AwypRYxFe",
        "colab_type": "text"
      },
      "source": [
        "### Create the Siamese Network and Initialize weights according to specifications\n",
        "- Conv layers: \n",
        "  - Weights: Normal(0, 1e-2)\n",
        "  - Bias: Normal(0.5, 1e-2)\n",
        "- Linear layers: \n",
        "  - Weights: Normal(0, 0.2)\n",
        "  - Bias: Normal(0.5, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPtfP3AUfhj",
        "colab_type": "code",
        "outputId": "1ef9986e-dd54-4da0-cdc5-543e55171593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "conv = ConvNet()\n",
        "siamese = SiameseNet(conv)\n",
        "siamese.apply(weights_init)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNet(\n",
              "  (convnet): ConvNet(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(10, 10), stride=(1, 1))\n",
              "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
              "    (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
              "    (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  )\n",
              "  (prediction_layer): Linear(in_features=4096, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_10kSyZO0p",
        "colab_type": "text"
      },
      "source": [
        "### Define the Loss (CrossEntropy) and the Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlxZADgYuMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "# Learning rate decay per epoch\n",
        "lr_decay_rate = 0.99\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.SGD(siamese.parameters(), lr = 0.01, momentum=0.5, weight_decay=2e-4)\n",
        "optim_scheduler = optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma = lr_decay_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtINTkF9mZUC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Hyperparameter Setting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GgaTl2Tmvg0",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z1QFPVDDogA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD__n5UxDScv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, data_path):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_path: str\n",
        "      Path to the pickle file\n",
        "    \"\"\"\n",
        "    self.data = None\n",
        "    self.alphabet_index = None\n",
        "    with open(data_path, \"rb\") as f:\n",
        "      X, i = pickle.load(f)\n",
        "      self.data = X.astype(\"float32\")\n",
        "      self.alphabet_index = i\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    index: int\n",
        "      index from which to get the data\n",
        "    \"\"\"\n",
        "    # get dimensions of the data\n",
        "    num_letters, num_drawings, height, width = self.data.shape\n",
        "\n",
        "    # initialize index2\n",
        "    # index of second letter from pair same as first\n",
        "    index2 = index\n",
        "\n",
        "    # randomly choose a variant of the letter\n",
        "    drawing_index = np.random.choice(num_drawings)\n",
        "    drawing_index2 = np.random.choice(num_drawings)\n",
        "\n",
        "    # choose image for first letter\n",
        "    X1 = self.data[index, drawing_index, :, :].reshape(width, height)\n",
        "    \n",
        "    # set label to be 1, i.e. same letter\n",
        "    y = np.array([1.0], dtype=\"float32\")\n",
        "\n",
        "    # with 50% probability,\n",
        "    # pick an image of a different letter\n",
        "    # and change the label to 0, i.e. different letter\n",
        "    if np.random.uniform() >= 0.5:\n",
        "      index2 = (index + np.random.randint(1, num_letters)) % num_letters\n",
        "      y = np.array([0.0], dtype=\"float32\")\n",
        "    \n",
        "    # choose image for the second letter\n",
        "    X2 = self.data[index2, drawing_index2, :, :].reshape(width, height)\n",
        "\n",
        "    return X1, X2, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJexVDSkPUQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "data_path = os.path.abspath(\"./data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofG33U42O8pR",
        "colab_type": "code",
        "outputId": "a7c7bd96-b783-4744-9b25-ac77ca3ef849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# get the processed data (after running create_data.py) from shared drive\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# choose a local (colab) directory to store the data.\n",
        "data_path = os.path.abspath(\"./data\")\n",
        "try:\n",
        "  os.makedirs(data_path)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# 2. Auto-iterate using the query syntax\n",
        "#    https://developers.google.com/drive/v2/web/search-parameters\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1QSf1sWhroRNa9yn335y15zynAoUmknA2' in parents\"}).GetList()\n",
        "\n",
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  fname = os.path.join(data_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading to /content/data/eval.pkl\n",
            "downloading to /content/data/train.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjDt6yUaqhrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the dataset object\n",
        "train_set = Dataset(os.path.join(data_path, \"train.pkl\"))\n",
        "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
        "\n",
        "device = torch.device('cuda')\n",
        "siamese.to(device, dtype=torch.float32)\n",
        "\n",
        "# set parameters for data creation\n",
        "batch_size = 128\n",
        "num_workers = 1\n",
        "\n",
        "params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': num_workers}\n",
        "\n",
        "# create the dataloader object which returns a generator over the data\n",
        "train_generator = data.DataLoader(train_set, **params)\n",
        "eval_generator = data.DataLoader(eval_set, **params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8kcukdmc5b",
        "colab_type": "text"
      },
      "source": [
        "---------------------------------\n",
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfozeCIjo1iZ",
        "colab_type": "code",
        "outputId": "198c9833-61ed-4aef-ac55-9e9ce124c01f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(200):\n",
        "  running_loss = 0.0\n",
        "  i = 0\n",
        "  \n",
        "  for X1, X2, y in train_generator:\n",
        "    X1 = X1.to(device)\n",
        "    X2 = X2.to(device)\n",
        "    y = y.to(device)\n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    X1 = X1.view(-1, 1, 105, 105)\n",
        "    X2 = X2.view(-1, 1, 105, 105)\n",
        "    \n",
        "\n",
        "    outputs = siamese(X1, X2)\n",
        "    # print(outputs, outputs.dtype)\n",
        "    # print(outputs.shape)\n",
        "    # print(y.shape)\n",
        "    # print(outputs)\n",
        "    # outputs_ = torch.cat((outputs.view(-1, 1), (1-outputs).view(-1, 1)), dim=1)\n",
        "    loss = criterion(outputs , y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    print('[%d, %5d] loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss / (i+1)))\n",
        "    i+=1\n",
        "  # Update the learning rate\n",
        "  optim_scheduler.step()\n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,     1] loss: 1.693\n",
            "[1,     2] loss: 1.226\n",
            "[1,     3] loss: 1.201\n",
            "[1,     4] loss: 1.086\n",
            "[1,     5] loss: 1.040\n",
            "[1,     6] loss: 0.990\n",
            "[1,     7] loss: 0.984\n",
            "[1,     8] loss: 0.990\n",
            "[2,     1] loss: 0.806\n",
            "[2,     2] loss: 0.946\n",
            "[2,     3] loss: 0.972\n",
            "[2,     4] loss: 1.011\n",
            "[2,     5] loss: 1.015\n",
            "[2,     6] loss: 1.015\n",
            "[2,     7] loss: 1.013\n",
            "[2,     8] loss: 1.002\n",
            "[3,     1] loss: 0.927\n",
            "[3,     2] loss: 0.881\n",
            "[3,     3] loss: 0.890\n",
            "[3,     4] loss: 0.854\n",
            "[3,     5] loss: 0.850\n",
            "[3,     6] loss: 0.868\n",
            "[3,     7] loss: 0.892\n",
            "[3,     8] loss: 0.877\n",
            "[4,     1] loss: 0.886\n",
            "[4,     2] loss: 0.831\n",
            "[4,     3] loss: 0.843\n",
            "[4,     4] loss: 0.831\n",
            "[4,     5] loss: 0.825\n",
            "[4,     6] loss: 0.828\n",
            "[4,     7] loss: 0.817\n",
            "[4,     8] loss: 0.810\n",
            "[5,     1] loss: 0.763\n",
            "[5,     2] loss: 0.782\n",
            "[5,     3] loss: 0.813\n",
            "[5,     4] loss: 0.816\n",
            "[5,     5] loss: 0.808\n",
            "[5,     6] loss: 0.810\n",
            "[5,     7] loss: 0.807\n",
            "[5,     8] loss: 0.813\n",
            "[6,     1] loss: 0.878\n",
            "[6,     2] loss: 0.883\n",
            "[6,     3] loss: 0.876\n",
            "[6,     4] loss: 0.848\n",
            "[6,     5] loss: 0.853\n",
            "[6,     6] loss: 0.852\n",
            "[6,     7] loss: 0.850\n",
            "[6,     8] loss: 0.854\n",
            "[7,     1] loss: 0.680\n",
            "[7,     2] loss: 0.746\n",
            "[7,     3] loss: 0.771\n",
            "[7,     4] loss: 0.776\n",
            "[7,     5] loss: 0.800\n",
            "[7,     6] loss: 0.800\n",
            "[7,     7] loss: 0.808\n",
            "[7,     8] loss: 0.805\n",
            "[8,     1] loss: 0.763\n",
            "[8,     2] loss: 0.794\n",
            "[8,     3] loss: 0.798\n",
            "[8,     4] loss: 0.790\n",
            "[8,     5] loss: 0.789\n",
            "[8,     6] loss: 0.795\n",
            "[8,     7] loss: 0.792\n",
            "[8,     8] loss: 0.788\n",
            "[9,     1] loss: 0.712\n",
            "[9,     2] loss: 0.766\n",
            "[9,     3] loss: 0.755\n",
            "[9,     4] loss: 0.744\n",
            "[9,     5] loss: 0.748\n",
            "[9,     6] loss: 0.758\n",
            "[9,     7] loss: 0.766\n",
            "[9,     8] loss: 0.772\n",
            "[10,     1] loss: 0.843\n",
            "[10,     2] loss: 0.822\n",
            "[10,     3] loss: 0.822\n",
            "[10,     4] loss: 0.846\n",
            "[10,     5] loss: 0.861\n",
            "[10,     6] loss: 0.858\n",
            "[10,     7] loss: 0.861\n",
            "[10,     8] loss: 0.853\n",
            "[11,     1] loss: 0.874\n",
            "[11,     2] loss: 0.874\n",
            "[11,     3] loss: 0.839\n",
            "[11,     4] loss: 0.806\n",
            "[11,     5] loss: 0.820\n",
            "[11,     6] loss: 0.817\n",
            "[11,     7] loss: 0.817\n",
            "[11,     8] loss: 0.823\n",
            "[12,     1] loss: 0.905\n",
            "[12,     2] loss: 0.864\n",
            "[12,     3] loss: 0.824\n",
            "[12,     4] loss: 0.794\n",
            "[12,     5] loss: 0.795\n",
            "[12,     6] loss: 0.778\n",
            "[12,     7] loss: 0.787\n",
            "[12,     8] loss: 0.777\n",
            "[13,     1] loss: 0.822\n",
            "[13,     2] loss: 0.745\n",
            "[13,     3] loss: 0.723\n",
            "[13,     4] loss: 0.711\n",
            "[13,     5] loss: 0.725\n",
            "[13,     6] loss: 0.738\n",
            "[13,     7] loss: 0.741\n",
            "[13,     8] loss: 0.735\n",
            "[14,     1] loss: 0.670\n",
            "[14,     2] loss: 0.706\n",
            "[14,     3] loss: 0.695\n",
            "[14,     4] loss: 0.708\n",
            "[14,     5] loss: 0.717\n",
            "[14,     6] loss: 0.728\n",
            "[14,     7] loss: 0.736\n",
            "[14,     8] loss: 0.729\n",
            "[15,     1] loss: 0.743\n",
            "[15,     2] loss: 0.727\n",
            "[15,     3] loss: 0.758\n",
            "[15,     4] loss: 0.758\n",
            "[15,     5] loss: 0.747\n",
            "[15,     6] loss: 0.760\n",
            "[15,     7] loss: 0.756\n",
            "[15,     8] loss: 0.741\n",
            "[16,     1] loss: 0.698\n",
            "[16,     2] loss: 0.722\n",
            "[16,     3] loss: 0.756\n",
            "[16,     4] loss: 0.740\n",
            "[16,     5] loss: 0.747\n",
            "[16,     6] loss: 0.748\n",
            "[16,     7] loss: 0.750\n",
            "[16,     8] loss: 0.762\n",
            "[17,     1] loss: 0.650\n",
            "[17,     2] loss: 0.674\n",
            "[17,     3] loss: 0.668\n",
            "[17,     4] loss: 0.695\n",
            "[17,     5] loss: 0.704\n",
            "[17,     6] loss: 0.708\n",
            "[17,     7] loss: 0.708\n",
            "[17,     8] loss: 0.701\n",
            "[18,     1] loss: 0.651\n",
            "[18,     2] loss: 0.690\n",
            "[18,     3] loss: 0.682\n",
            "[18,     4] loss: 0.709\n",
            "[18,     5] loss: 0.720\n",
            "[18,     6] loss: 0.727\n",
            "[18,     7] loss: 0.734\n",
            "[18,     8] loss: 0.729\n",
            "[19,     1] loss: 0.660\n",
            "[19,     2] loss: 0.718\n",
            "[19,     3] loss: 0.726\n",
            "[19,     4] loss: 0.713\n",
            "[19,     5] loss: 0.720\n",
            "[19,     6] loss: 0.724\n",
            "[19,     7] loss: 0.716\n",
            "[19,     8] loss: 0.714\n",
            "[20,     1] loss: 0.751\n",
            "[20,     2] loss: 0.762\n",
            "[20,     3] loss: 0.759\n",
            "[20,     4] loss: 0.738\n",
            "[20,     5] loss: 0.739\n",
            "[20,     6] loss: 0.744\n",
            "[20,     7] loss: 0.747\n",
            "[20,     8] loss: 0.743\n",
            "[21,     1] loss: 0.694\n",
            "[21,     2] loss: 0.703\n",
            "[21,     3] loss: 0.697\n",
            "[21,     4] loss: 0.685\n",
            "[21,     5] loss: 0.701\n",
            "[21,     6] loss: 0.697\n",
            "[21,     7] loss: 0.697\n",
            "[21,     8] loss: 0.700\n",
            "[22,     1] loss: 0.831\n",
            "[22,     2] loss: 0.743\n",
            "[22,     3] loss: 0.732\n",
            "[22,     4] loss: 0.720\n",
            "[22,     5] loss: 0.727\n",
            "[22,     6] loss: 0.728\n",
            "[22,     7] loss: 0.725\n",
            "[22,     8] loss: 0.738\n",
            "[23,     1] loss: 0.702\n",
            "[23,     2] loss: 0.680\n",
            "[23,     3] loss: 0.674\n",
            "[23,     4] loss: 0.682\n",
            "[23,     5] loss: 0.684\n",
            "[23,     6] loss: 0.677\n",
            "[23,     7] loss: 0.693\n",
            "[23,     8] loss: 0.697\n",
            "[24,     1] loss: 0.692\n",
            "[24,     2] loss: 0.677\n",
            "[24,     3] loss: 0.677\n",
            "[24,     4] loss: 0.669\n",
            "[24,     5] loss: 0.675\n",
            "[24,     6] loss: 0.679\n",
            "[24,     7] loss: 0.681\n",
            "[24,     8] loss: 0.705\n",
            "[25,     1] loss: 0.702\n",
            "[25,     2] loss: 0.743\n",
            "[25,     3] loss: 0.724\n",
            "[25,     4] loss: 0.702\n",
            "[25,     5] loss: 0.709\n",
            "[25,     6] loss: 0.725\n",
            "[25,     7] loss: 0.723\n",
            "[25,     8] loss: 0.729\n",
            "[26,     1] loss: 0.643\n",
            "[26,     2] loss: 0.645\n",
            "[26,     3] loss: 0.645\n",
            "[26,     4] loss: 0.670\n",
            "[26,     5] loss: 0.660\n",
            "[26,     6] loss: 0.663\n",
            "[26,     7] loss: 0.673\n",
            "[26,     8] loss: 0.675\n",
            "[27,     1] loss: 0.683\n",
            "[27,     2] loss: 0.766\n",
            "[27,     3] loss: 0.786\n",
            "[27,     4] loss: 0.771\n",
            "[27,     5] loss: 0.757\n",
            "[27,     6] loss: 0.756\n",
            "[27,     7] loss: 0.755\n",
            "[27,     8] loss: 0.750\n",
            "[28,     1] loss: 0.731\n",
            "[28,     2] loss: 0.748\n",
            "[28,     3] loss: 0.738\n",
            "[28,     4] loss: 0.723\n",
            "[28,     5] loss: 0.738\n",
            "[28,     6] loss: 0.747\n",
            "[28,     7] loss: 0.750\n",
            "[28,     8] loss: 0.766\n",
            "[29,     1] loss: 0.746\n",
            "[29,     2] loss: 0.782\n",
            "[29,     3] loss: 0.757\n",
            "[29,     4] loss: 0.746\n",
            "[29,     5] loss: 0.757\n",
            "[29,     6] loss: 0.757\n",
            "[29,     7] loss: 0.751\n",
            "[29,     8] loss: 0.745\n",
            "[30,     1] loss: 0.774\n",
            "[30,     2] loss: 0.773\n",
            "[30,     3] loss: 0.745\n",
            "[30,     4] loss: 0.755\n",
            "[30,     5] loss: 0.763\n",
            "[30,     6] loss: 0.765\n",
            "[30,     7] loss: 0.756\n",
            "[30,     8] loss: 0.770\n",
            "[31,     1] loss: 0.694\n",
            "[31,     2] loss: 0.712\n",
            "[31,     3] loss: 0.734\n",
            "[31,     4] loss: 0.725\n",
            "[31,     5] loss: 0.725\n",
            "[31,     6] loss: 0.726\n",
            "[31,     7] loss: 0.722\n",
            "[31,     8] loss: 0.726\n",
            "[32,     1] loss: 0.740\n",
            "[32,     2] loss: 0.723\n",
            "[32,     3] loss: 0.705\n",
            "[32,     4] loss: 0.712\n",
            "[32,     5] loss: 0.716\n",
            "[32,     6] loss: 0.727\n",
            "[32,     7] loss: 0.725\n",
            "[32,     8] loss: 0.727\n",
            "[33,     1] loss: 0.679\n",
            "[33,     2] loss: 0.709\n",
            "[33,     3] loss: 0.717\n",
            "[33,     4] loss: 0.713\n",
            "[33,     5] loss: 0.726\n",
            "[33,     6] loss: 0.728\n",
            "[33,     7] loss: 0.731\n",
            "[33,     8] loss: 0.739\n",
            "[34,     1] loss: 0.708\n",
            "[34,     2] loss: 0.745\n",
            "[34,     3] loss: 0.746\n",
            "[34,     4] loss: 0.742\n",
            "[34,     5] loss: 0.735\n",
            "[34,     6] loss: 0.743\n",
            "[34,     7] loss: 0.740\n",
            "[34,     8] loss: 0.746\n",
            "[35,     1] loss: 0.734\n",
            "[35,     2] loss: 0.777\n",
            "[35,     3] loss: 0.757\n",
            "[35,     4] loss: 0.727\n",
            "[35,     5] loss: 0.730\n",
            "[35,     6] loss: 0.726\n",
            "[35,     7] loss: 0.728\n",
            "[35,     8] loss: 0.717\n",
            "[36,     1] loss: 0.701\n",
            "[36,     2] loss: 0.723\n",
            "[36,     3] loss: 0.724\n",
            "[36,     4] loss: 0.748\n",
            "[36,     5] loss: 0.730\n",
            "[36,     6] loss: 0.728\n",
            "[36,     7] loss: 0.744\n",
            "[36,     8] loss: 0.751\n",
            "[37,     1] loss: 0.735\n",
            "[37,     2] loss: 0.717\n",
            "[37,     3] loss: 0.707\n",
            "[37,     4] loss: 0.696\n",
            "[37,     5] loss: 0.707\n",
            "[37,     6] loss: 0.720\n",
            "[37,     7] loss: 0.729\n",
            "[37,     8] loss: 0.727\n",
            "[38,     1] loss: 0.704\n",
            "[38,     2] loss: 0.733\n",
            "[38,     3] loss: 0.732\n",
            "[38,     4] loss: 0.718\n",
            "[38,     5] loss: 0.725\n",
            "[38,     6] loss: 0.728\n",
            "[38,     7] loss: 0.724\n",
            "[38,     8] loss: 0.732\n",
            "[39,     1] loss: 0.647\n",
            "[39,     2] loss: 0.666\n",
            "[39,     3] loss: 0.696\n",
            "[39,     4] loss: 0.709\n",
            "[39,     5] loss: 0.721\n",
            "[39,     6] loss: 0.713\n",
            "[39,     7] loss: 0.715\n",
            "[39,     8] loss: 0.711\n",
            "[40,     1] loss: 0.735\n",
            "[40,     2] loss: 0.698\n",
            "[40,     3] loss: 0.717\n",
            "[40,     4] loss: 0.720\n",
            "[40,     5] loss: 0.710\n",
            "[40,     6] loss: 0.709\n",
            "[40,     7] loss: 0.717\n",
            "[40,     8] loss: 0.724\n",
            "[41,     1] loss: 0.677\n",
            "[41,     2] loss: 0.711\n",
            "[41,     3] loss: 0.690\n",
            "[41,     4] loss: 0.695\n",
            "[41,     5] loss: 0.714\n",
            "[41,     6] loss: 0.715\n",
            "[41,     7] loss: 0.721\n",
            "[41,     8] loss: 0.740\n",
            "[42,     1] loss: 0.707\n",
            "[42,     2] loss: 0.682\n",
            "[42,     3] loss: 0.659\n",
            "[42,     4] loss: 0.679\n",
            "[42,     5] loss: 0.665\n",
            "[42,     6] loss: 0.681\n",
            "[42,     7] loss: 0.685\n",
            "[42,     8] loss: 0.698\n",
            "[43,     1] loss: 0.683\n",
            "[43,     2] loss: 0.704\n",
            "[43,     3] loss: 0.694\n",
            "[43,     4] loss: 0.677\n",
            "[43,     5] loss: 0.677\n",
            "[43,     6] loss: 0.675\n",
            "[43,     7] loss: 0.683\n",
            "[43,     8] loss: 0.695\n",
            "[44,     1] loss: 0.570\n",
            "[44,     2] loss: 0.663\n",
            "[44,     3] loss: 0.679\n",
            "[44,     4] loss: 0.672\n",
            "[44,     5] loss: 0.687\n",
            "[44,     6] loss: 0.687\n",
            "[44,     7] loss: 0.685\n",
            "[44,     8] loss: 0.696\n",
            "[45,     1] loss: 0.687\n",
            "[45,     2] loss: 0.678\n",
            "[45,     3] loss: 0.676\n",
            "[45,     4] loss: 0.713\n",
            "[45,     5] loss: 0.722\n",
            "[45,     6] loss: 0.725\n",
            "[45,     7] loss: 0.720\n",
            "[45,     8] loss: 0.714\n",
            "[46,     1] loss: 0.687\n",
            "[46,     2] loss: 0.694\n",
            "[46,     3] loss: 0.693\n",
            "[46,     4] loss: 0.699\n",
            "[46,     5] loss: 0.698\n",
            "[46,     6] loss: 0.702\n",
            "[46,     7] loss: 0.705\n",
            "[46,     8] loss: 0.710\n",
            "[47,     1] loss: 0.670\n",
            "[47,     2] loss: 0.716\n",
            "[47,     3] loss: 0.690\n",
            "[47,     4] loss: 0.680\n",
            "[47,     5] loss: 0.683\n",
            "[47,     6] loss: 0.696\n",
            "[47,     7] loss: 0.697\n",
            "[47,     8] loss: 0.698\n",
            "[48,     1] loss: 0.748\n",
            "[48,     2] loss: 0.750\n",
            "[48,     3] loss: 0.726\n",
            "[48,     4] loss: 0.697\n",
            "[48,     5] loss: 0.706\n",
            "[48,     6] loss: 0.697\n",
            "[48,     7] loss: 0.708\n",
            "[48,     8] loss: 0.711\n",
            "[49,     1] loss: 0.670\n",
            "[49,     2] loss: 0.697\n",
            "[49,     3] loss: 0.687\n",
            "[49,     4] loss: 0.686\n",
            "[49,     5] loss: 0.684\n",
            "[49,     6] loss: 0.683\n",
            "[49,     7] loss: 0.684\n",
            "[49,     8] loss: 0.687\n",
            "[50,     1] loss: 0.656\n",
            "[50,     2] loss: 0.662\n",
            "[50,     3] loss: 0.648\n",
            "[50,     4] loss: 0.668\n",
            "[50,     5] loss: 0.687\n",
            "[50,     6] loss: 0.683\n",
            "[50,     7] loss: 0.681\n",
            "[50,     8] loss: 0.686\n",
            "[51,     1] loss: 0.693\n",
            "[51,     2] loss: 0.672\n",
            "[51,     3] loss: 0.668\n",
            "[51,     4] loss: 0.693\n",
            "[51,     5] loss: 0.705\n",
            "[51,     6] loss: 0.710\n",
            "[51,     7] loss: 0.706\n",
            "[51,     8] loss: 0.698\n",
            "[52,     1] loss: 0.751\n",
            "[52,     2] loss: 0.737\n",
            "[52,     3] loss: 0.698\n",
            "[52,     4] loss: 0.703\n",
            "[52,     5] loss: 0.696\n",
            "[52,     6] loss: 0.697\n",
            "[52,     7] loss: 0.689\n",
            "[52,     8] loss: 0.692\n",
            "[53,     1] loss: 0.627\n",
            "[53,     2] loss: 0.641\n",
            "[53,     3] loss: 0.651\n",
            "[53,     4] loss: 0.649\n",
            "[53,     5] loss: 0.653\n",
            "[53,     6] loss: 0.667\n",
            "[53,     7] loss: 0.667\n",
            "[53,     8] loss: 0.680\n",
            "[54,     1] loss: 0.682\n",
            "[54,     2] loss: 0.680\n",
            "[54,     3] loss: 0.682\n",
            "[54,     4] loss: 0.683\n",
            "[54,     5] loss: 0.694\n",
            "[54,     6] loss: 0.691\n",
            "[54,     7] loss: 0.690\n",
            "[54,     8] loss: 0.706\n",
            "[55,     1] loss: 0.639\n",
            "[55,     2] loss: 0.660\n",
            "[55,     3] loss: 0.684\n",
            "[55,     4] loss: 0.674\n",
            "[55,     5] loss: 0.678\n",
            "[55,     6] loss: 0.692\n",
            "[55,     7] loss: 0.688\n",
            "[55,     8] loss: 0.692\n",
            "[56,     1] loss: 0.693\n",
            "[56,     2] loss: 0.703\n",
            "[56,     3] loss: 0.675\n",
            "[56,     4] loss: 0.674\n",
            "[56,     5] loss: 0.680\n",
            "[56,     6] loss: 0.675\n",
            "[56,     7] loss: 0.686\n",
            "[56,     8] loss: 0.694\n",
            "[57,     1] loss: 0.633\n",
            "[57,     2] loss: 0.681\n",
            "[57,     3] loss: 0.704\n",
            "[57,     4] loss: 0.685\n",
            "[57,     5] loss: 0.672\n",
            "[57,     6] loss: 0.686\n",
            "[57,     7] loss: 0.687\n",
            "[57,     8] loss: 0.703\n",
            "[58,     1] loss: 0.768\n",
            "[58,     2] loss: 0.727\n",
            "[58,     3] loss: 0.714\n",
            "[58,     4] loss: 0.696\n",
            "[58,     5] loss: 0.699\n",
            "[58,     6] loss: 0.701\n",
            "[58,     7] loss: 0.699\n",
            "[58,     8] loss: 0.710\n",
            "[59,     1] loss: 0.693\n",
            "[59,     2] loss: 0.674\n",
            "[59,     3] loss: 0.673\n",
            "[59,     4] loss: 0.672\n",
            "[59,     5] loss: 0.676\n",
            "[59,     6] loss: 0.682\n",
            "[59,     7] loss: 0.683\n",
            "[59,     8] loss: 0.681\n",
            "[60,     1] loss: 0.643\n",
            "[60,     2] loss: 0.694\n",
            "[60,     3] loss: 0.709\n",
            "[60,     4] loss: 0.686\n",
            "[60,     5] loss: 0.676\n",
            "[60,     6] loss: 0.675\n",
            "[60,     7] loss: 0.682\n",
            "[60,     8] loss: 0.702\n",
            "[61,     1] loss: 0.702\n",
            "[61,     2] loss: 0.682\n",
            "[61,     3] loss: 0.684\n",
            "[61,     4] loss: 0.668\n",
            "[61,     5] loss: 0.672\n",
            "[61,     6] loss: 0.678\n",
            "[61,     7] loss: 0.689\n",
            "[61,     8] loss: 0.683\n",
            "[62,     1] loss: 0.749\n",
            "[62,     2] loss: 0.765\n",
            "[62,     3] loss: 0.747\n",
            "[62,     4] loss: 0.734\n",
            "[62,     5] loss: 0.726\n",
            "[62,     6] loss: 0.725\n",
            "[62,     7] loss: 0.713\n",
            "[62,     8] loss: 0.716\n",
            "[63,     1] loss: 0.657\n",
            "[63,     2] loss: 0.669\n",
            "[63,     3] loss: 0.662\n",
            "[63,     4] loss: 0.671\n",
            "[63,     5] loss: 0.686\n",
            "[63,     6] loss: 0.684\n",
            "[63,     7] loss: 0.682\n",
            "[63,     8] loss: 0.690\n",
            "[64,     1] loss: 0.717\n",
            "[64,     2] loss: 0.708\n",
            "[64,     3] loss: 0.698\n",
            "[64,     4] loss: 0.706\n",
            "[64,     5] loss: 0.710\n",
            "[64,     6] loss: 0.704\n",
            "[64,     7] loss: 0.689\n",
            "[64,     8] loss: 0.685\n",
            "[65,     1] loss: 0.602\n",
            "[65,     2] loss: 0.639\n",
            "[65,     3] loss: 0.639\n",
            "[65,     4] loss: 0.642\n",
            "[65,     5] loss: 0.642\n",
            "[65,     6] loss: 0.649\n",
            "[65,     7] loss: 0.652\n",
            "[65,     8] loss: 0.660\n",
            "[66,     1] loss: 0.661\n",
            "[66,     2] loss: 0.694\n",
            "[66,     3] loss: 0.696\n",
            "[66,     4] loss: 0.699\n",
            "[66,     5] loss: 0.710\n",
            "[66,     6] loss: 0.713\n",
            "[66,     7] loss: 0.702\n",
            "[66,     8] loss: 0.702\n",
            "[67,     1] loss: 0.716\n",
            "[67,     2] loss: 0.661\n",
            "[67,     3] loss: 0.674\n",
            "[67,     4] loss: 0.668\n",
            "[67,     5] loss: 0.668\n",
            "[67,     6] loss: 0.679\n",
            "[67,     7] loss: 0.672\n",
            "[67,     8] loss: 0.676\n",
            "[68,     1] loss: 0.683\n",
            "[68,     2] loss: 0.669\n",
            "[68,     3] loss: 0.644\n",
            "[68,     4] loss: 0.634\n",
            "[68,     5] loss: 0.632\n",
            "[68,     6] loss: 0.643\n",
            "[68,     7] loss: 0.647\n",
            "[68,     8] loss: 0.649\n",
            "[69,     1] loss: 0.662\n",
            "[69,     2] loss: 0.648\n",
            "[69,     3] loss: 0.633\n",
            "[69,     4] loss: 0.653\n",
            "[69,     5] loss: 0.649\n",
            "[69,     6] loss: 0.643\n",
            "[69,     7] loss: 0.643\n",
            "[69,     8] loss: 0.657\n",
            "[70,     1] loss: 0.675\n",
            "[70,     2] loss: 0.689\n",
            "[70,     3] loss: 0.703\n",
            "[70,     4] loss: 0.719\n",
            "[70,     5] loss: 0.717\n",
            "[70,     6] loss: 0.717\n",
            "[70,     7] loss: 0.715\n",
            "[70,     8] loss: 0.724\n",
            "[71,     1] loss: 0.632\n",
            "[71,     2] loss: 0.651\n",
            "[71,     3] loss: 0.643\n",
            "[71,     4] loss: 0.643\n",
            "[71,     5] loss: 0.648\n",
            "[71,     6] loss: 0.664\n",
            "[71,     7] loss: 0.665\n",
            "[71,     8] loss: 0.652\n",
            "[72,     1] loss: 0.604\n",
            "[72,     2] loss: 0.645\n",
            "[72,     3] loss: 0.621\n",
            "[72,     4] loss: 0.612\n",
            "[72,     5] loss: 0.628\n",
            "[72,     6] loss: 0.637\n",
            "[72,     7] loss: 0.648\n",
            "[72,     8] loss: 0.652\n",
            "[73,     1] loss: 0.644\n",
            "[73,     2] loss: 0.659\n",
            "[73,     3] loss: 0.657\n",
            "[73,     4] loss: 0.663\n",
            "[73,     5] loss: 0.656\n",
            "[73,     6] loss: 0.667\n",
            "[73,     7] loss: 0.670\n",
            "[73,     8] loss: 0.674\n",
            "[74,     1] loss: 0.593\n",
            "[74,     2] loss: 0.649\n",
            "[74,     3] loss: 0.634\n",
            "[74,     4] loss: 0.651\n",
            "[74,     5] loss: 0.645\n",
            "[74,     6] loss: 0.653\n",
            "[74,     7] loss: 0.668\n",
            "[74,     8] loss: 0.671\n",
            "[75,     1] loss: 0.640\n",
            "[75,     2] loss: 0.620\n",
            "[75,     3] loss: 0.616\n",
            "[75,     4] loss: 0.642\n",
            "[75,     5] loss: 0.655\n",
            "[75,     6] loss: 0.653\n",
            "[75,     7] loss: 0.652\n",
            "[75,     8] loss: 0.664\n",
            "[76,     1] loss: 0.623\n",
            "[76,     2] loss: 0.678\n",
            "[76,     3] loss: 0.667\n",
            "[76,     4] loss: 0.674\n",
            "[76,     5] loss: 0.662\n",
            "[76,     6] loss: 0.677\n",
            "[76,     7] loss: 0.687\n",
            "[76,     8] loss: 0.686\n",
            "[77,     1] loss: 0.597\n",
            "[77,     2] loss: 0.641\n",
            "[77,     3] loss: 0.631\n",
            "[77,     4] loss: 0.636\n",
            "[77,     5] loss: 0.648\n",
            "[77,     6] loss: 0.651\n",
            "[77,     7] loss: 0.658\n",
            "[77,     8] loss: 0.654\n",
            "[78,     1] loss: 0.671\n",
            "[78,     2] loss: 0.649\n",
            "[78,     3] loss: 0.682\n",
            "[78,     4] loss: 0.675\n",
            "[78,     5] loss: 0.666\n",
            "[78,     6] loss: 0.667\n",
            "[78,     7] loss: 0.670\n",
            "[78,     8] loss: 0.669\n",
            "[79,     1] loss: 0.599\n",
            "[79,     2] loss: 0.588\n",
            "[79,     3] loss: 0.604\n",
            "[79,     4] loss: 0.625\n",
            "[79,     5] loss: 0.640\n",
            "[79,     6] loss: 0.646\n",
            "[79,     7] loss: 0.652\n",
            "[79,     8] loss: 0.654\n",
            "[80,     1] loss: 0.598\n",
            "[80,     2] loss: 0.640\n",
            "[80,     3] loss: 0.651\n",
            "[80,     4] loss: 0.668\n",
            "[80,     5] loss: 0.669\n",
            "[80,     6] loss: 0.672\n",
            "[80,     7] loss: 0.663\n",
            "[80,     8] loss: 0.666\n",
            "[81,     1] loss: 0.607\n",
            "[81,     2] loss: 0.616\n",
            "[81,     3] loss: 0.648\n",
            "[81,     4] loss: 0.663\n",
            "[81,     5] loss: 0.669\n",
            "[81,     6] loss: 0.679\n",
            "[81,     7] loss: 0.678\n",
            "[81,     8] loss: 0.672\n",
            "[82,     1] loss: 0.680\n",
            "[82,     2] loss: 0.632\n",
            "[82,     3] loss: 0.640\n",
            "[82,     4] loss: 0.647\n",
            "[82,     5] loss: 0.649\n",
            "[82,     6] loss: 0.654\n",
            "[82,     7] loss: 0.654\n",
            "[82,     8] loss: 0.668\n",
            "[83,     1] loss: 0.661\n",
            "[83,     2] loss: 0.693\n",
            "[83,     3] loss: 0.690\n",
            "[83,     4] loss: 0.686\n",
            "[83,     5] loss: 0.664\n",
            "[83,     6] loss: 0.667\n",
            "[83,     7] loss: 0.671\n",
            "[83,     8] loss: 0.685\n",
            "[84,     1] loss: 0.625\n",
            "[84,     2] loss: 0.643\n",
            "[84,     3] loss: 0.635\n",
            "[84,     4] loss: 0.647\n",
            "[84,     5] loss: 0.664\n",
            "[84,     6] loss: 0.654\n",
            "[84,     7] loss: 0.661\n",
            "[84,     8] loss: 0.673\n",
            "[85,     1] loss: 0.660\n",
            "[85,     2] loss: 0.658\n",
            "[85,     3] loss: 0.635\n",
            "[85,     4] loss: 0.649\n",
            "[85,     5] loss: 0.653\n",
            "[85,     6] loss: 0.651\n",
            "[85,     7] loss: 0.660\n",
            "[85,     8] loss: 0.654\n",
            "[86,     1] loss: 0.662\n",
            "[86,     2] loss: 0.660\n",
            "[86,     3] loss: 0.649\n",
            "[86,     4] loss: 0.667\n",
            "[86,     5] loss: 0.664\n",
            "[86,     6] loss: 0.672\n",
            "[86,     7] loss: 0.668\n",
            "[86,     8] loss: 0.675\n",
            "[87,     1] loss: 0.585\n",
            "[87,     2] loss: 0.592\n",
            "[87,     3] loss: 0.618\n",
            "[87,     4] loss: 0.626\n",
            "[87,     5] loss: 0.645\n",
            "[87,     6] loss: 0.662\n",
            "[87,     7] loss: 0.663\n",
            "[87,     8] loss: 0.677\n",
            "[88,     1] loss: 0.602\n",
            "[88,     2] loss: 0.630\n",
            "[88,     3] loss: 0.630\n",
            "[88,     4] loss: 0.646\n",
            "[88,     5] loss: 0.642\n",
            "[88,     6] loss: 0.635\n",
            "[88,     7] loss: 0.640\n",
            "[88,     8] loss: 0.641\n",
            "[89,     1] loss: 0.628\n",
            "[89,     2] loss: 0.692\n",
            "[89,     3] loss: 0.678\n",
            "[89,     4] loss: 0.672\n",
            "[89,     5] loss: 0.671\n",
            "[89,     6] loss: 0.657\n",
            "[89,     7] loss: 0.655\n",
            "[89,     8] loss: 0.655\n",
            "[90,     1] loss: 0.711\n",
            "[90,     2] loss: 0.676\n",
            "[90,     3] loss: 0.681\n",
            "[90,     4] loss: 0.666\n",
            "[90,     5] loss: 0.658\n",
            "[90,     6] loss: 0.659\n",
            "[90,     7] loss: 0.661\n",
            "[90,     8] loss: 0.664\n",
            "[91,     1] loss: 0.698\n",
            "[91,     2] loss: 0.688\n",
            "[91,     3] loss: 0.691\n",
            "[91,     4] loss: 0.682\n",
            "[91,     5] loss: 0.686\n",
            "[91,     6] loss: 0.676\n",
            "[91,     7] loss: 0.681\n",
            "[91,     8] loss: 0.677\n",
            "[92,     1] loss: 0.656\n",
            "[92,     2] loss: 0.691\n",
            "[92,     3] loss: 0.667\n",
            "[92,     4] loss: 0.662\n",
            "[92,     5] loss: 0.660\n",
            "[92,     6] loss: 0.654\n",
            "[92,     7] loss: 0.664\n",
            "[92,     8] loss: 0.657\n",
            "[93,     1] loss: 0.597\n",
            "[93,     2] loss: 0.623\n",
            "[93,     3] loss: 0.652\n",
            "[93,     4] loss: 0.667\n",
            "[93,     5] loss: 0.674\n",
            "[93,     6] loss: 0.666\n",
            "[93,     7] loss: 0.661\n",
            "[93,     8] loss: 0.667\n",
            "[94,     1] loss: 0.651\n",
            "[94,     2] loss: 0.653\n",
            "[94,     3] loss: 0.660\n",
            "[94,     4] loss: 0.639\n",
            "[94,     5] loss: 0.648\n",
            "[94,     6] loss: 0.658\n",
            "[94,     7] loss: 0.664\n",
            "[94,     8] loss: 0.667\n",
            "[95,     1] loss: 0.627\n",
            "[95,     2] loss: 0.617\n",
            "[95,     3] loss: 0.630\n",
            "[95,     4] loss: 0.624\n",
            "[95,     5] loss: 0.621\n",
            "[95,     6] loss: 0.631\n",
            "[95,     7] loss: 0.641\n",
            "[95,     8] loss: 0.655\n",
            "[96,     1] loss: 0.683\n",
            "[96,     2] loss: 0.652\n",
            "[96,     3] loss: 0.636\n",
            "[96,     4] loss: 0.658\n",
            "[96,     5] loss: 0.661\n",
            "[96,     6] loss: 0.661\n",
            "[96,     7] loss: 0.658\n",
            "[96,     8] loss: 0.659\n",
            "[97,     1] loss: 0.655\n",
            "[97,     2] loss: 0.641\n",
            "[97,     3] loss: 0.642\n",
            "[97,     4] loss: 0.632\n",
            "[97,     5] loss: 0.631\n",
            "[97,     6] loss: 0.627\n",
            "[97,     7] loss: 0.626\n",
            "[97,     8] loss: 0.634\n",
            "[98,     1] loss: 0.718\n",
            "[98,     2] loss: 0.658\n",
            "[98,     3] loss: 0.645\n",
            "[98,     4] loss: 0.641\n",
            "[98,     5] loss: 0.650\n",
            "[98,     6] loss: 0.651\n",
            "[98,     7] loss: 0.650\n",
            "[98,     8] loss: 0.668\n",
            "[99,     1] loss: 0.610\n",
            "[99,     2] loss: 0.650\n",
            "[99,     3] loss: 0.638\n",
            "[99,     4] loss: 0.636\n",
            "[99,     5] loss: 0.639\n",
            "[99,     6] loss: 0.646\n",
            "[99,     7] loss: 0.644\n",
            "[99,     8] loss: 0.655\n",
            "[100,     1] loss: 0.677\n",
            "[100,     2] loss: 0.662\n",
            "[100,     3] loss: 0.662\n",
            "[100,     4] loss: 0.637\n",
            "[100,     5] loss: 0.643\n",
            "[100,     6] loss: 0.648\n",
            "[100,     7] loss: 0.646\n",
            "[100,     8] loss: 0.657\n",
            "[101,     1] loss: 0.607\n",
            "[101,     2] loss: 0.640\n",
            "[101,     3] loss: 0.618\n",
            "[101,     4] loss: 0.618\n",
            "[101,     5] loss: 0.633\n",
            "[101,     6] loss: 0.638\n",
            "[101,     7] loss: 0.636\n",
            "[101,     8] loss: 0.635\n",
            "[102,     1] loss: 0.622\n",
            "[102,     2] loss: 0.641\n",
            "[102,     3] loss: 0.638\n",
            "[102,     4] loss: 0.631\n",
            "[102,     5] loss: 0.629\n",
            "[102,     6] loss: 0.635\n",
            "[102,     7] loss: 0.647\n",
            "[102,     8] loss: 0.644\n",
            "[103,     1] loss: 0.621\n",
            "[103,     2] loss: 0.667\n",
            "[103,     3] loss: 0.636\n",
            "[103,     4] loss: 0.643\n",
            "[103,     5] loss: 0.642\n",
            "[103,     6] loss: 0.650\n",
            "[103,     7] loss: 0.652\n",
            "[103,     8] loss: 0.659\n",
            "[104,     1] loss: 0.580\n",
            "[104,     2] loss: 0.625\n",
            "[104,     3] loss: 0.628\n",
            "[104,     4] loss: 0.622\n",
            "[104,     5] loss: 0.626\n",
            "[104,     6] loss: 0.635\n",
            "[104,     7] loss: 0.635\n",
            "[104,     8] loss: 0.642\n",
            "[105,     1] loss: 0.639\n",
            "[105,     2] loss: 0.665\n",
            "[105,     3] loss: 0.658\n",
            "[105,     4] loss: 0.665\n",
            "[105,     5] loss: 0.656\n",
            "[105,     6] loss: 0.658\n",
            "[105,     7] loss: 0.666\n",
            "[105,     8] loss: 0.660\n",
            "[106,     1] loss: 0.624\n",
            "[106,     2] loss: 0.633\n",
            "[106,     3] loss: 0.620\n",
            "[106,     4] loss: 0.623\n",
            "[106,     5] loss: 0.628\n",
            "[106,     6] loss: 0.638\n",
            "[106,     7] loss: 0.641\n",
            "[106,     8] loss: 0.644\n",
            "[107,     1] loss: 0.574\n",
            "[107,     2] loss: 0.587\n",
            "[107,     3] loss: 0.584\n",
            "[107,     4] loss: 0.601\n",
            "[107,     5] loss: 0.606\n",
            "[107,     6] loss: 0.613\n",
            "[107,     7] loss: 0.617\n",
            "[107,     8] loss: 0.619\n",
            "[108,     1] loss: 0.590\n",
            "[108,     2] loss: 0.596\n",
            "[108,     3] loss: 0.611\n",
            "[108,     4] loss: 0.618\n",
            "[108,     5] loss: 0.628\n",
            "[108,     6] loss: 0.627\n",
            "[108,     7] loss: 0.633\n",
            "[108,     8] loss: 0.629\n",
            "[109,     1] loss: 0.618\n",
            "[109,     2] loss: 0.633\n",
            "[109,     3] loss: 0.605\n",
            "[109,     4] loss: 0.611\n",
            "[109,     5] loss: 0.616\n",
            "[109,     6] loss: 0.618\n",
            "[109,     7] loss: 0.627\n",
            "[109,     8] loss: 0.634\n",
            "[110,     1] loss: 0.690\n",
            "[110,     2] loss: 0.673\n",
            "[110,     3] loss: 0.666\n",
            "[110,     4] loss: 0.661\n",
            "[110,     5] loss: 0.666\n",
            "[110,     6] loss: 0.663\n",
            "[110,     7] loss: 0.660\n",
            "[110,     8] loss: 0.661\n",
            "[111,     1] loss: 0.628\n",
            "[111,     2] loss: 0.671\n",
            "[111,     3] loss: 0.656\n",
            "[111,     4] loss: 0.646\n",
            "[111,     5] loss: 0.635\n",
            "[111,     6] loss: 0.639\n",
            "[111,     7] loss: 0.634\n",
            "[111,     8] loss: 0.636\n",
            "[112,     1] loss: 0.620\n",
            "[112,     2] loss: 0.606\n",
            "[112,     3] loss: 0.591\n",
            "[112,     4] loss: 0.602\n",
            "[112,     5] loss: 0.606\n",
            "[112,     6] loss: 0.629\n",
            "[112,     7] loss: 0.642\n",
            "[112,     8] loss: 0.661\n",
            "[113,     1] loss: 0.835\n",
            "[113,     2] loss: 0.798\n",
            "[113,     3] loss: 0.801\n",
            "[113,     4] loss: 0.803\n",
            "[113,     5] loss: 0.775\n",
            "[113,     6] loss: 0.777\n",
            "[113,     7] loss: 0.774\n",
            "[113,     8] loss: 0.798\n",
            "[114,     1] loss: 0.766\n",
            "[114,     2] loss: 0.775\n",
            "[114,     3] loss: 0.761\n",
            "[114,     4] loss: 0.763\n",
            "[114,     5] loss: 0.761\n",
            "[114,     6] loss: 0.784\n",
            "[114,     7] loss: 0.771\n",
            "[114,     8] loss: 0.804\n",
            "[115,     1] loss: 0.767\n",
            "[115,     2] loss: 0.734\n",
            "[115,     3] loss: 0.756\n",
            "[115,     4] loss: 0.752\n",
            "[115,     5] loss: 0.760\n",
            "[115,     6] loss: 0.746\n",
            "[115,     7] loss: 0.760\n",
            "[115,     8] loss: 0.776\n",
            "[116,     1] loss: 0.717\n",
            "[116,     2] loss: 0.756\n",
            "[116,     3] loss: 0.755\n",
            "[116,     4] loss: 0.759\n",
            "[116,     5] loss: 0.743\n",
            "[116,     6] loss: 0.758\n",
            "[116,     7] loss: 0.763\n",
            "[116,     8] loss: 0.772\n",
            "[117,     1] loss: 0.733\n",
            "[117,     2] loss: 0.830\n",
            "[117,     3] loss: 0.785\n",
            "[117,     4] loss: 0.758\n",
            "[117,     5] loss: 0.747\n",
            "[117,     6] loss: 0.740\n",
            "[117,     7] loss: 0.757\n",
            "[117,     8] loss: 0.768\n",
            "[118,     1] loss: 0.726\n",
            "[118,     2] loss: 0.756\n",
            "[118,     3] loss: 0.743\n",
            "[118,     4] loss: 0.760\n",
            "[118,     5] loss: 0.787\n",
            "[118,     6] loss: 0.784\n",
            "[118,     7] loss: 0.790\n",
            "[118,     8] loss: 0.785\n",
            "[119,     1] loss: 0.713\n",
            "[119,     2] loss: 0.739\n",
            "[119,     3] loss: 0.751\n",
            "[119,     4] loss: 0.777\n",
            "[119,     5] loss: 0.779\n",
            "[119,     6] loss: 0.796\n",
            "[119,     7] loss: 0.778\n",
            "[119,     8] loss: 0.780\n",
            "[120,     1] loss: 0.751\n",
            "[120,     2] loss: 0.769\n",
            "[120,     3] loss: 0.737\n",
            "[120,     4] loss: 0.743\n",
            "[120,     5] loss: 0.760\n",
            "[120,     6] loss: 0.762\n",
            "[120,     7] loss: 0.782\n",
            "[120,     8] loss: 0.793\n",
            "[121,     1] loss: 0.683\n",
            "[121,     2] loss: 0.719\n",
            "[121,     3] loss: 0.726\n",
            "[121,     4] loss: 0.736\n",
            "[121,     5] loss: 0.744\n",
            "[121,     6] loss: 0.753\n",
            "[121,     7] loss: 0.758\n",
            "[121,     8] loss: 0.742\n",
            "[122,     1] loss: 0.720\n",
            "[122,     2] loss: 0.716\n",
            "[122,     3] loss: 0.718\n",
            "[122,     4] loss: 0.732\n",
            "[122,     5] loss: 0.740\n",
            "[122,     6] loss: 0.729\n",
            "[122,     7] loss: 0.736\n",
            "[122,     8] loss: 0.743\n",
            "[123,     1] loss: 0.756\n",
            "[123,     2] loss: 0.781\n",
            "[123,     3] loss: 0.767\n",
            "[123,     4] loss: 0.771\n",
            "[123,     5] loss: 0.780\n",
            "[123,     6] loss: 0.775\n",
            "[123,     7] loss: 0.774\n",
            "[123,     8] loss: 0.788\n",
            "[124,     1] loss: 0.725\n",
            "[124,     2] loss: 0.714\n",
            "[124,     3] loss: 0.713\n",
            "[124,     4] loss: 0.709\n",
            "[124,     5] loss: 0.725\n",
            "[124,     6] loss: 0.738\n",
            "[124,     7] loss: 0.740\n",
            "[124,     8] loss: 0.753\n",
            "[125,     1] loss: 0.662\n",
            "[125,     2] loss: 0.735\n",
            "[125,     3] loss: 0.725\n",
            "[125,     4] loss: 0.737\n",
            "[125,     5] loss: 0.707\n",
            "[125,     6] loss: 0.737\n",
            "[125,     7] loss: 0.736\n",
            "[125,     8] loss: 0.738\n",
            "[126,     1] loss: 0.690\n",
            "[126,     2] loss: 0.721\n",
            "[126,     3] loss: 0.704\n",
            "[126,     4] loss: 0.711\n",
            "[126,     5] loss: 0.725\n",
            "[126,     6] loss: 0.741\n",
            "[126,     7] loss: 0.727\n",
            "[126,     8] loss: 0.746\n",
            "[127,     1] loss: 0.683\n",
            "[127,     2] loss: 0.692\n",
            "[127,     3] loss: 0.718\n",
            "[127,     4] loss: 0.721\n",
            "[127,     5] loss: 0.725\n",
            "[127,     6] loss: 0.756\n",
            "[127,     7] loss: 0.750\n",
            "[127,     8] loss: 0.750\n",
            "[128,     1] loss: 0.663\n",
            "[128,     2] loss: 0.727\n",
            "[128,     3] loss: 0.703\n",
            "[128,     4] loss: 0.702\n",
            "[128,     5] loss: 0.714\n",
            "[128,     6] loss: 0.724\n",
            "[128,     7] loss: 0.731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOtNI0OORbSI",
        "colab_type": "code",
        "outputId": "5e1f8ff6-094d-4136-828b-cdc404f7c32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "### Test to see if labels are correct\n",
        "\n",
        "eval_set = Dataset(os.path.join(data_path, \"eval.pkl\"))\n",
        "\n",
        "batch_size = 1\n",
        "eval_generator = data.DataLoader(eval_set, **{'batch_size': batch_size})\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure(figsize=(5, 5))\n",
        "i=1\n",
        "columns = 2\n",
        "rows = batch_size\n",
        "for x1, x2, y in eval_generator:\n",
        "  print(x1.shape)\n",
        "  print(x2.shape)\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  # img = np.random.randint(10, size=(h,w))\n",
        "  plt.imshow(x1.squeeze())\n",
        "  i+=1\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.imshow(x2.squeeze())\n",
        "  i+=1\n",
        "  if (y == 1):\n",
        "    print(\"same letter\")\n",
        "  else:\n",
        "    print(\"different letter\")\n",
        "  break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 105, 105])\n",
            "torch.Size([1, 105, 105])\n",
            "same letter\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAChCAYAAABdyN06AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4oWq6bmz5Z",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjP2BRqhCSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gDe5nYvhFQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}